{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sandbox.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/cheneeheng/Transformer-TF/blob/colab_dev/Cifar10_VIT.ipynb",
      "authorship_tag": "ABX9TyNpzuoDofmwalK1TUd5hLQs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheneeheng/Transformer-TF/blob/colab_dev/Cifar10_LAMBDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTYg3rNWKMt7"
      },
      "source": [
        "# **CIFAR10 Classification**\n",
        "\n",
        "Based on [CIFAR10_Keras_GPU.ipynb](https://github.com/katnoria/cifar10-native-vs-colab/blob/master/CIFAR10_Keras_GPU.ipynb) from [katnoria/cifar10-native-vs-colab](https://github.com/katnoria/cifar10-native-vs-colab) .\n",
        "\n",
        "General info about the dataset:\n",
        "- 50K Train, 10K Test\n",
        "- 10 object classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncg-GVYwK1TG"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV1eLcgmDsyN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7ab4eb03-dd98-4e24-a0c9-e2eef431b621"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")\n",
        "\n",
        "SAVE_PATH = \"/content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323\"\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "from time import time\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnrIX5LiLGqz"
      },
      "source": [
        "# Data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwetmhNHLGPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bde2c10-7a4b-4cd2-d5ef-2f41ca3675e6"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(x_trn_full, y_trn_full), (x_tst, y_tst) = cifar10.load_data()\n",
        "\n",
        "# remove the last dimension\n",
        "y_trn_full = y_trn_full.reshape(y_trn_full.shape[0],)\n",
        "y_tst = y_tst.reshape(y_tst.shape[0],)\n",
        "\n",
        "# perform in model.\n",
        "# # normalize data to 0..1\n",
        "# x_trn_full, x_tst = x_trn_full / 255.0, x_tst / 255.0\n",
        "\n",
        "# create validation split\n",
        "# split = 0.2\n",
        "# x_trn, x_val, y_trn, y_val = train_test_split(\n",
        "#     x_trn_full, y_trn_full, test_size=split, random_state=1969)\n",
        "x_trn, x_val, y_trn, y_val = x_trn_full, x_tst, y_trn_full, y_tst\n",
        "\n",
        "print(f'x_trn.shape: {x_trn.shape}')\n",
        "print(f'y_trn.shape: {y_trn.shape}')\n",
        "print(f'x_val.shape: {x_val.shape}')\n",
        "print(f'y_val.shape: {y_val.shape}')\n",
        "print(f'x_tst shape: {x_tst.shape}')\n",
        "print(f'y_tst.shape: {y_tst.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "x_trn.shape: (50000, 32, 32, 3)\n",
            "y_trn.shape: (50000,)\n",
            "x_val.shape: (10000, 32, 32, 3)\n",
            "y_val.shape: (10000,)\n",
            "x_tst shape: (10000, 32, 32, 3)\n",
            "y_tst.shape: (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JqdGbPROPRm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "267bae83-f352-4b98-ef4c-7c1188c33da7"
      },
      "source": [
        "# pick 25 random images and plot\n",
        "idxs = np.random.randint(x_trn.shape[0], size=25)\n",
        "images = x_trn[idxs]\n",
        "labels = y_trn[idxs]\n",
        "classnames = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "              'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "fig, axes = plt.subplots(5,5, figsize=(8,9))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "  ax.imshow(images[i])\n",
        "  ax.axis('off')\n",
        "  idx = labels[i]\n",
        "  ax.set_title(classnames[idx])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x648 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ0WDLT4s00F"
      },
      "source": [
        "class Patches(tf.keras.layers.Layer):\n",
        "  \"\"\"Creates patches from an image. Implemented as a keras layer.\"\"\"\n",
        "  def __init__(self, patch_size: int):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "  def call(self, x):\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=x,\n",
        "        sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "        strides=[1, self.patch_size, self.patch_size, 1],\n",
        "        rates=[1, 1, 1, 1],\n",
        "        padding=\"VALID\",\n",
        "    )\n",
        "    patch_dims = patches.shape[-1]\n",
        "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "    return patches\n",
        "\n",
        "  def get_config(self):\n",
        "    return {\"patch_size\": self.patch_size}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "vRUfSiYstKud",
        "outputId": "52b17ca5-9c05-4ac0-a44b-dbdde457db69"
      },
      "source": [
        "plt.figure(figsize=(4, 4))\n",
        "image = x_trn[np.random.choice(range(x_trn.shape[0]))]\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "image_size = 32\n",
        "patch_size = 4\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image size: 32 X 32\n",
            "Patch size: 4 X 4\n",
            "Patches per image: 64\n",
            "Elements per patch: 48\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 64 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHDdb2NJOVYU"
      },
      "source": [
        "def data_generator(split: str, batch_size: int, shuffle_buffer: int = 10000):\n",
        "  \"\"\"Creates a tf.data.Dataset instance.\n",
        "\n",
        "  Args:\n",
        "      split: The type of data to generate, ['train', 'val', 'test'].\n",
        "      batch_size: Batch size.\n",
        "      shuffle_buffer: Number of elements used for shuffling.\n",
        "\n",
        "  Returns:\n",
        "      A tf.data.Dataset instance.\n",
        "\n",
        "  Raises:\n",
        "      ValueError: If `split` is not ['train', 'val', 'test'].\n",
        "\n",
        "  \"\"\"\n",
        "  if split == 'train':\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x_trn, y_trn))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "  elif split == 'val':\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "  elif split == 'test':\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x_tst, y_tst))\n",
        "  else:\n",
        "    raise ValueError(f\"Unknown data split : {split}\")\n",
        "  return ds.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqkixwrjM7wE"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn9ZY763Jg7x"
      },
      "source": [
        "##CNN\n",
        "\n",
        "A simple CNN example from the keras [CNN benchmark](https://github.com/keras-team/keras/blob/master/keras/benchmarks/keras_examples_benchmarks/cifar10_cnn_benchmark_test.py) .  \n",
        "It is based on this [CNN example](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py) but the example is no longer available.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McvCeeVJJf8j"
      },
      "source": [
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (\n",
        "  Conv2D, \n",
        "  BatchNormalization, \n",
        "  MaxPool2D,\n",
        "  Dropout,\n",
        "  Flatten,\n",
        "  Dense\n",
        ")\n",
        "from tensorflow.keras.initializers import TruncatedNormal, Constant, Zeros\n",
        "from tensorflow.keras.regularizers import L2\n",
        "from tensorflow.keras.activations import relu\n",
        "from tensorflow.keras.layers.experimental.preprocessing import (\n",
        "  RandomFlip,\n",
        "  RandomTranslation,\n",
        "  RandomRotation,\n",
        "  RandomZoom,\n",
        "  Normalization,\n",
        "  Rescaling,\n",
        "  Resizing\n",
        ")\n",
        "\n",
        "def conv2d(filters, kernel_size, strides=(1, 1), padding='valid', **kwargs):\n",
        "  return Conv2D(filters, \n",
        "                kernel_size, \n",
        "                strides, \n",
        "                padding=padding, \n",
        "                activation=relu,\n",
        "                **kwargs)\n",
        "  \n",
        "def maxpool2d(**kwargs):\n",
        "  return MaxPool2D([3, 3], 2, padding='valid', **kwargs)\n",
        "\n",
        "class SmallCNN(Model):\n",
        "  \"\"\"SmallCNN implementation.\n",
        "  \n",
        "  Changes compared to the original model:\n",
        "  - maxpool2d uses a kernel_size=3, stride=2.\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, \n",
        "               num_classes: int, \n",
        "               image_height: int, \n",
        "               image_width: int,\n",
        "               preprocess: bool = False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_classes = num_classes\n",
        "    self.image_height = image_height\n",
        "    self.image_width = image_width\n",
        "    self.preprocess = preprocess\n",
        "\n",
        "    self.rescale = Rescaling(1./255., name='rescale')\n",
        "    self.resize = Resizing(self.image_height, self.image_width, name='resize')\n",
        "\n",
        "    # Preprocessing\n",
        "    self.augs = [\n",
        "      RandomFlip(\"horizontal\", name='data_aug_flip'),\n",
        "      # RandomTranslation((-0.1, 0.1), (-0.1, 0.1), name='data_aug2'),\n",
        "      RandomRotation(0.02, name='data_aug_rot'),\n",
        "      RandomZoom(0.2, 0.2, name='data_aug_zoom'),\n",
        "    ]\n",
        "\n",
        "    # Model\n",
        "    self.conv1 = conv2d(32, 3, name='conv1', padding='same')\n",
        "    self.conv2 = conv2d(32, 3, name='conv2')\n",
        "    self.pool2 = maxpool2d(name='pool1')\n",
        "    self.drop2 = Dropout(0.5, name='drop2')\n",
        "    self.conv3 = conv2d(64, 3, name='conv3', padding='same')\n",
        "    self.conv4 = conv2d(64, 3, name='conv4')\n",
        "    self.pool4 = maxpool2d(name='pool4')\n",
        "    self.drop4 = Dropout(0.25, name='drop4')\n",
        "    self.flat5 = Flatten(name='flat5')\n",
        "    self.dens5 = Dense(512, activation=relu, name='dens5')\n",
        "    self.drop5 = Dropout(0.5, name='drop5')\n",
        "    self.dens6 = Dense(self.num_classes, name='logits')\n",
        "\n",
        "  def call(self, x, training: bool = None):\n",
        "    x = self.rescale(x)\n",
        "    x = self.resize(x)\n",
        "    if self.preprocess:\n",
        "      for aug_i in self.augs:\n",
        "        x = aug_i(x, training=training)\n",
        "    x = self.pool2(self.conv2(self.conv1(x)))\n",
        "    x = self.drop2(x, training=training)\n",
        "    x = self.pool4(self.conv4(self.conv3(x)))\n",
        "    x = self.drop4(x, training=training)\n",
        "    x = self.dens5(self.flat5(x))\n",
        "    x = self.drop5(x, training=training)\n",
        "    x = self.dens6(x)\n",
        "    return x\n",
        "  \n",
        "  # https://stackoverflow.com/questions/55235212\n",
        "  def model(self, input_shape=None):\n",
        "    if input_shape is None:\n",
        "      x = Input(shape=(self.image_height, self.image_width, 3))\n",
        "    else:\n",
        "      x = Input(shape=input_shape)\n",
        "    return Model(inputs=[x], outputs=self.call(x))\n",
        "  \n",
        "  def get_config(self):\n",
        "    return {\"num_classes\": self.num_classes,\n",
        "            \"image_height\": self.image_height,\n",
        "            \"image_width\": self.image_width,\n",
        "            \"preprocess\": self.preprocess}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGHzfefDfO5z"
      },
      "source": [
        "##VIT\n",
        "\n",
        "[Vision Transformer](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy et al.  \n",
        "Code taken from this [example](https://keras.io/examples/vision/image_classification_with_vision_transformer/) from the official keras website."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfUPza88W8GY"
      },
      "source": [
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (\n",
        "  Layer,\n",
        "  Add,\n",
        "  BatchNormalization, \n",
        "  Conv2D, \n",
        "  Dense,\n",
        "  Dropout,\n",
        "  Embedding,\n",
        "  Flatten,\n",
        "  LayerNormalization,\n",
        "  MaxPool2D,\n",
        "  MultiHeadAttention\n",
        ")\n",
        "from tensorflow.keras.initializers import TruncatedNormal, Constant, Zeros\n",
        "from tensorflow.keras.regularizers import L2\n",
        "from tensorflow.keras.activations import relu, gelu\n",
        "from tensorflow.keras.layers.experimental.preprocessing import (\n",
        "  Normalization,\n",
        "  RandomFlip,\n",
        "  RandomRotation,\n",
        "  RandomZoom,\n",
        "  Rescaling,\n",
        "  Resizing,\n",
        ")\n",
        "\n",
        "def mlp(hidden_units: list, dropout_rate: float, name: str = ''):\n",
        "  \"\"\"Multilayer perceptron.\"\"\"\n",
        "  layers = []\n",
        "  for idx, units in enumerate(hidden_units):\n",
        "    layers.append(Dense(units, activation=gelu, name=f'{name}/dens{idx}'))\n",
        "    layers.append(Dropout(dropout_rate, name=f'{name}/drop{idx}'))\n",
        "  return layers\n",
        "\n",
        "class MLP(Layer):\n",
        "  \"\"\"Multilayer perceptron.\"\"\"\n",
        "  def __init__(self, hidden_units: list, dropout_rate: float):\n",
        "    super().__init__()\n",
        "    self.hidden_units = hidden_units\n",
        "    self.dropout_rate = dropout_rate\n",
        "    \n",
        "    self.layers = []\n",
        "    self.dropouts = []\n",
        "    for idx, units in enumerate(self.hidden_units):\n",
        "      self.layers.append(Dense(units, activation=gelu, name=f'dens{idx}'))\n",
        "      self.dropouts.append(Dropout(dropout_rate))\n",
        "\n",
        "  def call(self, x, training: bool = None):\n",
        "    for layer_i, dropout_i in zip(self.layers, self.dropouts):\n",
        "        x = layer_i(x)\n",
        "        x = dropout_i(x, training=training)\n",
        "    return x\n",
        "  \n",
        "  def get_config(self):\n",
        "    return {\"hidden_units\": self.hidden_units,\n",
        "            \"dropout_rate\": self.dropout_rate}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n",
        "\n",
        "\n",
        "class PatchEncoder(Layer):\n",
        "  \"\"\"Encoder for the image patches.\"\"\"\n",
        "  def __init__(self, num_patches: int, projection_dim: int):\n",
        "    super().__init__()\n",
        "    self.num_patches = num_patches\n",
        "    self.projection_dim = projection_dim\n",
        "    self.projection = Dense(units=self.projection_dim)\n",
        "    self.position_embedding = Embedding(input_dim=num_patches,\n",
        "                                        output_dim=self.projection_dim)\n",
        "\n",
        "  def call(self, x):\n",
        "    positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "    encoded = self.projection(x) + self.position_embedding(positions)\n",
        "    return encoded\n",
        "  \n",
        "  def get_config(self):\n",
        "    return {\"num_patches\": self.num_patches,\n",
        "            \"projection_dim\": self.projection_dim}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n",
        "\n",
        "\n",
        "class VIT(Model):\n",
        "  \"\"\"Vision Transformer implementation.\n",
        "  \n",
        "  Changes compared to the original model:\n",
        "  - maxpool2d uses a kernel_size=3, stride=2.\n",
        "  \n",
        "  \"\"\"\n",
        "  def __init__(self, \n",
        "               num_classes: int, \n",
        "               image_height: int, \n",
        "               image_width: int,\n",
        "               patch_size: int, \n",
        "               projection_dim: int,\n",
        "               num_layers: int,\n",
        "               num_heads: int,\n",
        "               mlp_dims: list,\n",
        "               classifier_mlp_dims: list,\n",
        "               preprocess: bool = False):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.num_classes = num_classes\n",
        "    self.image_height = image_height\n",
        "    self.image_width = image_width\n",
        "    self.patch_size = patch_size\n",
        "    self.projection_dim = projection_dim\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.mlp_dims = mlp_dims\n",
        "    self.classifier_mlp_dims = classifier_mlp_dims\n",
        "    self.preprocess = preprocess\n",
        "\n",
        "    self.rescale = Rescaling(1./255., name='rescale')\n",
        "    self.resize = Resizing(self.image_height, self.image_width, name='resize')\n",
        "\n",
        "    # Preprocessing\n",
        "    self.augs = [\n",
        "      RandomFlip(\"horizontal\", name='data_aug_flip'),\n",
        "      # RandomTranslation((-0.1, 0.1), (-0.1, 0.1), name='data_aug2'),\n",
        "      RandomRotation(0.02, name='data_aug_rot'),\n",
        "      RandomZoom(0.2, 0.2, name='data_aug_zoom'),\n",
        "    ]\n",
        "\n",
        "    # Patches\n",
        "    self.patch = Patches(self.patch_size)\n",
        "    num_patches = (self.image_height // self.patch_size) * \\\n",
        "                  (self.image_width // self.patch_size)\n",
        "    self.patch_enc = PatchEncoder(num_patches, self.projection_dim)\n",
        "\n",
        "    # Model\n",
        "    self.transformer_blocks = []\n",
        "    for idx in range(self.num_layers):\n",
        "      block = []\n",
        "      block += [LayerNormalization(epsilon=1e-6, name=f'ln{idx + 1}_1')]\n",
        "      block += [MultiHeadAttention(self.num_heads, \n",
        "                                   self.projection_dim, \n",
        "                                   dropout=0.1, \n",
        "                                   name=f'mha{idx + 1}')]\n",
        "      block += [Add(name=f'skip{idx + 1}_1')]\n",
        "      block += [LayerNormalization(epsilon=1e-6, name=f'ln{idx + 1}_2')]\n",
        "      block += [mlp(mlp_dims, dropout_rate=0.1, name=f'mlp{idx + 1}')]\n",
        "      block += [Add(name=f'skip{idx + 1}_2')]\n",
        "      self.transformer_blocks.append(block)\n",
        "\n",
        "    self.lnorm = LayerNormalization(epsilon=1e-6, name='classifier_ln')\n",
        "    self.flat = Flatten(name='classifier_flat')\n",
        "    self.drop = Dropout(0.5, name='classifier_drop')\n",
        "    self.mlp = mlp(self.classifier_mlp_dims, dropout_rate=0.5, \n",
        "                   name='classifier_mlp')\n",
        "    self.logits = Dense(self.num_classes, name='logits')\n",
        "\n",
        "  def call(self, x, training: bool = None):\n",
        "    x = self.rescale(x)\n",
        "    x = self.resize(x)\n",
        "\n",
        "    if self.preprocess:\n",
        "      for aug_i in self.augs:\n",
        "        x = aug_i(x, training=training)\n",
        "\n",
        "    x = self.patch(x)\n",
        "    x = self.patch_enc(x)\n",
        "    \n",
        "    for block in self.transformer_blocks:\n",
        "      x_layer_norm_1 = block[0](x)\n",
        "      x_attention    = block[1](x_layer_norm_1, x_layer_norm_1)\n",
        "      x_skip         = block[2]([x_attention, x])\n",
        "      x_layer_norm_2 = block[3](x_skip)\n",
        "      x_mlp          = self._iterate_mlp(block[4], x_layer_norm_2)\n",
        "      x              = block[5]([x_mlp, x_skip])\n",
        "    \n",
        "    x = self.lnorm(x)\n",
        "    x = self.flat(x)\n",
        "    x = self.drop(x, training=training)\n",
        "    x = self._iterate_mlp(self.mlp, x)\n",
        "    x = self.logits(x)\n",
        "    return x\n",
        "  \n",
        "  @staticmethod\n",
        "  def _iterate_mlp(mlp_layers: list, x: tf.Tensor):\n",
        "    _x = x\n",
        "    for mlp_i in mlp_layers:\n",
        "      _x = mlp_i(_x)\n",
        "    return _x\n",
        "\n",
        "  # https://stackoverflow.com/questions/55235212\n",
        "  def model(self, input_shape=None):\n",
        "    if input_shape is None:\n",
        "      x = Input(shape=(self.image_height, self.image_width, 3))\n",
        "    else:\n",
        "      x = Input(shape=input_shape)\n",
        "    return Model(inputs=[x], outputs=self.call(x))\n",
        "  \n",
        "  def get_config(self):\n",
        "      return {\"num_classes\": self.num_classes,\n",
        "              \"image_height\": self.image_height,\n",
        "              \"image_width\": self.image_width,\n",
        "              \"patch_size\": self.patch_size,\n",
        "              \"projection_dim\": self.projection_dim,\n",
        "              \"num_layers\": self.num_layers,\n",
        "              \"num_heads\": self.num_heads,\n",
        "              \"mlp_dims\": self.mlp_dims,\n",
        "              \"classifier_mlp_dims\": self.classifier_mlp_dims,\n",
        "              \"preprocess\": self.preprocess}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "      return cls(**config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWcTLWi9KTFh"
      },
      "source": [
        "##LambdaNetworks\n",
        "\n",
        "[LambdaNetworks](https://arxiv.org/abs/2102.08602) from Irwan Bello.  \n",
        "\n",
        "The LambdaNets is based on [tfkeras.py](https://github.com/lucidrains/lambda-networks/blob/main/lambda_networks/tfkeras.py) from [lucidrains/lambda-networks](https://github.com/lucidrains/lambda-networks) and [lambda2d.py](https://github.com/g0lemXIV/LambdaNetworks/blob/main/lambda_layers/lambda2d.py) from [g0lemXIV/LambdaNetworks](https://github.com/g0lemXIV/LambdaNetworks) .  \n",
        "\n",
        "**Good to know points from the paper**\n",
        "*   The paper main introduces lambdalayer and uses it to replace the convs in ResNet to create LambdaResNet.\n",
        "*   LambdaResNet achieves high accuracy but is very slow (~7x slower).\n",
        "*   The main conclusion is that a hybrid model of conv + lambda has the best speed-accuracy tradeoff.\n",
        "*   The best hybrid model replaces only some blocks in C4 and all the blocks in C5.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-_CmWcIKSxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a60637c-f79b-417c-c146-f8e2d225ef1c"
      },
      "source": [
        "! pip install einops\n",
        "from einops.layers.tensorflow import Rearrange\n",
        "\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (\n",
        "  BatchNormalization, \n",
        "  Conv2D,\n",
        "  Conv3D\n",
        ")\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.activations import relu\n",
        "from tensorflow.keras.layers.experimental.preprocessing import (\n",
        "  Normalization,\n",
        "  RandomFlip,\n",
        "  RandomRotation,\n",
        "  RandomZoom,\n",
        "  Rescaling,\n",
        "  Resizing,\n",
        ")\n",
        "from tensorflow import einsum, meshgrid\n",
        "\n",
        "\n",
        "def conv2d(filters, kernel_size, strides=(1, 1), padding='valid', **kwargs):\n",
        "  return Conv2D(filters, \n",
        "                kernel_size, \n",
        "                strides, \n",
        "                padding=padding, \n",
        "                activation=relu,\n",
        "                **kwargs)\n",
        "  \n",
        "\n",
        "def maxpool2d(**kwargs):\n",
        "  return MaxPool2D([3, 3], 2, padding='valid', **kwargs)\n",
        "\n",
        "\n",
        "def calc_rel_pos(n: int):\n",
        "  \"\"\"Generates a relative position meshgrid.\n",
        "  \n",
        "  Args:\n",
        "    n: Size of the original meshgrid. Size = Height = Width.\n",
        "\n",
        "  Returns: \n",
        "    rel_pos: An array of [n*n, n*n, 2] with value range from [-n+1, n-1] to \n",
        "             [0, 2n-2].\n",
        "  \"\"\"\n",
        "  # [2, n, n]\n",
        "  pos = tf.stack(meshgrid(tf.range(n), tf.range(n), indexing = 'ij'))\n",
        "  # [n*n, 2], pos[n] = (i, j)\n",
        "  pos = Rearrange('n i j -> (i j) n')(pos)             \n",
        "  # [n*n, n*n, 2], rel_pos[n, m] = (rel_i, rel_j)\n",
        "  rel_pos = pos[None, :] - pos[:, None]                \n",
        "  # shift value range from [-n+1, n-1] to [0, 2n-2]\n",
        "  rel_pos += n - 1\n",
        "  # [n*n, n*n, 2]                      \n",
        "  return rel_pos\n",
        "\n",
        "\n",
        "class Lambda(Layer):\n",
        "  \"\"\"Lambda Networks implementation.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               output_dim: int,\n",
        "               k_dim: int = 16,\n",
        "               u_dim: int = 1,\n",
        "               num_heads: int = 4,\n",
        "               n_r_size: int = None,\n",
        "               local_contexts: bool = False,\n",
        "               batch_norm: bool = True,\n",
        "               **kwargs):\n",
        "    \"\"\"Constructor\n",
        "\n",
        "    Args:\n",
        "      output_dim: Output dimension of the layer or v_dim * `num_heads`.\n",
        "      k_dim: Dimension of key.\n",
        "      u_dim: Intra depth for multiquery heads.\n",
        "      num_heads: Number of heads for multiquery.\n",
        "      n_r_size: If `local_contexts=True` n = height*width of query,\n",
        "                else r = receptive field.\n",
        "      local_contexts: If True lambdaconv is used, \n",
        "                      else relative position embeddings are used.\n",
        "      batch_norm: Whether to apply batch norm to query and value after\n",
        "                  the linear projection.\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.output_dim = output_dim\n",
        "    self.u_dim = u_dim  # intra-depth dimension\n",
        "    self.num_heads = num_heads\n",
        "    self.n_r_size = n_r_size\n",
        "    self.local_contexts = local_contexts\n",
        "    self.batch_norm = batch_norm\n",
        "\n",
        "    assert (self.output_dim % self.num_heads) == 0, \\\n",
        "      '`output_dim` must be divisible by `num_heads` for multi-head query.'\n",
        "    self.v_dim = self.output_dim // self.num_heads\n",
        "    self.k_dim = k_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.to_q = Conv2D(self.k_dim * self.num_heads, 1, use_bias=self.batch_norm)\n",
        "    self.to_k = Conv2D(self.k_dim * self.u_dim, 1, use_bias=self.batch_norm)\n",
        "    self.to_v = Conv2D(self.v_dim * self.u_dim, 1, use_bias=self.batch_norm)\n",
        "\n",
        "    self.norm_q = BatchNormalization() if self.batch_norm else None\n",
        "    self.norm_v = BatchNormalization() if self.batch_norm else None\n",
        "\n",
        "    if self.local_contexts:\n",
        "      assert (self.n_r_size % 2) == 1, 'Receptive kernel size should be odd'\n",
        "      self.pos_conv = Conv3D(self.k_dim, \n",
        "                             (1, self.n_r_size, self.n_r_size), \n",
        "                             padding='same')\n",
        "    else:\n",
        "      assert n is not None, 'You must specify the window length (n = h = w)'\n",
        "      rel_length = 2 * self.n_r_size - 1\n",
        "      self.rel_pos_emb = self.add_weight(name='pos_emb',\n",
        "                                         shape=(rel_length, rel_length, \n",
        "                                                self.k_dim, self.u_dim),\n",
        "                                         initializer=RandomNormal,\n",
        "                                         trainable=True)\n",
        "      self.rel_pos = calc_rel_pos(self.n_r_size)\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Info on the notations for reference:\n",
        "    x = 2D Image data (Assumption)\n",
        "    q = query\n",
        "    k = key\n",
        "    v = value\n",
        "    h = number of heads for multiquery\n",
        "    u = intradepth\n",
        "    b = batch size\n",
        "    hh = height of input data\n",
        "    ww = width of input data\n",
        "    m = height * width of key / value.\n",
        "    n = height * width of query.\n",
        "    \"\"\"\n",
        "    b, hh, ww, c, u, h = *x.get_shape().as_list(), self.u_dim, self.num_heads\n",
        "\n",
        "    q = self.to_q(x)\n",
        "    k = self.to_k(x)\n",
        "    v = self.to_v(x)\n",
        "\n",
        "    if self.batch_norm:\n",
        "      q = self.norm_q(q)\n",
        "      v = self.norm_v(v)\n",
        "\n",
        "    q = Rearrange('b hh ww (h k) -> b h k (hh ww)', h=h)(q)\n",
        "    k = Rearrange('b hh ww (u k) -> b u k (hh ww)', u=u)(k)\n",
        "    v = Rearrange('b hh ww (u v) -> b u v (hh ww)', u=u)(v)\n",
        "\n",
        "    k = tf.nn.softmax(k)\n",
        "\n",
        "    Lc = einsum('b u k m, b u v m -> b k v', k, v)\n",
        "    Yc = einsum('b h k n, b k v -> b n h v', q, Lc)\n",
        "\n",
        "    if self.local_contexts:\n",
        "      # lambda convs, embedding is represented by the conv kernels.\n",
        "      v = Rearrange('b u v (hh ww) -> b v hh ww u', hh=hh, ww=ww)(v)\n",
        "      Lp = self.pos_conv(v)\n",
        "      Lp = Rearrange('b v h w k -> b v k (h w)')(Lp)\n",
        "      Yp = einsum('b h k n, b v k n -> b n h v', q, Lp)\n",
        "    else:\n",
        "      # relative position embedding.\n",
        "      rel_pos_emb = tf.gather_nd(self.rel_pos_emb, self.rel_pos)\n",
        "      Lp = einsum('n m k u, b u v m -> b n k v', rel_pos_emb, v)\n",
        "      Yp = einsum('b h k n, b n k v -> b n h v', q, Lp)\n",
        "\n",
        "    Y = Yc + Yp\n",
        "    out = Rearrange('b (hh ww) h v -> b hh ww (h v)', hh = hh, ww = ww)(Y)\n",
        "    return out\n",
        "\n",
        "  def get_config(self):\n",
        "    return {\"output_dim\": self.output_dim,\n",
        "            \"k_dim\": self.k_dim,\n",
        "            \"u_dim\": self.u_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"n_r_size\": self.n_r_size,\n",
        "            \"local_contexts\": self.local_contexts,\n",
        "            \"batch_norm\": self.batch_norm}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n",
        "\n",
        "\n",
        "class LambdaNetwork(Model):\n",
        "  \"\"\"LambdaNetworks implementation. \"\"\"\n",
        "  \n",
        "  def __init__(self, \n",
        "               num_classes: int, \n",
        "               image_height: int, \n",
        "               image_width: int,\n",
        "               k_dim: int = 16,\n",
        "               u_dim: int = 1,\n",
        "               num_heads: int = 4,\n",
        "               n_r_size: int = None,\n",
        "               local_contexts: bool = False,\n",
        "               preprocess: bool = False,\n",
        "               **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    \n",
        "    self.num_classes = num_classes\n",
        "    self.image_height = image_height\n",
        "    self.image_width = image_width\n",
        "    self.k_dim = k_dim\n",
        "    self.u_dim = u_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.n_r_size = n_r_size\n",
        "    self.local_contexts = local_contexts\n",
        "    self.preprocess = preprocess\n",
        "\n",
        "    self.rescale = Rescaling(1./255., name='rescale')\n",
        "    self.resize = Resizing(self.image_height, self.image_width, name='resize')\n",
        "\n",
        "    # Preprocessing\n",
        "    self.augs = [\n",
        "      RandomFlip(\"horizontal\", name='data_aug_flip'),\n",
        "      # RandomTranslation((-0.1, 0.1), (-0.1, 0.1), name='data_aug2'),\n",
        "      RandomRotation(0.02, name='data_aug_rot'),\n",
        "      RandomZoom(0.2, 0.2, name='data_aug_zoom'),\n",
        "    ]\n",
        "\n",
        "    # Model\n",
        "    self.conv1 = conv2d(32, 3, name='conv1', padding='same')\n",
        "    self.conv2 = conv2d(32, 3, name='conv2')\n",
        "    self.pool2 = maxpool2d(name='pool1')\n",
        "    self.drop2 = Dropout(0.5, name='drop2')\n",
        "\n",
        "    # self.lamb3 = Lambda(64, k_dim=8, u_dim=1, num_heads=2, n_r_size=14, \n",
        "    #                     local_contexts=False, batch_norm=False)\n",
        "    # self.lamb4 = Lambda(64, k_dim=8, u_dim=1, num_heads=2, n_r_size=14, \n",
        "    #                     local_contexts=False, batch_norm=False)\n",
        "    self.lamb3 = Lambda(64, k_dim=self.k_dim, u_dim=self.u_dim, \n",
        "                        num_heads=self.num_heads, n_r_size=self.n_r_size, \n",
        "                        local_contexts=self.local_contexts, batch_norm=False,\n",
        "                        name='lamb3')\n",
        "    self.lamb4 = Lambda(64, k_dim=self.k_dim, u_dim=self.u_dim, \n",
        "                        num_heads=self.num_heads, n_r_size=self.n_r_size, \n",
        "                        local_contexts=self.local_contexts, batch_norm=False,\n",
        "                        name='lamb4')\n",
        "    self.pool4 = maxpool2d(name='pool4')\n",
        "    self.drop4 = Dropout(0.25, name='drop4')\n",
        "\n",
        "    self.flat5 = Flatten(name='flat5')\n",
        "    self.dens5 = Dense(512, activation=relu, name='dens5')\n",
        "    self.drop5 = Dropout(0.5, name='drop5')\n",
        "    self.logits = Dense(self.num_classes, name='logits')\n",
        "\n",
        "  def call(self, x, training: bool = None):\n",
        "    x = self.rescale(x)\n",
        "    x = self.resize(x)\n",
        "\n",
        "    if self.preprocess:\n",
        "      for aug_i in self.augs:\n",
        "        x = aug_i(x, training=training)\n",
        "\n",
        "    x = self.pool2(self.conv2(self.conv1(x)))\n",
        "    x = self.drop2(x, training=training)\n",
        "\n",
        "    x = self.pool4(self.lamb4(self.lamb3(x)))\n",
        "    x = self.drop4(x, training=training)\n",
        "\n",
        "    x = self.dens5(self.flat5(x))\n",
        "    x = self.drop5(x, training=training)\n",
        "    x = self.logits(x)\n",
        "\n",
        "    return x\n",
        "  \n",
        "  # https://stackoverflow.com/questions/55235212\n",
        "  def model(self, input_shape=None):\n",
        "    if input_shape is None:\n",
        "      x = Input(shape=(self.image_height, self.image_width, 3))\n",
        "    else:\n",
        "      x = Input(shape=input_shape)\n",
        "    return Model(inputs=[x], outputs=self.call(x))\n",
        "  \n",
        "  def get_config(self):\n",
        "      return {\"num_classes\": self.num_classes,\n",
        "              \"image_height\": self.image_height,\n",
        "              \"image_width\": self.image_width,\n",
        "              \"patch_size\": self.patch_size,\n",
        "              \"projection_dim\": self.projection_dim,\n",
        "              \"num_layers\": self.num_layers,\n",
        "              \"num_heads\": self.num_heads,\n",
        "              \"mlp_dims\": self.mlp_dims,\n",
        "              \"classifier_mlp_dims\": self.classifier_mlp_dims,\n",
        "              \"preprocess\": self.preprocess}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "      return cls(**config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "470J_-chCJN-"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A3t5koq9X5x"
      },
      "source": [
        "Training configs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0rfLzW6CKvI"
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "acc_metric_fn = tf.keras.metrics.SparseCategoricalAccuracy\n",
        "batch_size = 256\n",
        "shuffle_buffer = 50000\n",
        "epochs = 50\n",
        "learning_rate = 1e-3\n",
        "\n",
        "num_classes = 10 \n",
        "image_height = 32\n",
        "image_width = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK_xHPHD81nx"
      },
      "source": [
        "Keras model.compile(...) and model.fit(...) .\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x83Opo2GCQWY"
      },
      "source": [
        "def train_and_eval(_model, _checkpoint_filepath, verbose=1):\n",
        "  \"\"\"Wrapper code for training and evaluating.\n",
        "\n",
        "  Args:\n",
        "      _model: A keras Model.\n",
        "      _checkpoint_filepath: Path to save a checkpoint.\n",
        "      verbose: Option for logging output during train and eval.\n",
        "\n",
        "  Returns:\n",
        "      A history instance that contains logged values per epoch.\n",
        "\n",
        "  \"\"\"\n",
        "  _model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "    loss=loss_fn,\n",
        "    metrics=[acc_metric_fn(name='sparse_categorical_accuracy')]\n",
        "  )\n",
        "\n",
        "  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    _checkpoint_filepath,\n",
        "    monitor=\"val_sparse_categorical_accuracy\",\n",
        "    save_best_only=True,\n",
        "  )\n",
        "\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=os.path.join(_checkpoint_filepath, 'logs'), \n",
        "    histogram_freq=epochs//10,\n",
        "    update_freq='epoch'\n",
        "  )\n",
        "\n",
        "  start = time()\n",
        "  history = _model.fit(\n",
        "      data_generator('train', batch_size, shuffle_buffer), \n",
        "      epochs=epochs, \n",
        "      steps_per_epoch=len(data_generator('train', batch_size, shuffle_buffer)),\n",
        "      validation_data=data_generator('val', batch_size),\n",
        "      callbacks=[tensorboard_callback, checkpoint_callback],\n",
        "      verbose=verbose\n",
        "  )\n",
        "  end = time()\n",
        "  print(f'Total training time {end - start} seconds')\n",
        "  \n",
        "  # Save history output, should be the same as the tensorboard logs.\n",
        "  np.save(os.path.join(_checkpoint_filepath, 'history.npy'), history.history)\n",
        "  \n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n8bpH3YC3jZ"
      },
      "source": [
        "# Loss and accuracy plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYmEyrccDQBr"
      },
      "source": [
        "def plot(losses: list, \n",
        "         accuracies: list, \n",
        "         legend_labels: list, \n",
        "         subplot_title: list):\n",
        "  \n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,5))\n",
        "  \n",
        "  for x, ll in zip(losses, legend_labels):\n",
        "    c = ax1.plot(x[0], label='Trn: ' + ll, linestyle='--')[0].get_c()\n",
        "    _ = ax1.plot(x[1], label='Val: ' + ll, linestyle='-', color=c)\n",
        "  ax1.set_xlabel('Epochs')\n",
        "  ax1.set_ylabel('Loss')\n",
        "  ax1.set_title(subplot_title[0])\n",
        "  ax1.legend()   \n",
        "\n",
        "  for x, ll in zip(accuracies, legend_labels):\n",
        "    c = ax2.plot(x[0], label='Trn: ' + ll, linestyle='--')[0].get_c()\n",
        "    _ = ax2.plot(x[1], label='Val: ' + ll, linestyle='-', color=c)\n",
        "  ax2.set_xlabel('Epochs')\n",
        "  ax2.set_ylabel('Accuracy')\n",
        "  ax2.set_title(subplot_title[1])\n",
        "  ax2.legend()   \n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F1flqO1FDl1"
      },
      "source": [
        "#Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiQV1JV7Oqj2"
      },
      "source": [
        "history = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBFk329fU0D1"
      },
      "source": [
        "##Exp 1\n",
        "Model : CNN  \n",
        "Preprocessing : False  \n",
        "Batch size : 256"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2h3TyCICccj",
        "outputId": "e168bb9f-40a2-4b1e-cdd7-b0124980b6ce"
      },
      "source": [
        "model = SmallCNN(num_classes=num_classes, \n",
        "                 image_height=image_height, \n",
        "                 image_width=image_width,\n",
        "                 preprocess=False).model()\n",
        "model.summary()\n",
        "history['CNN'] = train_and_eval(model, SAVE_PATH + '_CNN', verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv2D)               (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv4 (Conv2D)               (None, 12, 12, 64)        36928     \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               819712    \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 890,410\n",
            "Trainable params: 890,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 37s - loss: 1.7954 - sparse_categorical_accuracy: 0.3313 - val_loss: 1.4085 - val_sparse_categorical_accuracy: 0.4829\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 2/50\n",
            "196/196 - 3s - loss: 1.3658 - sparse_categorical_accuracy: 0.5078 - val_loss: 1.1837 - val_sparse_categorical_accuracy: 0.5817\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 3/50\n",
            "196/196 - 3s - loss: 1.1989 - sparse_categorical_accuracy: 0.5724 - val_loss: 1.0710 - val_sparse_categorical_accuracy: 0.6171\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 4/50\n",
            "196/196 - 3s - loss: 1.0995 - sparse_categorical_accuracy: 0.6114 - val_loss: 0.9693 - val_sparse_categorical_accuracy: 0.6599\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 5/50\n",
            "196/196 - 3s - loss: 1.0089 - sparse_categorical_accuracy: 0.6446 - val_loss: 0.8989 - val_sparse_categorical_accuracy: 0.6846\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 6/50\n",
            "196/196 - 3s - loss: 0.9364 - sparse_categorical_accuracy: 0.6704 - val_loss: 0.8413 - val_sparse_categorical_accuracy: 0.7025\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 7/50\n",
            "196/196 - 3s - loss: 0.8877 - sparse_categorical_accuracy: 0.6858 - val_loss: 0.7773 - val_sparse_categorical_accuracy: 0.7340\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 8/50\n",
            "196/196 - 3s - loss: 0.8473 - sparse_categorical_accuracy: 0.7012 - val_loss: 0.7501 - val_sparse_categorical_accuracy: 0.7418\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 9/50\n",
            "196/196 - 3s - loss: 0.8119 - sparse_categorical_accuracy: 0.7178 - val_loss: 0.7259 - val_sparse_categorical_accuracy: 0.7510\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 10/50\n",
            "196/196 - 3s - loss: 0.7727 - sparse_categorical_accuracy: 0.7302 - val_loss: 0.7068 - val_sparse_categorical_accuracy: 0.7549\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 11/50\n",
            "196/196 - 3s - loss: 0.7506 - sparse_categorical_accuracy: 0.7384 - val_loss: 0.6983 - val_sparse_categorical_accuracy: 0.7595\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 12/50\n",
            "196/196 - 3s - loss: 0.7369 - sparse_categorical_accuracy: 0.7407 - val_loss: 0.7324 - val_sparse_categorical_accuracy: 0.7487\n",
            "Epoch 13/50\n",
            "196/196 - 3s - loss: 0.7101 - sparse_categorical_accuracy: 0.7519 - val_loss: 0.6601 - val_sparse_categorical_accuracy: 0.7727\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 14/50\n",
            "196/196 - 3s - loss: 0.6904 - sparse_categorical_accuracy: 0.7557 - val_loss: 0.6570 - val_sparse_categorical_accuracy: 0.7723\n",
            "Epoch 15/50\n",
            "196/196 - 3s - loss: 0.6809 - sparse_categorical_accuracy: 0.7591 - val_loss: 0.6329 - val_sparse_categorical_accuracy: 0.7828\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 16/50\n",
            "196/196 - 3s - loss: 0.6566 - sparse_categorical_accuracy: 0.7684 - val_loss: 0.6404 - val_sparse_categorical_accuracy: 0.7739\n",
            "Epoch 17/50\n",
            "196/196 - 3s - loss: 0.6420 - sparse_categorical_accuracy: 0.7749 - val_loss: 0.6156 - val_sparse_categorical_accuracy: 0.7885\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 18/50\n",
            "196/196 - 3s - loss: 0.6339 - sparse_categorical_accuracy: 0.7751 - val_loss: 0.5998 - val_sparse_categorical_accuracy: 0.7903\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 19/50\n",
            "196/196 - 3s - loss: 0.6177 - sparse_categorical_accuracy: 0.7835 - val_loss: 0.6047 - val_sparse_categorical_accuracy: 0.7947\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 20/50\n",
            "196/196 - 3s - loss: 0.6010 - sparse_categorical_accuracy: 0.7873 - val_loss: 0.6112 - val_sparse_categorical_accuracy: 0.7874\n",
            "Epoch 21/50\n",
            "196/196 - 3s - loss: 0.6006 - sparse_categorical_accuracy: 0.7879 - val_loss: 0.6004 - val_sparse_categorical_accuracy: 0.7953\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 22/50\n",
            "196/196 - 3s - loss: 0.5811 - sparse_categorical_accuracy: 0.7955 - val_loss: 0.6070 - val_sparse_categorical_accuracy: 0.7916\n",
            "Epoch 23/50\n",
            "196/196 - 3s - loss: 0.5648 - sparse_categorical_accuracy: 0.7999 - val_loss: 0.6180 - val_sparse_categorical_accuracy: 0.7865\n",
            "Epoch 24/50\n",
            "196/196 - 3s - loss: 0.5659 - sparse_categorical_accuracy: 0.8014 - val_loss: 0.5936 - val_sparse_categorical_accuracy: 0.7969\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 25/50\n",
            "196/196 - 3s - loss: 0.5541 - sparse_categorical_accuracy: 0.8040 - val_loss: 0.5852 - val_sparse_categorical_accuracy: 0.7975\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 26/50\n",
            "196/196 - 3s - loss: 0.5341 - sparse_categorical_accuracy: 0.8133 - val_loss: 0.5847 - val_sparse_categorical_accuracy: 0.8020\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 27/50\n",
            "196/196 - 3s - loss: 0.5340 - sparse_categorical_accuracy: 0.8111 - val_loss: 0.5824 - val_sparse_categorical_accuracy: 0.7994\n",
            "Epoch 28/50\n",
            "196/196 - 3s - loss: 0.5250 - sparse_categorical_accuracy: 0.8164 - val_loss: 0.5780 - val_sparse_categorical_accuracy: 0.8030\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 29/50\n",
            "196/196 - 3s - loss: 0.5185 - sparse_categorical_accuracy: 0.8172 - val_loss: 0.5768 - val_sparse_categorical_accuracy: 0.8054\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 30/50\n",
            "196/196 - 3s - loss: 0.5109 - sparse_categorical_accuracy: 0.8181 - val_loss: 0.5960 - val_sparse_categorical_accuracy: 0.7988\n",
            "Epoch 31/50\n",
            "196/196 - 3s - loss: 0.5061 - sparse_categorical_accuracy: 0.8218 - val_loss: 0.5866 - val_sparse_categorical_accuracy: 0.8036\n",
            "Epoch 32/50\n",
            "196/196 - 3s - loss: 0.5042 - sparse_categorical_accuracy: 0.8195 - val_loss: 0.5842 - val_sparse_categorical_accuracy: 0.8053\n",
            "Epoch 33/50\n",
            "196/196 - 3s - loss: 0.4911 - sparse_categorical_accuracy: 0.8254 - val_loss: 0.5690 - val_sparse_categorical_accuracy: 0.8084\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 34/50\n",
            "196/196 - 3s - loss: 0.4948 - sparse_categorical_accuracy: 0.8230 - val_loss: 0.5765 - val_sparse_categorical_accuracy: 0.8072\n",
            "Epoch 35/50\n",
            "196/196 - 3s - loss: 0.4833 - sparse_categorical_accuracy: 0.8287 - val_loss: 0.5787 - val_sparse_categorical_accuracy: 0.8052\n",
            "Epoch 36/50\n",
            "196/196 - 3s - loss: 0.4797 - sparse_categorical_accuracy: 0.8310 - val_loss: 0.5587 - val_sparse_categorical_accuracy: 0.8116\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 37/50\n",
            "196/196 - 3s - loss: 0.4763 - sparse_categorical_accuracy: 0.8302 - val_loss: 0.5675 - val_sparse_categorical_accuracy: 0.8084\n",
            "Epoch 38/50\n",
            "196/196 - 3s - loss: 0.4700 - sparse_categorical_accuracy: 0.8348 - val_loss: 0.5534 - val_sparse_categorical_accuracy: 0.8142\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 39/50\n",
            "196/196 - 3s - loss: 0.4657 - sparse_categorical_accuracy: 0.8339 - val_loss: 0.5728 - val_sparse_categorical_accuracy: 0.8066\n",
            "Epoch 40/50\n",
            "196/196 - 3s - loss: 0.4603 - sparse_categorical_accuracy: 0.8364 - val_loss: 0.5899 - val_sparse_categorical_accuracy: 0.8035\n",
            "Epoch 41/50\n",
            "196/196 - 3s - loss: 0.4542 - sparse_categorical_accuracy: 0.8397 - val_loss: 0.5589 - val_sparse_categorical_accuracy: 0.8134\n",
            "Epoch 42/50\n",
            "196/196 - 3s - loss: 0.4453 - sparse_categorical_accuracy: 0.8397 - val_loss: 0.5912 - val_sparse_categorical_accuracy: 0.8104\n",
            "Epoch 43/50\n",
            "196/196 - 3s - loss: 0.4436 - sparse_categorical_accuracy: 0.8421 - val_loss: 0.5698 - val_sparse_categorical_accuracy: 0.8116\n",
            "Epoch 44/50\n",
            "196/196 - 3s - loss: 0.4408 - sparse_categorical_accuracy: 0.8421 - val_loss: 0.5802 - val_sparse_categorical_accuracy: 0.8096\n",
            "Epoch 45/50\n",
            "196/196 - 3s - loss: 0.4350 - sparse_categorical_accuracy: 0.8460 - val_loss: 0.5715 - val_sparse_categorical_accuracy: 0.8104\n",
            "Epoch 46/50\n",
            "196/196 - 3s - loss: 0.4295 - sparse_categorical_accuracy: 0.8468 - val_loss: 0.5623 - val_sparse_categorical_accuracy: 0.8162\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_CNN/assets\n",
            "Epoch 47/50\n",
            "196/196 - 3s - loss: 0.4285 - sparse_categorical_accuracy: 0.8486 - val_loss: 0.5642 - val_sparse_categorical_accuracy: 0.8123\n",
            "Epoch 48/50\n",
            "196/196 - 3s - loss: 0.4346 - sparse_categorical_accuracy: 0.8450 - val_loss: 0.5702 - val_sparse_categorical_accuracy: 0.8097\n",
            "Epoch 49/50\n",
            "196/196 - 3s - loss: 0.4242 - sparse_categorical_accuracy: 0.8482 - val_loss: 0.5697 - val_sparse_categorical_accuracy: 0.8102\n",
            "Epoch 50/50\n",
            "196/196 - 3s - loss: 0.4176 - sparse_categorical_accuracy: 0.8524 - val_loss: 0.5738 - val_sparse_categorical_accuracy: 0.8098\n",
            "Total training time 236.06853914260864 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XKFICEKCZxE"
      },
      "source": [
        "##Exp 2\n",
        "Model : VIT  \n",
        "Preprocessing : False  \n",
        "Batch size : 256  \n",
        "Patch : 4x4  \n",
        "Heads : 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lyrU-_sXgkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db2f4fb-fa2b-4901-80f3-6e1739205d92"
      },
      "source": [
        "# Saves a full copy of the model.\n",
        "model = VIT(num_classes=num_classes, \n",
        "            image_height=image_height, \n",
        "            image_width=image_width,\n",
        "            patch_size=4, \n",
        "            projection_dim=32,\n",
        "            num_layers=4,\n",
        "            num_heads=2,\n",
        "            mlp_dims=[64, 32],\n",
        "            classifier_mlp_dims=[512],\n",
        "            preprocess=False).model()\n",
        "model.summary()\n",
        "history['VIT'] = train_and_eval(model, SAVE_PATH + '_VIT_4x4_2hds', verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "patches_1 (Patches)             (None, None, 48)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "patch_encoder (PatchEncoder)    (None, 64, 32)       3616        patches_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "ln1_1 (LayerNormalization)      (None, 64, 32)       64          patch_encoder[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "mha1 (MultiHeadAttention)       (None, 64, 32)       8416        ln1_1[0][0]                      \n",
            "                                                                 ln1_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "skip1_1 (Add)                   (None, 64, 32)       0           mha1[0][0]                       \n",
            "                                                                 patch_encoder[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "ln1_2 (LayerNormalization)      (None, 64, 32)       64          skip1_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "mlp1/dens0 (Dense)              (None, 64, 64)       2112        ln1_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "mlp1/drop0 (Dropout)            (None, 64, 64)       0           mlp1/dens0[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "mlp1/dens1 (Dense)              (None, 64, 32)       2080        mlp1/drop0[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "mlp1/drop1 (Dropout)            (None, 64, 32)       0           mlp1/dens1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip1_2 (Add)                   (None, 64, 32)       0           mlp1/drop1[0][0]                 \n",
            "                                                                 skip1_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "ln2_1 (LayerNormalization)      (None, 64, 32)       64          skip1_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "mha2 (MultiHeadAttention)       (None, 64, 32)       8416        ln2_1[0][0]                      \n",
            "                                                                 ln2_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "skip2_1 (Add)                   (None, 64, 32)       0           mha2[0][0]                       \n",
            "                                                                 skip1_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "ln2_2 (LayerNormalization)      (None, 64, 32)       64          skip2_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "mlp2/dens0 (Dense)              (None, 64, 64)       2112        ln2_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "mlp2/drop0 (Dropout)            (None, 64, 64)       0           mlp2/dens0[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "mlp2/dens1 (Dense)              (None, 64, 32)       2080        mlp2/drop0[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "mlp2/drop1 (Dropout)            (None, 64, 32)       0           mlp2/dens1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip2_2 (Add)                   (None, 64, 32)       0           mlp2/drop1[0][0]                 \n",
            "                                                                 skip2_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "ln3_1 (LayerNormalization)      (None, 64, 32)       64          skip2_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "mha3 (MultiHeadAttention)       (None, 64, 32)       8416        ln3_1[0][0]                      \n",
            "                                                                 ln3_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "skip3_1 (Add)                   (None, 64, 32)       0           mha3[0][0]                       \n",
            "                                                                 skip2_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "ln3_2 (LayerNormalization)      (None, 64, 32)       64          skip3_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "mlp3/dens0 (Dense)              (None, 64, 64)       2112        ln3_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "mlp3/drop0 (Dropout)            (None, 64, 64)       0           mlp3/dens0[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "mlp3/dens1 (Dense)              (None, 64, 32)       2080        mlp3/drop0[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "mlp3/drop1 (Dropout)            (None, 64, 32)       0           mlp3/dens1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip3_2 (Add)                   (None, 64, 32)       0           mlp3/drop1[0][0]                 \n",
            "                                                                 skip3_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "ln4_1 (LayerNormalization)      (None, 64, 32)       64          skip3_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "mha4 (MultiHeadAttention)       (None, 64, 32)       8416        ln4_1[0][0]                      \n",
            "                                                                 ln4_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "skip4_1 (Add)                   (None, 64, 32)       0           mha4[0][0]                       \n",
            "                                                                 skip3_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "ln4_2 (LayerNormalization)      (None, 64, 32)       64          skip4_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "mlp4/dens0 (Dense)              (None, 64, 64)       2112        ln4_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "mlp4/drop0 (Dropout)            (None, 64, 64)       0           mlp4/dens0[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "mlp4/dens1 (Dense)              (None, 64, 32)       2080        mlp4/drop0[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "mlp4/drop1 (Dropout)            (None, 64, 32)       0           mlp4/dens1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip4_2 (Add)                   (None, 64, 32)       0           mlp4/drop1[0][0]                 \n",
            "                                                                 skip4_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "classifier_ln (LayerNormalizati (None, 64, 32)       64          skip4_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "classifier_flat (Flatten)       (None, 2048)         0           classifier_ln[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "classifier_drop (Dropout)       (None, 2048)         0           classifier_flat[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "classifier_mlp/dens0 (Dense)    (None, 512)          1049088     classifier_drop[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "classifier_mlp/drop0 (Dropout)  (None, 512)          0           classifier_mlp/dens0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 10)           5130        classifier_mlp/drop0[0][0]       \n",
            "==================================================================================================\n",
            "Total params: 1,108,842\n",
            "Trainable params: 1,108,842\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 12s - loss: 2.0692 - sparse_categorical_accuracy: 0.2570 - val_loss: 1.6679 - val_sparse_categorical_accuracy: 0.4042\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 6s - loss: 1.7072 - sparse_categorical_accuracy: 0.3775 - val_loss: 1.4654 - val_sparse_categorical_accuracy: 0.4720\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 6s - loss: 1.5587 - sparse_categorical_accuracy: 0.4312 - val_loss: 1.4093 - val_sparse_categorical_accuracy: 0.4961\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 6s - loss: 1.4578 - sparse_categorical_accuracy: 0.4725 - val_loss: 1.2855 - val_sparse_categorical_accuracy: 0.5354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 6s - loss: 1.3977 - sparse_categorical_accuracy: 0.4958 - val_loss: 1.2420 - val_sparse_categorical_accuracy: 0.5482\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 6s - loss: 1.3296 - sparse_categorical_accuracy: 0.5233 - val_loss: 1.1754 - val_sparse_categorical_accuracy: 0.5787\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 6s - loss: 1.2868 - sparse_categorical_accuracy: 0.5377 - val_loss: 1.1521 - val_sparse_categorical_accuracy: 0.5810\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 6s - loss: 1.2424 - sparse_categorical_accuracy: 0.5539 - val_loss: 1.1191 - val_sparse_categorical_accuracy: 0.5951\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 6s - loss: 1.2141 - sparse_categorical_accuracy: 0.5662 - val_loss: 1.0781 - val_sparse_categorical_accuracy: 0.6107\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 6s - loss: 1.1788 - sparse_categorical_accuracy: 0.5776 - val_loss: 1.0839 - val_sparse_categorical_accuracy: 0.6049\n",
            "Epoch 11/50\n",
            "196/196 - 6s - loss: 1.1511 - sparse_categorical_accuracy: 0.5890 - val_loss: 1.0549 - val_sparse_categorical_accuracy: 0.6226\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 6s - loss: 1.1217 - sparse_categorical_accuracy: 0.6014 - val_loss: 1.0272 - val_sparse_categorical_accuracy: 0.6289\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 6s - loss: 1.0942 - sparse_categorical_accuracy: 0.6103 - val_loss: 1.0334 - val_sparse_categorical_accuracy: 0.6219\n",
            "Epoch 14/50\n",
            "196/196 - 6s - loss: 1.0669 - sparse_categorical_accuracy: 0.6197 - val_loss: 1.0082 - val_sparse_categorical_accuracy: 0.6408\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 6s - loss: 1.0443 - sparse_categorical_accuracy: 0.6269 - val_loss: 0.9707 - val_sparse_categorical_accuracy: 0.6486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 6s - loss: 1.0287 - sparse_categorical_accuracy: 0.6345 - val_loss: 0.9649 - val_sparse_categorical_accuracy: 0.6513\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 - 6s - loss: 1.0009 - sparse_categorical_accuracy: 0.6431 - val_loss: 0.9537 - val_sparse_categorical_accuracy: 0.6600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 - 6s - loss: 0.9819 - sparse_categorical_accuracy: 0.6505 - val_loss: 0.9304 - val_sparse_categorical_accuracy: 0.6718\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 - 6s - loss: 0.9595 - sparse_categorical_accuracy: 0.6591 - val_loss: 0.9306 - val_sparse_categorical_accuracy: 0.6705\n",
            "Epoch 20/50\n",
            "196/196 - 6s - loss: 0.9577 - sparse_categorical_accuracy: 0.6577 - val_loss: 0.9258 - val_sparse_categorical_accuracy: 0.6742\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 6s - loss: 0.9310 - sparse_categorical_accuracy: 0.6702 - val_loss: 0.9180 - val_sparse_categorical_accuracy: 0.6723\n",
            "Epoch 22/50\n",
            "196/196 - 6s - loss: 0.9131 - sparse_categorical_accuracy: 0.6749 - val_loss: 0.8912 - val_sparse_categorical_accuracy: 0.6858\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 - 6s - loss: 0.8980 - sparse_categorical_accuracy: 0.6799 - val_loss: 0.8793 - val_sparse_categorical_accuracy: 0.6847\n",
            "Epoch 24/50\n",
            "196/196 - 6s - loss: 0.8820 - sparse_categorical_accuracy: 0.6843 - val_loss: 0.8743 - val_sparse_categorical_accuracy: 0.6910\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 - 6s - loss: 0.8638 - sparse_categorical_accuracy: 0.6905 - val_loss: 0.8763 - val_sparse_categorical_accuracy: 0.6867\n",
            "Epoch 26/50\n",
            "196/196 - 6s - loss: 0.8465 - sparse_categorical_accuracy: 0.6986 - val_loss: 0.8663 - val_sparse_categorical_accuracy: 0.6936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 - 6s - loss: 0.8361 - sparse_categorical_accuracy: 0.7020 - val_loss: 0.8728 - val_sparse_categorical_accuracy: 0.6889\n",
            "Epoch 28/50\n",
            "196/196 - 6s - loss: 0.8234 - sparse_categorical_accuracy: 0.7086 - val_loss: 0.8494 - val_sparse_categorical_accuracy: 0.6996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 - 6s - loss: 0.8061 - sparse_categorical_accuracy: 0.7129 - val_loss: 0.8624 - val_sparse_categorical_accuracy: 0.6981\n",
            "Epoch 30/50\n",
            "196/196 - 6s - loss: 0.7985 - sparse_categorical_accuracy: 0.7149 - val_loss: 0.8629 - val_sparse_categorical_accuracy: 0.6987\n",
            "Epoch 31/50\n",
            "196/196 - 6s - loss: 0.7852 - sparse_categorical_accuracy: 0.7209 - val_loss: 0.8379 - val_sparse_categorical_accuracy: 0.7024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32/50\n",
            "196/196 - 6s - loss: 0.7762 - sparse_categorical_accuracy: 0.7219 - val_loss: 0.8242 - val_sparse_categorical_accuracy: 0.7112\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33/50\n",
            "196/196 - 6s - loss: 0.7619 - sparse_categorical_accuracy: 0.7284 - val_loss: 0.8218 - val_sparse_categorical_accuracy: 0.7071\n",
            "Epoch 34/50\n",
            "196/196 - 6s - loss: 0.7564 - sparse_categorical_accuracy: 0.7330 - val_loss: 0.8533 - val_sparse_categorical_accuracy: 0.7015\n",
            "Epoch 35/50\n",
            "196/196 - 6s - loss: 0.7421 - sparse_categorical_accuracy: 0.7363 - val_loss: 0.8177 - val_sparse_categorical_accuracy: 0.7130\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36/50\n",
            "196/196 - 6s - loss: 0.7229 - sparse_categorical_accuracy: 0.7418 - val_loss: 0.8244 - val_sparse_categorical_accuracy: 0.7107\n",
            "Epoch 37/50\n",
            "196/196 - 6s - loss: 0.7133 - sparse_categorical_accuracy: 0.7472 - val_loss: 0.8130 - val_sparse_categorical_accuracy: 0.7123\n",
            "Epoch 38/50\n",
            "196/196 - 6s - loss: 0.7037 - sparse_categorical_accuracy: 0.7489 - val_loss: 0.7976 - val_sparse_categorical_accuracy: 0.7218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39/50\n",
            "196/196 - 6s - loss: 0.6902 - sparse_categorical_accuracy: 0.7547 - val_loss: 0.8009 - val_sparse_categorical_accuracy: 0.7187\n",
            "Epoch 40/50\n",
            "196/196 - 6s - loss: 0.6886 - sparse_categorical_accuracy: 0.7551 - val_loss: 0.8135 - val_sparse_categorical_accuracy: 0.7131\n",
            "Epoch 41/50\n",
            "196/196 - 6s - loss: 0.6731 - sparse_categorical_accuracy: 0.7602 - val_loss: 0.8010 - val_sparse_categorical_accuracy: 0.7212\n",
            "Epoch 42/50\n",
            "196/196 - 6s - loss: 0.6617 - sparse_categorical_accuracy: 0.7638 - val_loss: 0.7898 - val_sparse_categorical_accuracy: 0.7234\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43/50\n",
            "196/196 - 6s - loss: 0.6600 - sparse_categorical_accuracy: 0.7654 - val_loss: 0.8034 - val_sparse_categorical_accuracy: 0.7213\n",
            "Epoch 44/50\n",
            "196/196 - 6s - loss: 0.6512 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.7909 - val_sparse_categorical_accuracy: 0.7226\n",
            "Epoch 45/50\n",
            "196/196 - 6s - loss: 0.6384 - sparse_categorical_accuracy: 0.7721 - val_loss: 0.7869 - val_sparse_categorical_accuracy: 0.7250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 46/50\n",
            "196/196 - 6s - loss: 0.6362 - sparse_categorical_accuracy: 0.7748 - val_loss: 0.7808 - val_sparse_categorical_accuracy: 0.7279\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47/50\n",
            "196/196 - 6s - loss: 0.6303 - sparse_categorical_accuracy: 0.7764 - val_loss: 0.7727 - val_sparse_categorical_accuracy: 0.7299\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 48/50\n",
            "196/196 - 6s - loss: 0.6203 - sparse_categorical_accuracy: 0.7765 - val_loss: 0.7717 - val_sparse_categorical_accuracy: 0.7311\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 49/50\n",
            "196/196 - 6s - loss: 0.6104 - sparse_categorical_accuracy: 0.7819 - val_loss: 0.7695 - val_sparse_categorical_accuracy: 0.7333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50/50\n",
            "196/196 - 6s - loss: 0.6085 - sparse_categorical_accuracy: 0.7846 - val_loss: 0.7621 - val_sparse_categorical_accuracy: 0.7351\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 130). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_VIT_4x4_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total training time 545.2050430774689 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzOlQHR-GYWP"
      },
      "source": [
        "##Exp 3 - X Heads, Y Key dim\n",
        "Model : LAMBDA  \n",
        "Preprocessing : False  \n",
        "Batch size : 256  \n",
        "Key dim : 1,2,4,8  \n",
        "u dim : 1  \n",
        "Heads : 1,2,4,8  \n",
        "Pos. emb. size : 14  \n",
        "LambdaConv : False  \n",
        "\n",
        "---\n",
        "<pre>\n",
        "Q = k * hd       =  1, 2, 4, 8| 2, 4, 8,16| 4, 8,16,32| 8,16,32,64|\n",
        "K = k * u        =  1, 2, 4, 8| 1, 2, 4, 8| 1, 2, 4, 8| 1, 2, 4, 8|\n",
        "V = 64 // hd * u = 64,64,64,64|32,32,32,32|16,16,16,16| 8, 8, 8, 8|\n",
        "</pre>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8yHokOJbjoc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92926d48-7f3d-4f10-c21a-717bed42d40a"
      },
      "source": [
        "u_dim = 1\n",
        "for hds in [1,2,4,8]:\n",
        "  for k_dim in [1,2,4,8]:\n",
        "    model = LambdaNetwork(num_classes=num_classes, \n",
        "                          image_height=image_height, \n",
        "                          image_width=image_width,\n",
        "                          k_dim=k_dim,\n",
        "                          u_dim=u_dim,\n",
        "                          num_heads=hds,\n",
        "                          n_r_size=14,\n",
        "                          local_contexts=False,\n",
        "                          preprocess=False).model()\n",
        "    model.summary()\n",
        "    if 'LAMBDA' not in history:\n",
        "      history['LAMBDA'] = dict()\n",
        "    if hds not in history['LAMBDA']:\n",
        "      history['LAMBDA'][hds] = dict()\n",
        "    if u_dim not in history['LAMBDA'][hds]:\n",
        "      history['LAMBDA'][hds][u_dim] = dict()\n",
        "    history['LAMBDA'][hds][u_dim][k_dim]= train_and_eval(model, SAVE_PATH + f'_LAMBDA_{k_dim}kdim_{u_dim}udim_{hds}hds', verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        2841      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        4953      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,203,228\n",
            "Trainable params: 1,203,228\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 8s - loss: 2.0449 - sparse_categorical_accuracy: 0.2462 - val_loss: 1.8420 - val_sparse_categorical_accuracy: 0.3233\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 6s - loss: 1.6739 - sparse_categorical_accuracy: 0.3867 - val_loss: 1.6357 - val_sparse_categorical_accuracy: 0.4218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 6s - loss: 1.4889 - sparse_categorical_accuracy: 0.4623 - val_loss: 1.5121 - val_sparse_categorical_accuracy: 0.4676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 6s - loss: 1.3920 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.4084 - val_sparse_categorical_accuracy: 0.5061\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 6s - loss: 1.3131 - sparse_categorical_accuracy: 0.5318 - val_loss: 1.2645 - val_sparse_categorical_accuracy: 0.5508\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 6s - loss: 1.2381 - sparse_categorical_accuracy: 0.5597 - val_loss: 1.1662 - val_sparse_categorical_accuracy: 0.5865\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 6s - loss: 1.1785 - sparse_categorical_accuracy: 0.5841 - val_loss: 1.0993 - val_sparse_categorical_accuracy: 0.6105\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 6s - loss: 1.1314 - sparse_categorical_accuracy: 0.6008 - val_loss: 1.1828 - val_sparse_categorical_accuracy: 0.5859\n",
            "Epoch 9/50\n",
            "196/196 - 6s - loss: 1.0834 - sparse_categorical_accuracy: 0.6192 - val_loss: 0.9708 - val_sparse_categorical_accuracy: 0.6559\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 6s - loss: 1.0636 - sparse_categorical_accuracy: 0.6266 - val_loss: 0.9530 - val_sparse_categorical_accuracy: 0.6599\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 6s - loss: 1.0305 - sparse_categorical_accuracy: 0.6373 - val_loss: 0.9747 - val_sparse_categorical_accuracy: 0.6572\n",
            "Epoch 12/50\n",
            "196/196 - 6s - loss: 0.9996 - sparse_categorical_accuracy: 0.6498 - val_loss: 0.9076 - val_sparse_categorical_accuracy: 0.6815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 6s - loss: 0.9847 - sparse_categorical_accuracy: 0.6547 - val_loss: 0.9252 - val_sparse_categorical_accuracy: 0.6804\n",
            "Epoch 14/50\n",
            "196/196 - 6s - loss: 0.9635 - sparse_categorical_accuracy: 0.6646 - val_loss: 0.9097 - val_sparse_categorical_accuracy: 0.6837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 6s - loss: 0.9484 - sparse_categorical_accuracy: 0.6664 - val_loss: 0.9091 - val_sparse_categorical_accuracy: 0.6889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 6s - loss: 0.9334 - sparse_categorical_accuracy: 0.6746 - val_loss: 0.8693 - val_sparse_categorical_accuracy: 0.7020\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 - 6s - loss: 0.9230 - sparse_categorical_accuracy: 0.6769 - val_loss: 0.8577 - val_sparse_categorical_accuracy: 0.7070\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 - 6s - loss: 0.9045 - sparse_categorical_accuracy: 0.6849 - val_loss: 0.8720 - val_sparse_categorical_accuracy: 0.7028\n",
            "Epoch 19/50\n",
            "196/196 - 6s - loss: 0.8946 - sparse_categorical_accuracy: 0.6865 - val_loss: 0.9158 - val_sparse_categorical_accuracy: 0.6958\n",
            "Epoch 20/50\n",
            "196/196 - 6s - loss: 0.8856 - sparse_categorical_accuracy: 0.6917 - val_loss: 0.8203 - val_sparse_categorical_accuracy: 0.7200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 6s - loss: 0.8758 - sparse_categorical_accuracy: 0.6943 - val_loss: 0.8463 - val_sparse_categorical_accuracy: 0.7034\n",
            "Epoch 22/50\n",
            "196/196 - 6s - loss: 0.8627 - sparse_categorical_accuracy: 0.6988 - val_loss: 0.8103 - val_sparse_categorical_accuracy: 0.7166\n",
            "Epoch 23/50\n",
            "196/196 - 6s - loss: 0.8593 - sparse_categorical_accuracy: 0.6990 - val_loss: 0.9559 - val_sparse_categorical_accuracy: 0.6935\n",
            "Epoch 24/50\n",
            "196/196 - 6s - loss: 0.8463 - sparse_categorical_accuracy: 0.7036 - val_loss: 0.7711 - val_sparse_categorical_accuracy: 0.7276\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 - 6s - loss: 0.8420 - sparse_categorical_accuracy: 0.7058 - val_loss: 0.7994 - val_sparse_categorical_accuracy: 0.7302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26/50\n",
            "196/196 - 6s - loss: 0.8302 - sparse_categorical_accuracy: 0.7089 - val_loss: 0.7608 - val_sparse_categorical_accuracy: 0.7301\n",
            "Epoch 27/50\n",
            "196/196 - 6s - loss: 0.8218 - sparse_categorical_accuracy: 0.7137 - val_loss: 0.7697 - val_sparse_categorical_accuracy: 0.7286\n",
            "Epoch 28/50\n",
            "196/196 - 6s - loss: 0.8166 - sparse_categorical_accuracy: 0.7137 - val_loss: 0.8062 - val_sparse_categorical_accuracy: 0.7215\n",
            "Epoch 29/50\n",
            "196/196 - 6s - loss: 0.7997 - sparse_categorical_accuracy: 0.7196 - val_loss: 0.7559 - val_sparse_categorical_accuracy: 0.7410\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n",
            "196/196 - 6s - loss: 0.7967 - sparse_categorical_accuracy: 0.7195 - val_loss: 0.7696 - val_sparse_categorical_accuracy: 0.7360\n",
            "Epoch 31/50\n",
            "196/196 - 6s - loss: 0.7856 - sparse_categorical_accuracy: 0.7244 - val_loss: 0.7618 - val_sparse_categorical_accuracy: 0.7309\n",
            "Epoch 32/50\n",
            "196/196 - 6s - loss: 0.7783 - sparse_categorical_accuracy: 0.7270 - val_loss: 0.8010 - val_sparse_categorical_accuracy: 0.7297\n",
            "Epoch 33/50\n",
            "196/196 - 6s - loss: 0.7769 - sparse_categorical_accuracy: 0.7269 - val_loss: 0.7473 - val_sparse_categorical_accuracy: 0.7394\n",
            "Epoch 34/50\n",
            "196/196 - 6s - loss: 0.7690 - sparse_categorical_accuracy: 0.7310 - val_loss: 0.7781 - val_sparse_categorical_accuracy: 0.7422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 - 6s - loss: 0.7637 - sparse_categorical_accuracy: 0.7321 - val_loss: 0.7512 - val_sparse_categorical_accuracy: 0.7425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36/50\n",
            "196/196 - 6s - loss: 0.7536 - sparse_categorical_accuracy: 0.7356 - val_loss: 0.7786 - val_sparse_categorical_accuracy: 0.7521\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37/50\n",
            "196/196 - 6s - loss: 0.7537 - sparse_categorical_accuracy: 0.7366 - val_loss: 0.7858 - val_sparse_categorical_accuracy: 0.7483\n",
            "Epoch 38/50\n",
            "196/196 - 6s - loss: 0.7468 - sparse_categorical_accuracy: 0.7366 - val_loss: 0.8365 - val_sparse_categorical_accuracy: 0.7342\n",
            "Epoch 39/50\n",
            "196/196 - 6s - loss: 0.7353 - sparse_categorical_accuracy: 0.7413 - val_loss: 0.7008 - val_sparse_categorical_accuracy: 0.7562\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40/50\n",
            "196/196 - 6s - loss: 0.7269 - sparse_categorical_accuracy: 0.7462 - val_loss: 0.7066 - val_sparse_categorical_accuracy: 0.7544\n",
            "Epoch 41/50\n",
            "196/196 - 6s - loss: 0.7238 - sparse_categorical_accuracy: 0.7449 - val_loss: 0.7502 - val_sparse_categorical_accuracy: 0.7534\n",
            "Epoch 42/50\n",
            "196/196 - 6s - loss: 0.7188 - sparse_categorical_accuracy: 0.7485 - val_loss: 0.7275 - val_sparse_categorical_accuracy: 0.7551\n",
            "Epoch 43/50\n",
            "196/196 - 6s - loss: 0.7157 - sparse_categorical_accuracy: 0.7487 - val_loss: 0.7032 - val_sparse_categorical_accuracy: 0.7600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 44/50\n",
            "196/196 - 6s - loss: 0.7050 - sparse_categorical_accuracy: 0.7528 - val_loss: 0.6891 - val_sparse_categorical_accuracy: 0.7577\n",
            "Epoch 45/50\n",
            "196/196 - 6s - loss: 0.7049 - sparse_categorical_accuracy: 0.7524 - val_loss: 0.7701 - val_sparse_categorical_accuracy: 0.7519\n",
            "Epoch 46/50\n",
            "196/196 - 6s - loss: 0.6936 - sparse_categorical_accuracy: 0.7554 - val_loss: 0.7644 - val_sparse_categorical_accuracy: 0.7520\n",
            "Epoch 47/50\n",
            "196/196 - 6s - loss: 0.6983 - sparse_categorical_accuracy: 0.7555 - val_loss: 0.6840 - val_sparse_categorical_accuracy: 0.7630\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 48/50\n",
            "196/196 - 6s - loss: 0.6872 - sparse_categorical_accuracy: 0.7574 - val_loss: 0.6682 - val_sparse_categorical_accuracy: 0.7677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 49/50\n",
            "196/196 - 6s - loss: 0.6837 - sparse_categorical_accuracy: 0.7602 - val_loss: 0.6902 - val_sparse_categorical_accuracy: 0.7686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50/50\n",
            "196/196 - 6s - loss: 0.6759 - sparse_categorical_accuracy: 0.7630 - val_loss: 0.7365 - val_sparse_categorical_accuracy: 0.7542\n",
            "Total training time 373.8983519077301 seconds\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        3634      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        5810      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,204,878\n",
            "Trainable params: 1,204,878\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 9s - loss: 1.9876 - sparse_categorical_accuracy: 0.2702 - val_loss: 1.6805 - val_sparse_categorical_accuracy: 0.4049\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 7s - loss: 1.5890 - sparse_categorical_accuracy: 0.4264 - val_loss: 1.5203 - val_sparse_categorical_accuracy: 0.4767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 7s - loss: 1.4395 - sparse_categorical_accuracy: 0.4816 - val_loss: 1.3304 - val_sparse_categorical_accuracy: 0.5209\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 7s - loss: 1.3441 - sparse_categorical_accuracy: 0.5197 - val_loss: 1.2177 - val_sparse_categorical_accuracy: 0.5608\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 7s - loss: 1.2651 - sparse_categorical_accuracy: 0.5512 - val_loss: 1.1447 - val_sparse_categorical_accuracy: 0.5918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 7s - loss: 1.2081 - sparse_categorical_accuracy: 0.5705 - val_loss: 1.1208 - val_sparse_categorical_accuracy: 0.5940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 7s - loss: 1.1470 - sparse_categorical_accuracy: 0.5918 - val_loss: 1.0420 - val_sparse_categorical_accuracy: 0.6270\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 7s - loss: 1.1098 - sparse_categorical_accuracy: 0.6080 - val_loss: 1.0514 - val_sparse_categorical_accuracy: 0.6228\n",
            "Epoch 9/50\n",
            "196/196 - 7s - loss: 1.0873 - sparse_categorical_accuracy: 0.6142 - val_loss: 0.9903 - val_sparse_categorical_accuracy: 0.6480\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 7s - loss: 1.0636 - sparse_categorical_accuracy: 0.6248 - val_loss: 1.0056 - val_sparse_categorical_accuracy: 0.6456\n",
            "Epoch 11/50\n",
            "196/196 - 7s - loss: 1.0338 - sparse_categorical_accuracy: 0.6351 - val_loss: 0.9213 - val_sparse_categorical_accuracy: 0.6785\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 7s - loss: 1.0237 - sparse_categorical_accuracy: 0.6385 - val_loss: 1.0027 - val_sparse_categorical_accuracy: 0.6496\n",
            "Epoch 13/50\n",
            "196/196 - 7s - loss: 0.9982 - sparse_categorical_accuracy: 0.6476 - val_loss: 0.9078 - val_sparse_categorical_accuracy: 0.6818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 - 7s - loss: 0.9861 - sparse_categorical_accuracy: 0.6534 - val_loss: 0.8706 - val_sparse_categorical_accuracy: 0.6912\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 7s - loss: 0.9642 - sparse_categorical_accuracy: 0.6584 - val_loss: 0.8592 - val_sparse_categorical_accuracy: 0.6994\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 7s - loss: 0.9579 - sparse_categorical_accuracy: 0.6650 - val_loss: 0.8753 - val_sparse_categorical_accuracy: 0.6940\n",
            "Epoch 17/50\n",
            "196/196 - 7s - loss: 0.9442 - sparse_categorical_accuracy: 0.6682 - val_loss: 0.8528 - val_sparse_categorical_accuracy: 0.7031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 - 7s - loss: 0.9358 - sparse_categorical_accuracy: 0.6737 - val_loss: 0.8742 - val_sparse_categorical_accuracy: 0.6997\n",
            "Epoch 19/50\n",
            "196/196 - 7s - loss: 0.9226 - sparse_categorical_accuracy: 0.6761 - val_loss: 0.8455 - val_sparse_categorical_accuracy: 0.7001\n",
            "Epoch 20/50\n",
            "196/196 - 7s - loss: 0.9059 - sparse_categorical_accuracy: 0.6806 - val_loss: 0.8301 - val_sparse_categorical_accuracy: 0.7108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 7s - loss: 0.8921 - sparse_categorical_accuracy: 0.6904 - val_loss: 0.8317 - val_sparse_categorical_accuracy: 0.7099\n",
            "Epoch 22/50\n",
            "196/196 - 7s - loss: 0.8875 - sparse_categorical_accuracy: 0.6868 - val_loss: 0.8135 - val_sparse_categorical_accuracy: 0.7136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 - 7s - loss: 0.8799 - sparse_categorical_accuracy: 0.6899 - val_loss: 0.7889 - val_sparse_categorical_accuracy: 0.7176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 - 7s - loss: 0.8654 - sparse_categorical_accuracy: 0.6942 - val_loss: 0.7806 - val_sparse_categorical_accuracy: 0.7209\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 - 7s - loss: 0.8580 - sparse_categorical_accuracy: 0.6975 - val_loss: 0.8054 - val_sparse_categorical_accuracy: 0.7214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26/50\n",
            "196/196 - 7s - loss: 0.8496 - sparse_categorical_accuracy: 0.7023 - val_loss: 0.7997 - val_sparse_categorical_accuracy: 0.7232\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 - 7s - loss: 0.8385 - sparse_categorical_accuracy: 0.7067 - val_loss: 0.7754 - val_sparse_categorical_accuracy: 0.7329\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 - 7s - loss: 0.8370 - sparse_categorical_accuracy: 0.7074 - val_loss: 0.7670 - val_sparse_categorical_accuracy: 0.7309\n",
            "Epoch 29/50\n",
            "196/196 - 7s - loss: 0.8329 - sparse_categorical_accuracy: 0.7084 - val_loss: 0.7936 - val_sparse_categorical_accuracy: 0.7235\n",
            "Epoch 30/50\n",
            "196/196 - 7s - loss: 0.8211 - sparse_categorical_accuracy: 0.7104 - val_loss: 0.7715 - val_sparse_categorical_accuracy: 0.7312\n",
            "Epoch 31/50\n",
            "196/196 - 7s - loss: 0.8123 - sparse_categorical_accuracy: 0.7164 - val_loss: 0.7765 - val_sparse_categorical_accuracy: 0.7313\n",
            "Epoch 32/50\n",
            "196/196 - 7s - loss: 0.8062 - sparse_categorical_accuracy: 0.7174 - val_loss: 0.7688 - val_sparse_categorical_accuracy: 0.7293\n",
            "Epoch 33/50\n",
            "196/196 - 7s - loss: 0.8055 - sparse_categorical_accuracy: 0.7174 - val_loss: 0.7502 - val_sparse_categorical_accuracy: 0.7401\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "196/196 - 7s - loss: 0.7971 - sparse_categorical_accuracy: 0.7214 - val_loss: 0.7308 - val_sparse_categorical_accuracy: 0.7455\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 - 7s - loss: 0.7885 - sparse_categorical_accuracy: 0.7229 - val_loss: 0.7281 - val_sparse_categorical_accuracy: 0.7432\n",
            "Epoch 36/50\n",
            "196/196 - 7s - loss: 0.7774 - sparse_categorical_accuracy: 0.7268 - val_loss: 0.7333 - val_sparse_categorical_accuracy: 0.7488\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37/50\n",
            "196/196 - 7s - loss: 0.7733 - sparse_categorical_accuracy: 0.7285 - val_loss: 0.7370 - val_sparse_categorical_accuracy: 0.7414\n",
            "Epoch 38/50\n",
            "196/196 - 7s - loss: 0.7729 - sparse_categorical_accuracy: 0.7287 - val_loss: 0.7166 - val_sparse_categorical_accuracy: 0.7537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39/50\n",
            "196/196 - 7s - loss: 0.7603 - sparse_categorical_accuracy: 0.7341 - val_loss: 0.7280 - val_sparse_categorical_accuracy: 0.7521\n",
            "Epoch 40/50\n",
            "196/196 - 7s - loss: 0.7540 - sparse_categorical_accuracy: 0.7348 - val_loss: 0.7166 - val_sparse_categorical_accuracy: 0.7607\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 41/50\n",
            "196/196 - 7s - loss: 0.7456 - sparse_categorical_accuracy: 0.7383 - val_loss: 0.6952 - val_sparse_categorical_accuracy: 0.7592\n",
            "Epoch 42/50\n",
            "196/196 - 7s - loss: 0.7500 - sparse_categorical_accuracy: 0.7379 - val_loss: 0.6927 - val_sparse_categorical_accuracy: 0.7615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43/50\n",
            "196/196 - 7s - loss: 0.7384 - sparse_categorical_accuracy: 0.7381 - val_loss: 0.7107 - val_sparse_categorical_accuracy: 0.7593\n",
            "Epoch 44/50\n",
            "196/196 - 7s - loss: 0.7286 - sparse_categorical_accuracy: 0.7456 - val_loss: 0.7488 - val_sparse_categorical_accuracy: 0.7616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 45/50\n",
            "196/196 - 7s - loss: 0.7346 - sparse_categorical_accuracy: 0.7420 - val_loss: 0.7474 - val_sparse_categorical_accuracy: 0.7588\n",
            "Epoch 46/50\n",
            "196/196 - 7s - loss: 0.7319 - sparse_categorical_accuracy: 0.7433 - val_loss: 0.6966 - val_sparse_categorical_accuracy: 0.7657\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47/50\n",
            "196/196 - 7s - loss: 0.7265 - sparse_categorical_accuracy: 0.7461 - val_loss: 0.7039 - val_sparse_categorical_accuracy: 0.7575\n",
            "Epoch 48/50\n",
            "196/196 - 7s - loss: 0.7185 - sparse_categorical_accuracy: 0.7489 - val_loss: 0.7001 - val_sparse_categorical_accuracy: 0.7678\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 49/50\n",
            "196/196 - 7s - loss: 0.7136 - sparse_categorical_accuracy: 0.7479 - val_loss: 0.6878 - val_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 50/50\n",
            "196/196 - 7s - loss: 0.7107 - sparse_categorical_accuracy: 0.7499 - val_loss: 0.6642 - val_sparse_categorical_accuracy: 0.7687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, conv2d_8_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total training time 438.0464377403259 seconds\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        5220      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        7524      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,208,178\n",
            "Trainable params: 1,208,178\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 11s - loss: 1.9652 - sparse_categorical_accuracy: 0.2708 - val_loss: 1.7696 - val_sparse_categorical_accuracy: 0.3439\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 9s - loss: 1.6177 - sparse_categorical_accuracy: 0.4047 - val_loss: 1.5537 - val_sparse_categorical_accuracy: 0.4404\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 9s - loss: 1.4487 - sparse_categorical_accuracy: 0.4761 - val_loss: 1.3992 - val_sparse_categorical_accuracy: 0.4945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 9s - loss: 1.3256 - sparse_categorical_accuracy: 0.5209 - val_loss: 1.2746 - val_sparse_categorical_accuracy: 0.5417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 9s - loss: 1.2382 - sparse_categorical_accuracy: 0.5556 - val_loss: 1.1645 - val_sparse_categorical_accuracy: 0.5839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 9s - loss: 1.1795 - sparse_categorical_accuracy: 0.5811 - val_loss: 1.2360 - val_sparse_categorical_accuracy: 0.5708\n",
            "Epoch 7/50\n",
            "196/196 - 9s - loss: 1.1456 - sparse_categorical_accuracy: 0.5914 - val_loss: 1.1348 - val_sparse_categorical_accuracy: 0.5924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 9s - loss: 1.1000 - sparse_categorical_accuracy: 0.6101 - val_loss: 1.1102 - val_sparse_categorical_accuracy: 0.6021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 9s - loss: 1.0675 - sparse_categorical_accuracy: 0.6189 - val_loss: 1.0204 - val_sparse_categorical_accuracy: 0.6425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 9s - loss: 1.0387 - sparse_categorical_accuracy: 0.6301 - val_loss: 0.9925 - val_sparse_categorical_accuracy: 0.6437\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 9s - loss: 1.0148 - sparse_categorical_accuracy: 0.6407 - val_loss: 0.9094 - val_sparse_categorical_accuracy: 0.6750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 9s - loss: 1.0001 - sparse_categorical_accuracy: 0.6459 - val_loss: 0.9344 - val_sparse_categorical_accuracy: 0.6693\n",
            "Epoch 13/50\n",
            "196/196 - 9s - loss: 0.9844 - sparse_categorical_accuracy: 0.6519 - val_loss: 0.9579 - val_sparse_categorical_accuracy: 0.6630\n",
            "Epoch 14/50\n",
            "196/196 - 9s - loss: 0.9632 - sparse_categorical_accuracy: 0.6600 - val_loss: 0.9162 - val_sparse_categorical_accuracy: 0.6761\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 9s - loss: 0.9399 - sparse_categorical_accuracy: 0.6682 - val_loss: 0.8951 - val_sparse_categorical_accuracy: 0.6843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 9s - loss: 0.9269 - sparse_categorical_accuracy: 0.6719 - val_loss: 0.9088 - val_sparse_categorical_accuracy: 0.6780\n",
            "Epoch 17/50\n",
            "196/196 - 9s - loss: 0.9196 - sparse_categorical_accuracy: 0.6765 - val_loss: 0.8973 - val_sparse_categorical_accuracy: 0.6831\n",
            "Epoch 18/50\n",
            "196/196 - 9s - loss: 0.9108 - sparse_categorical_accuracy: 0.6797 - val_loss: 0.8658 - val_sparse_categorical_accuracy: 0.6961\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 - 9s - loss: 0.9004 - sparse_categorical_accuracy: 0.6832 - val_loss: 0.8930 - val_sparse_categorical_accuracy: 0.6835\n",
            "Epoch 20/50\n",
            "196/196 - 9s - loss: 0.8809 - sparse_categorical_accuracy: 0.6891 - val_loss: 0.8569 - val_sparse_categorical_accuracy: 0.7026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 9s - loss: 0.8750 - sparse_categorical_accuracy: 0.6917 - val_loss: 0.8809 - val_sparse_categorical_accuracy: 0.6999\n",
            "Epoch 22/50\n",
            "196/196 - 9s - loss: 0.8682 - sparse_categorical_accuracy: 0.6956 - val_loss: 0.8165 - val_sparse_categorical_accuracy: 0.7108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 - 9s - loss: 0.8479 - sparse_categorical_accuracy: 0.7011 - val_loss: 0.8266 - val_sparse_categorical_accuracy: 0.7119\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 - 9s - loss: 0.8407 - sparse_categorical_accuracy: 0.7033 - val_loss: 0.8057 - val_sparse_categorical_accuracy: 0.7199\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 - 9s - loss: 0.8314 - sparse_categorical_accuracy: 0.7064 - val_loss: 0.8161 - val_sparse_categorical_accuracy: 0.7171\n",
            "Epoch 26/50\n",
            "196/196 - 9s - loss: 0.8362 - sparse_categorical_accuracy: 0.7068 - val_loss: 0.7994 - val_sparse_categorical_accuracy: 0.7197\n",
            "Epoch 27/50\n",
            "196/196 - 9s - loss: 0.8169 - sparse_categorical_accuracy: 0.7133 - val_loss: 0.7951 - val_sparse_categorical_accuracy: 0.7200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 - 9s - loss: 0.8135 - sparse_categorical_accuracy: 0.7143 - val_loss: 0.7843 - val_sparse_categorical_accuracy: 0.7300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 - 9s - loss: 0.8083 - sparse_categorical_accuracy: 0.7155 - val_loss: 0.8025 - val_sparse_categorical_accuracy: 0.7285\n",
            "Epoch 30/50\n",
            "196/196 - 9s - loss: 0.7903 - sparse_categorical_accuracy: 0.7217 - val_loss: 0.7709 - val_sparse_categorical_accuracy: 0.7366\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 - 9s - loss: 0.7906 - sparse_categorical_accuracy: 0.7221 - val_loss: 0.7705 - val_sparse_categorical_accuracy: 0.7311\n",
            "Epoch 32/50\n",
            "196/196 - 9s - loss: 0.7800 - sparse_categorical_accuracy: 0.7244 - val_loss: 0.7712 - val_sparse_categorical_accuracy: 0.7343\n",
            "Epoch 33/50\n",
            "196/196 - 9s - loss: 0.7763 - sparse_categorical_accuracy: 0.7270 - val_loss: 0.7889 - val_sparse_categorical_accuracy: 0.7383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "196/196 - 9s - loss: 0.7750 - sparse_categorical_accuracy: 0.7243 - val_loss: 0.7843 - val_sparse_categorical_accuracy: 0.7272\n",
            "Epoch 35/50\n",
            "196/196 - 9s - loss: 0.7690 - sparse_categorical_accuracy: 0.7265 - val_loss: 0.7408 - val_sparse_categorical_accuracy: 0.7427\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36/50\n",
            "196/196 - 9s - loss: 0.7585 - sparse_categorical_accuracy: 0.7347 - val_loss: 0.7804 - val_sparse_categorical_accuracy: 0.7288\n",
            "Epoch 37/50\n",
            "196/196 - 9s - loss: 0.7565 - sparse_categorical_accuracy: 0.7330 - val_loss: 0.7397 - val_sparse_categorical_accuracy: 0.7464\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 - 9s - loss: 0.7457 - sparse_categorical_accuracy: 0.7367 - val_loss: 0.7836 - val_sparse_categorical_accuracy: 0.7371\n",
            "Epoch 39/50\n",
            "196/196 - 9s - loss: 0.7463 - sparse_categorical_accuracy: 0.7359 - val_loss: 0.7328 - val_sparse_categorical_accuracy: 0.7522\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40/50\n",
            "196/196 - 9s - loss: 0.7363 - sparse_categorical_accuracy: 0.7404 - val_loss: 0.7539 - val_sparse_categorical_accuracy: 0.7520\n",
            "Epoch 41/50\n",
            "196/196 - 9s - loss: 0.7405 - sparse_categorical_accuracy: 0.7387 - val_loss: 0.7066 - val_sparse_categorical_accuracy: 0.7575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 42/50\n",
            "196/196 - 9s - loss: 0.7256 - sparse_categorical_accuracy: 0.7439 - val_loss: 0.7584 - val_sparse_categorical_accuracy: 0.7563\n",
            "Epoch 43/50\n",
            "196/196 - 9s - loss: 0.7216 - sparse_categorical_accuracy: 0.7449 - val_loss: 0.7194 - val_sparse_categorical_accuracy: 0.7484\n",
            "Epoch 44/50\n",
            "196/196 - 9s - loss: 0.7180 - sparse_categorical_accuracy: 0.7448 - val_loss: 0.7190 - val_sparse_categorical_accuracy: 0.7496\n",
            "Epoch 45/50\n",
            "196/196 - 9s - loss: 0.7161 - sparse_categorical_accuracy: 0.7497 - val_loss: 0.7314 - val_sparse_categorical_accuracy: 0.7578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 46/50\n",
            "196/196 - 9s - loss: 0.7093 - sparse_categorical_accuracy: 0.7511 - val_loss: 0.7389 - val_sparse_categorical_accuracy: 0.7524\n",
            "Epoch 47/50\n",
            "196/196 - 9s - loss: 0.7022 - sparse_categorical_accuracy: 0.7533 - val_loss: 0.7355 - val_sparse_categorical_accuracy: 0.7517\n",
            "Epoch 48/50\n",
            "196/196 - 9s - loss: 0.7091 - sparse_categorical_accuracy: 0.7511 - val_loss: 0.7049 - val_sparse_categorical_accuracy: 0.7558\n",
            "Epoch 49/50\n",
            "196/196 - 9s - loss: 0.6967 - sparse_categorical_accuracy: 0.7563 - val_loss: 0.7199 - val_sparse_categorical_accuracy: 0.7562\n",
            "Epoch 50/50\n",
            "196/196 - 9s - loss: 0.6901 - sparse_categorical_accuracy: 0.7571 - val_loss: 0.7694 - val_sparse_categorical_accuracy: 0.7466\n",
            "Total training time 501.1399636268616 seconds\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        8392      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        10952     \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,214,778\n",
            "Trainable params: 1,214,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 14s - loss: 2.0332 - sparse_categorical_accuracy: 0.2380 - val_loss: 1.9016 - val_sparse_categorical_accuracy: 0.2888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 12s - loss: 1.6279 - sparse_categorical_accuracy: 0.4051 - val_loss: 1.5823 - val_sparse_categorical_accuracy: 0.4178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 12s - loss: 1.4496 - sparse_categorical_accuracy: 0.4709 - val_loss: 1.4704 - val_sparse_categorical_accuracy: 0.4672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 12s - loss: 1.3337 - sparse_categorical_accuracy: 0.5185 - val_loss: 1.4524 - val_sparse_categorical_accuracy: 0.4869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 12s - loss: 1.2452 - sparse_categorical_accuracy: 0.5530 - val_loss: 1.3499 - val_sparse_categorical_accuracy: 0.5268\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 12s - loss: 1.1798 - sparse_categorical_accuracy: 0.5813 - val_loss: 1.0985 - val_sparse_categorical_accuracy: 0.6089\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 12s - loss: 1.1292 - sparse_categorical_accuracy: 0.5991 - val_loss: 1.0452 - val_sparse_categorical_accuracy: 0.6342\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 12s - loss: 1.0885 - sparse_categorical_accuracy: 0.6170 - val_loss: 0.9866 - val_sparse_categorical_accuracy: 0.6533\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 12s - loss: 1.0484 - sparse_categorical_accuracy: 0.6294 - val_loss: 0.9540 - val_sparse_categorical_accuracy: 0.6657\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 12s - loss: 1.0190 - sparse_categorical_accuracy: 0.6417 - val_loss: 0.9658 - val_sparse_categorical_accuracy: 0.6668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 12s - loss: 1.0053 - sparse_categorical_accuracy: 0.6433 - val_loss: 0.9161 - val_sparse_categorical_accuracy: 0.6778\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 12s - loss: 0.9872 - sparse_categorical_accuracy: 0.6537 - val_loss: 1.0110 - val_sparse_categorical_accuracy: 0.6523\n",
            "Epoch 13/50\n",
            "196/196 - 12s - loss: 0.9636 - sparse_categorical_accuracy: 0.6609 - val_loss: 0.8616 - val_sparse_categorical_accuracy: 0.6995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 - 12s - loss: 0.9436 - sparse_categorical_accuracy: 0.6669 - val_loss: 0.9228 - val_sparse_categorical_accuracy: 0.6847\n",
            "Epoch 15/50\n",
            "196/196 - 12s - loss: 0.9171 - sparse_categorical_accuracy: 0.6764 - val_loss: 0.8792 - val_sparse_categorical_accuracy: 0.6918\n",
            "Epoch 16/50\n",
            "196/196 - 12s - loss: 0.9078 - sparse_categorical_accuracy: 0.6816 - val_loss: 0.9159 - val_sparse_categorical_accuracy: 0.6849\n",
            "Epoch 17/50\n",
            "196/196 - 12s - loss: 0.8939 - sparse_categorical_accuracy: 0.6868 - val_loss: 0.8480 - val_sparse_categorical_accuracy: 0.7045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 - 12s - loss: 0.8782 - sparse_categorical_accuracy: 0.6937 - val_loss: 0.7966 - val_sparse_categorical_accuracy: 0.7235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 - 12s - loss: 0.8736 - sparse_categorical_accuracy: 0.6921 - val_loss: 0.8599 - val_sparse_categorical_accuracy: 0.6948\n",
            "Epoch 20/50\n",
            "196/196 - 12s - loss: 0.8592 - sparse_categorical_accuracy: 0.6997 - val_loss: 0.8178 - val_sparse_categorical_accuracy: 0.7246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 12s - loss: 0.8483 - sparse_categorical_accuracy: 0.7022 - val_loss: 0.7883 - val_sparse_categorical_accuracy: 0.7297\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 - 12s - loss: 0.8280 - sparse_categorical_accuracy: 0.7095 - val_loss: 0.8215 - val_sparse_categorical_accuracy: 0.7162\n",
            "Epoch 23/50\n",
            "196/196 - 12s - loss: 0.8188 - sparse_categorical_accuracy: 0.7125 - val_loss: 0.7529 - val_sparse_categorical_accuracy: 0.7413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 - 12s - loss: 0.8211 - sparse_categorical_accuracy: 0.7122 - val_loss: 0.7410 - val_sparse_categorical_accuracy: 0.7412\n",
            "Epoch 25/50\n",
            "196/196 - 12s - loss: 0.8040 - sparse_categorical_accuracy: 0.7181 - val_loss: 0.7557 - val_sparse_categorical_accuracy: 0.7400\n",
            "Epoch 26/50\n",
            "196/196 - 12s - loss: 0.8044 - sparse_categorical_accuracy: 0.7200 - val_loss: 0.7628 - val_sparse_categorical_accuracy: 0.7355\n",
            "Epoch 27/50\n",
            "196/196 - 12s - loss: 0.7973 - sparse_categorical_accuracy: 0.7223 - val_loss: 0.7218 - val_sparse_categorical_accuracy: 0.7449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 - 12s - loss: 0.7843 - sparse_categorical_accuracy: 0.7239 - val_loss: 0.7408 - val_sparse_categorical_accuracy: 0.7413\n",
            "Epoch 29/50\n",
            "196/196 - 12s - loss: 0.7813 - sparse_categorical_accuracy: 0.7265 - val_loss: 0.7427 - val_sparse_categorical_accuracy: 0.7451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n",
            "196/196 - 12s - loss: 0.7800 - sparse_categorical_accuracy: 0.7285 - val_loss: 0.7293 - val_sparse_categorical_accuracy: 0.7492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 - 12s - loss: 0.7713 - sparse_categorical_accuracy: 0.7306 - val_loss: 0.7284 - val_sparse_categorical_accuracy: 0.7488\n",
            "Epoch 32/50\n",
            "196/196 - 12s - loss: 0.7523 - sparse_categorical_accuracy: 0.7364 - val_loss: 0.7463 - val_sparse_categorical_accuracy: 0.7496\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33/50\n",
            "196/196 - 12s - loss: 0.7641 - sparse_categorical_accuracy: 0.7354 - val_loss: 0.7153 - val_sparse_categorical_accuracy: 0.7555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "196/196 - 12s - loss: 0.7453 - sparse_categorical_accuracy: 0.7377 - val_loss: 0.7058 - val_sparse_categorical_accuracy: 0.7566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 - 12s - loss: 0.7449 - sparse_categorical_accuracy: 0.7395 - val_loss: 0.7174 - val_sparse_categorical_accuracy: 0.7536\n",
            "Epoch 36/50\n",
            "196/196 - 12s - loss: 0.7389 - sparse_categorical_accuracy: 0.7415 - val_loss: 0.7241 - val_sparse_categorical_accuracy: 0.7502\n",
            "Epoch 37/50\n",
            "196/196 - 12s - loss: 0.7409 - sparse_categorical_accuracy: 0.7388 - val_loss: 0.6946 - val_sparse_categorical_accuracy: 0.7586\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 - 12s - loss: 0.7254 - sparse_categorical_accuracy: 0.7463 - val_loss: 0.6897 - val_sparse_categorical_accuracy: 0.7614\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39/50\n",
            "196/196 - 12s - loss: 0.7237 - sparse_categorical_accuracy: 0.7462 - val_loss: 0.6871 - val_sparse_categorical_accuracy: 0.7641\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40/50\n",
            "196/196 - 12s - loss: 0.7130 - sparse_categorical_accuracy: 0.7486 - val_loss: 0.7101 - val_sparse_categorical_accuracy: 0.7547\n",
            "Epoch 41/50\n",
            "196/196 - 12s - loss: 0.7180 - sparse_categorical_accuracy: 0.7475 - val_loss: 0.6585 - val_sparse_categorical_accuracy: 0.7679\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 42/50\n",
            "196/196 - 12s - loss: 0.7067 - sparse_categorical_accuracy: 0.7491 - val_loss: 0.6928 - val_sparse_categorical_accuracy: 0.7618\n",
            "Epoch 43/50\n",
            "196/196 - 12s - loss: 0.7040 - sparse_categorical_accuracy: 0.7537 - val_loss: 0.7019 - val_sparse_categorical_accuracy: 0.7600\n",
            "Epoch 44/50\n",
            "196/196 - 12s - loss: 0.7043 - sparse_categorical_accuracy: 0.7539 - val_loss: 0.7190 - val_sparse_categorical_accuracy: 0.7638\n",
            "Epoch 45/50\n",
            "196/196 - 12s - loss: 0.7012 - sparse_categorical_accuracy: 0.7558 - val_loss: 0.6646 - val_sparse_categorical_accuracy: 0.7722\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 46/50\n",
            "196/196 - 12s - loss: 0.6938 - sparse_categorical_accuracy: 0.7550 - val_loss: 0.6673 - val_sparse_categorical_accuracy: 0.7745\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses, conv2d_19_layer_call_fn, conv2d_19_layer_call_and_return_conditional_losses, conv2d_20_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_1hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47/50\n",
            "196/196 - 12s - loss: 0.6929 - sparse_categorical_accuracy: 0.7577 - val_loss: 0.6675 - val_sparse_categorical_accuracy: 0.7686\n",
            "Epoch 48/50\n",
            "196/196 - 12s - loss: 0.6901 - sparse_categorical_accuracy: 0.7592 - val_loss: 0.7037 - val_sparse_categorical_accuracy: 0.7649\n",
            "Epoch 49/50\n",
            "196/196 - 12s - loss: 0.6816 - sparse_categorical_accuracy: 0.7603 - val_loss: 0.6537 - val_sparse_categorical_accuracy: 0.7732\n",
            "Epoch 50/50\n",
            "196/196 - 12s - loss: 0.6771 - sparse_categorical_accuracy: 0.7604 - val_loss: 0.6565 - val_sparse_categorical_accuracy: 0.7730\n",
            "Total training time 668.6443707942963 seconds\n",
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        1849      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        2969      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,200,252\n",
            "Trainable params: 1,200,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 7s - loss: 2.0534 - sparse_categorical_accuracy: 0.2404 - val_loss: 1.8585 - val_sparse_categorical_accuracy: 0.3205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 5s - loss: 1.6892 - sparse_categorical_accuracy: 0.3818 - val_loss: 1.6366 - val_sparse_categorical_accuracy: 0.3988\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 5s - loss: 1.5285 - sparse_categorical_accuracy: 0.4423 - val_loss: 1.5108 - val_sparse_categorical_accuracy: 0.4567\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 5s - loss: 1.4221 - sparse_categorical_accuracy: 0.4827 - val_loss: 1.4354 - val_sparse_categorical_accuracy: 0.4817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 5s - loss: 1.3357 - sparse_categorical_accuracy: 0.5177 - val_loss: 1.2844 - val_sparse_categorical_accuracy: 0.5378\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 5s - loss: 1.2667 - sparse_categorical_accuracy: 0.5443 - val_loss: 1.1950 - val_sparse_categorical_accuracy: 0.5692\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 5s - loss: 1.2194 - sparse_categorical_accuracy: 0.5641 - val_loss: 1.1799 - val_sparse_categorical_accuracy: 0.5835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 5s - loss: 1.1643 - sparse_categorical_accuracy: 0.5865 - val_loss: 1.1077 - val_sparse_categorical_accuracy: 0.6058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 5s - loss: 1.1272 - sparse_categorical_accuracy: 0.5986 - val_loss: 1.0716 - val_sparse_categorical_accuracy: 0.6238\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 5s - loss: 1.0999 - sparse_categorical_accuracy: 0.6091 - val_loss: 1.0345 - val_sparse_categorical_accuracy: 0.6353\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 5s - loss: 1.0659 - sparse_categorical_accuracy: 0.6203 - val_loss: 1.0110 - val_sparse_categorical_accuracy: 0.6447\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 5s - loss: 1.0438 - sparse_categorical_accuracy: 0.6276 - val_loss: 0.9739 - val_sparse_categorical_accuracy: 0.6572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 5s - loss: 1.0248 - sparse_categorical_accuracy: 0.6387 - val_loss: 0.9643 - val_sparse_categorical_accuracy: 0.6565\n",
            "Epoch 14/50\n",
            "196/196 - 5s - loss: 1.0065 - sparse_categorical_accuracy: 0.6423 - val_loss: 0.9496 - val_sparse_categorical_accuracy: 0.6649\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 5s - loss: 0.9900 - sparse_categorical_accuracy: 0.6497 - val_loss: 0.9356 - val_sparse_categorical_accuracy: 0.6737\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 5s - loss: 0.9716 - sparse_categorical_accuracy: 0.6549 - val_loss: 0.9358 - val_sparse_categorical_accuracy: 0.6704\n",
            "Epoch 17/50\n",
            "196/196 - 5s - loss: 0.9541 - sparse_categorical_accuracy: 0.6617 - val_loss: 0.9045 - val_sparse_categorical_accuracy: 0.6825\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 - 5s - loss: 0.9442 - sparse_categorical_accuracy: 0.6663 - val_loss: 0.9377 - val_sparse_categorical_accuracy: 0.6703\n",
            "Epoch 19/50\n",
            "196/196 - 5s - loss: 0.9398 - sparse_categorical_accuracy: 0.6668 - val_loss: 0.9123 - val_sparse_categorical_accuracy: 0.6778\n",
            "Epoch 20/50\n",
            "196/196 - 5s - loss: 0.9247 - sparse_categorical_accuracy: 0.6732 - val_loss: 0.8658 - val_sparse_categorical_accuracy: 0.6968\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 5s - loss: 0.9171 - sparse_categorical_accuracy: 0.6748 - val_loss: 0.8799 - val_sparse_categorical_accuracy: 0.6941\n",
            "Epoch 22/50\n",
            "196/196 - 5s - loss: 0.9043 - sparse_categorical_accuracy: 0.6804 - val_loss: 0.8652 - val_sparse_categorical_accuracy: 0.6941\n",
            "Epoch 23/50\n",
            "196/196 - 5s - loss: 0.8936 - sparse_categorical_accuracy: 0.6850 - val_loss: 0.8616 - val_sparse_categorical_accuracy: 0.6936\n",
            "Epoch 24/50\n",
            "196/196 - 5s - loss: 0.8806 - sparse_categorical_accuracy: 0.6879 - val_loss: 0.8760 - val_sparse_categorical_accuracy: 0.6959\n",
            "Epoch 25/50\n",
            "196/196 - 5s - loss: 0.8718 - sparse_categorical_accuracy: 0.6906 - val_loss: 0.9357 - val_sparse_categorical_accuracy: 0.6797\n",
            "Epoch 26/50\n",
            "196/196 - 5s - loss: 0.8703 - sparse_categorical_accuracy: 0.6912 - val_loss: 0.8754 - val_sparse_categorical_accuracy: 0.6964\n",
            "Epoch 27/50\n",
            "196/196 - 5s - loss: 0.8636 - sparse_categorical_accuracy: 0.6967 - val_loss: 0.8269 - val_sparse_categorical_accuracy: 0.7132\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 - 5s - loss: 0.8512 - sparse_categorical_accuracy: 0.6969 - val_loss: 0.8606 - val_sparse_categorical_accuracy: 0.7011\n",
            "Epoch 29/50\n",
            "196/196 - 5s - loss: 0.8443 - sparse_categorical_accuracy: 0.6992 - val_loss: 0.8280 - val_sparse_categorical_accuracy: 0.7119\n",
            "Epoch 30/50\n",
            "196/196 - 5s - loss: 0.8448 - sparse_categorical_accuracy: 0.7001 - val_loss: 0.8198 - val_sparse_categorical_accuracy: 0.7139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 - 5s - loss: 0.8351 - sparse_categorical_accuracy: 0.7038 - val_loss: 0.8604 - val_sparse_categorical_accuracy: 0.7023\n",
            "Epoch 32/50\n",
            "196/196 - 5s - loss: 0.8357 - sparse_categorical_accuracy: 0.7072 - val_loss: 0.8224 - val_sparse_categorical_accuracy: 0.7124\n",
            "Epoch 33/50\n",
            "196/196 - 5s - loss: 0.8175 - sparse_categorical_accuracy: 0.7104 - val_loss: 0.8305 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 34/50\n",
            "196/196 - 5s - loss: 0.8227 - sparse_categorical_accuracy: 0.7084 - val_loss: 0.8253 - val_sparse_categorical_accuracy: 0.7165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 - 5s - loss: 0.8065 - sparse_categorical_accuracy: 0.7143 - val_loss: 0.8198 - val_sparse_categorical_accuracy: 0.7124\n",
            "Epoch 36/50\n",
            "196/196 - 5s - loss: 0.8071 - sparse_categorical_accuracy: 0.7137 - val_loss: 0.8315 - val_sparse_categorical_accuracy: 0.7203\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37/50\n",
            "196/196 - 5s - loss: 0.7953 - sparse_categorical_accuracy: 0.7167 - val_loss: 0.8101 - val_sparse_categorical_accuracy: 0.7231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 - 5s - loss: 0.7933 - sparse_categorical_accuracy: 0.7177 - val_loss: 0.7945 - val_sparse_categorical_accuracy: 0.7248\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39/50\n",
            "196/196 - 5s - loss: 0.7847 - sparse_categorical_accuracy: 0.7179 - val_loss: 0.8264 - val_sparse_categorical_accuracy: 0.7206\n",
            "Epoch 40/50\n",
            "196/196 - 5s - loss: 0.7860 - sparse_categorical_accuracy: 0.7201 - val_loss: 0.8505 - val_sparse_categorical_accuracy: 0.7015\n",
            "Epoch 41/50\n",
            "196/196 - 5s - loss: 0.7814 - sparse_categorical_accuracy: 0.7217 - val_loss: 0.8002 - val_sparse_categorical_accuracy: 0.7247\n",
            "Epoch 42/50\n",
            "196/196 - 5s - loss: 0.7767 - sparse_categorical_accuracy: 0.7238 - val_loss: 0.8014 - val_sparse_categorical_accuracy: 0.7246\n",
            "Epoch 43/50\n",
            "196/196 - 5s - loss: 0.7725 - sparse_categorical_accuracy: 0.7269 - val_loss: 0.8482 - val_sparse_categorical_accuracy: 0.7119\n",
            "Epoch 44/50\n",
            "196/196 - 5s - loss: 0.7654 - sparse_categorical_accuracy: 0.7265 - val_loss: 0.8268 - val_sparse_categorical_accuracy: 0.7206\n",
            "Epoch 45/50\n",
            "196/196 - 5s - loss: 0.7642 - sparse_categorical_accuracy: 0.7280 - val_loss: 0.8138 - val_sparse_categorical_accuracy: 0.7274\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 46/50\n",
            "196/196 - 5s - loss: 0.7569 - sparse_categorical_accuracy: 0.7304 - val_loss: 0.7995 - val_sparse_categorical_accuracy: 0.7302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47/50\n",
            "196/196 - 5s - loss: 0.7613 - sparse_categorical_accuracy: 0.7284 - val_loss: 0.8249 - val_sparse_categorical_accuracy: 0.7204\n",
            "Epoch 48/50\n",
            "196/196 - 5s - loss: 0.7552 - sparse_categorical_accuracy: 0.7319 - val_loss: 0.8127 - val_sparse_categorical_accuracy: 0.7158\n",
            "Epoch 49/50\n",
            "196/196 - 5s - loss: 0.7489 - sparse_categorical_accuracy: 0.7329 - val_loss: 0.7880 - val_sparse_categorical_accuracy: 0.7291\n",
            "Epoch 50/50\n",
            "196/196 - 5s - loss: 0.7469 - sparse_categorical_accuracy: 0.7358 - val_loss: 0.7821 - val_sparse_categorical_accuracy: 0.7363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_24_layer_call_fn, conv2d_24_layer_call_and_return_conditional_losses, conv2d_25_layer_call_fn, conv2d_25_layer_call_and_return_conditional_losses, conv2d_26_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total training time 319.11335849761963 seconds\n",
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        2674      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        3890      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,201,998\n",
            "Trainable params: 1,201,998\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 8s - loss: 2.0423 - sparse_categorical_accuracy: 0.2375 - val_loss: 1.8149 - val_sparse_categorical_accuracy: 0.3071\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 6s - loss: 1.6553 - sparse_categorical_accuracy: 0.3948 - val_loss: 1.5814 - val_sparse_categorical_accuracy: 0.4116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 6s - loss: 1.4684 - sparse_categorical_accuracy: 0.4697 - val_loss: 1.4873 - val_sparse_categorical_accuracy: 0.4444\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 6s - loss: 1.3571 - sparse_categorical_accuracy: 0.5130 - val_loss: 1.3523 - val_sparse_categorical_accuracy: 0.5174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 6s - loss: 1.2622 - sparse_categorical_accuracy: 0.5488 - val_loss: 1.1915 - val_sparse_categorical_accuracy: 0.5681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 6s - loss: 1.2093 - sparse_categorical_accuracy: 0.5701 - val_loss: 1.1247 - val_sparse_categorical_accuracy: 0.5985\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 6s - loss: 1.1612 - sparse_categorical_accuracy: 0.5856 - val_loss: 1.1242 - val_sparse_categorical_accuracy: 0.6041\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 6s - loss: 1.1245 - sparse_categorical_accuracy: 0.6004 - val_loss: 1.0207 - val_sparse_categorical_accuracy: 0.6380\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 6s - loss: 1.0896 - sparse_categorical_accuracy: 0.6121 - val_loss: 0.9900 - val_sparse_categorical_accuracy: 0.6522\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 6s - loss: 1.0638 - sparse_categorical_accuracy: 0.6251 - val_loss: 0.9794 - val_sparse_categorical_accuracy: 0.6518\n",
            "Epoch 11/50\n",
            "196/196 - 6s - loss: 1.0352 - sparse_categorical_accuracy: 0.6359 - val_loss: 1.0757 - val_sparse_categorical_accuracy: 0.6364\n",
            "Epoch 12/50\n",
            "196/196 - 6s - loss: 1.0154 - sparse_categorical_accuracy: 0.6405 - val_loss: 0.9588 - val_sparse_categorical_accuracy: 0.6576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 6s - loss: 0.9976 - sparse_categorical_accuracy: 0.6463 - val_loss: 0.9589 - val_sparse_categorical_accuracy: 0.6619\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 - 6s - loss: 0.9739 - sparse_categorical_accuracy: 0.6553 - val_loss: 0.9101 - val_sparse_categorical_accuracy: 0.6814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 6s - loss: 0.9645 - sparse_categorical_accuracy: 0.6592 - val_loss: 0.9360 - val_sparse_categorical_accuracy: 0.6667\n",
            "Epoch 16/50\n",
            "196/196 - 6s - loss: 0.9447 - sparse_categorical_accuracy: 0.6678 - val_loss: 0.8499 - val_sparse_categorical_accuracy: 0.6965\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 - 6s - loss: 0.9275 - sparse_categorical_accuracy: 0.6720 - val_loss: 0.8580 - val_sparse_categorical_accuracy: 0.6947\n",
            "Epoch 18/50\n",
            "196/196 - 6s - loss: 0.9146 - sparse_categorical_accuracy: 0.6771 - val_loss: 0.9456 - val_sparse_categorical_accuracy: 0.6716\n",
            "Epoch 19/50\n",
            "196/196 - 6s - loss: 0.8962 - sparse_categorical_accuracy: 0.6820 - val_loss: 0.8090 - val_sparse_categorical_accuracy: 0.7149\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 - 6s - loss: 0.8879 - sparse_categorical_accuracy: 0.6858 - val_loss: 0.8138 - val_sparse_categorical_accuracy: 0.7157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 6s - loss: 0.8691 - sparse_categorical_accuracy: 0.6910 - val_loss: 0.8328 - val_sparse_categorical_accuracy: 0.7071\n",
            "Epoch 22/50\n",
            "196/196 - 6s - loss: 0.8723 - sparse_categorical_accuracy: 0.6924 - val_loss: 0.7984 - val_sparse_categorical_accuracy: 0.7157\n",
            "Epoch 23/50\n",
            "196/196 - 6s - loss: 0.8503 - sparse_categorical_accuracy: 0.6996 - val_loss: 0.8371 - val_sparse_categorical_accuracy: 0.7105\n",
            "Epoch 24/50\n",
            "196/196 - 6s - loss: 0.8358 - sparse_categorical_accuracy: 0.7053 - val_loss: 0.7923 - val_sparse_categorical_accuracy: 0.7210\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 - 6s - loss: 0.8306 - sparse_categorical_accuracy: 0.7069 - val_loss: 0.8166 - val_sparse_categorical_accuracy: 0.7165\n",
            "Epoch 26/50\n",
            "196/196 - 6s - loss: 0.8304 - sparse_categorical_accuracy: 0.7076 - val_loss: 0.7902 - val_sparse_categorical_accuracy: 0.7271\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 - 6s - loss: 0.8186 - sparse_categorical_accuracy: 0.7094 - val_loss: 0.7982 - val_sparse_categorical_accuracy: 0.7230\n",
            "Epoch 28/50\n",
            "196/196 - 6s - loss: 0.8110 - sparse_categorical_accuracy: 0.7129 - val_loss: 0.7877 - val_sparse_categorical_accuracy: 0.7300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 - 6s - loss: 0.8093 - sparse_categorical_accuracy: 0.7126 - val_loss: 0.7571 - val_sparse_categorical_accuracy: 0.7364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n",
            "196/196 - 6s - loss: 0.7944 - sparse_categorical_accuracy: 0.7178 - val_loss: 0.7544 - val_sparse_categorical_accuracy: 0.7371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 - 6s - loss: 0.7940 - sparse_categorical_accuracy: 0.7189 - val_loss: 0.7616 - val_sparse_categorical_accuracy: 0.7345\n",
            "Epoch 32/50\n",
            "196/196 - 6s - loss: 0.7829 - sparse_categorical_accuracy: 0.7223 - val_loss: 0.7624 - val_sparse_categorical_accuracy: 0.7407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33/50\n",
            "196/196 - 6s - loss: 0.7766 - sparse_categorical_accuracy: 0.7250 - val_loss: 0.7314 - val_sparse_categorical_accuracy: 0.7467\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "196/196 - 6s - loss: 0.7751 - sparse_categorical_accuracy: 0.7255 - val_loss: 0.7753 - val_sparse_categorical_accuracy: 0.7334\n",
            "Epoch 35/50\n",
            "196/196 - 6s - loss: 0.7642 - sparse_categorical_accuracy: 0.7304 - val_loss: 0.7511 - val_sparse_categorical_accuracy: 0.7409\n",
            "Epoch 36/50\n",
            "196/196 - 6s - loss: 0.7569 - sparse_categorical_accuracy: 0.7316 - val_loss: 0.7433 - val_sparse_categorical_accuracy: 0.7421\n",
            "Epoch 37/50\n",
            "196/196 - 6s - loss: 0.7605 - sparse_categorical_accuracy: 0.7310 - val_loss: 0.7514 - val_sparse_categorical_accuracy: 0.7429\n",
            "Epoch 38/50\n",
            "196/196 - 6s - loss: 0.7518 - sparse_categorical_accuracy: 0.7345 - val_loss: 0.7421 - val_sparse_categorical_accuracy: 0.7467\n",
            "Epoch 39/50\n",
            "196/196 - 6s - loss: 0.7467 - sparse_categorical_accuracy: 0.7363 - val_loss: 0.7322 - val_sparse_categorical_accuracy: 0.7442\n",
            "Epoch 40/50\n",
            "196/196 - 6s - loss: 0.7388 - sparse_categorical_accuracy: 0.7372 - val_loss: 0.7304 - val_sparse_categorical_accuracy: 0.7494\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 41/50\n",
            "196/196 - 6s - loss: 0.7270 - sparse_categorical_accuracy: 0.7412 - val_loss: 0.7553 - val_sparse_categorical_accuracy: 0.7445\n",
            "Epoch 42/50\n",
            "196/196 - 6s - loss: 0.7304 - sparse_categorical_accuracy: 0.7417 - val_loss: 0.7520 - val_sparse_categorical_accuracy: 0.7490\n",
            "Epoch 43/50\n",
            "196/196 - 6s - loss: 0.7291 - sparse_categorical_accuracy: 0.7428 - val_loss: 0.7394 - val_sparse_categorical_accuracy: 0.7469\n",
            "Epoch 44/50\n",
            "196/196 - 6s - loss: 0.7219 - sparse_categorical_accuracy: 0.7460 - val_loss: 0.7330 - val_sparse_categorical_accuracy: 0.7474\n",
            "Epoch 45/50\n",
            "196/196 - 6s - loss: 0.7201 - sparse_categorical_accuracy: 0.7450 - val_loss: 0.7342 - val_sparse_categorical_accuracy: 0.7559\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 46/50\n",
            "196/196 - 6s - loss: 0.7175 - sparse_categorical_accuracy: 0.7454 - val_loss: 0.6976 - val_sparse_categorical_accuracy: 0.7566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47/50\n",
            "196/196 - 6s - loss: 0.7023 - sparse_categorical_accuracy: 0.7505 - val_loss: 0.7498 - val_sparse_categorical_accuracy: 0.7458\n",
            "Epoch 48/50\n",
            "196/196 - 6s - loss: 0.7006 - sparse_categorical_accuracy: 0.7500 - val_loss: 0.7012 - val_sparse_categorical_accuracy: 0.7637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_30_layer_call_fn, conv2d_30_layer_call_and_return_conditional_losses, conv2d_31_layer_call_fn, conv2d_31_layer_call_and_return_conditional_losses, conv2d_32_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 49/50\n",
            "196/196 - 6s - loss: 0.7001 - sparse_categorical_accuracy: 0.7530 - val_loss: 0.6926 - val_sparse_categorical_accuracy: 0.7624\n",
            "Epoch 50/50\n",
            "196/196 - 6s - loss: 0.6962 - sparse_categorical_accuracy: 0.7518 - val_loss: 0.7546 - val_sparse_categorical_accuracy: 0.7518\n",
            "Total training time 384.7061195373535 seconds\n",
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        4324      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        5732      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,205,490\n",
            "Trainable params: 1,205,490\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 9s - loss: 1.8995 - sparse_categorical_accuracy: 0.3003 - val_loss: 1.6848 - val_sparse_categorical_accuracy: 0.3808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 7s - loss: 1.5612 - sparse_categorical_accuracy: 0.4395 - val_loss: 1.4758 - val_sparse_categorical_accuracy: 0.4655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 7s - loss: 1.4046 - sparse_categorical_accuracy: 0.4996 - val_loss: 1.3281 - val_sparse_categorical_accuracy: 0.5317\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 7s - loss: 1.3032 - sparse_categorical_accuracy: 0.5368 - val_loss: 1.2902 - val_sparse_categorical_accuracy: 0.5419\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 7s - loss: 1.2412 - sparse_categorical_accuracy: 0.5623 - val_loss: 1.1860 - val_sparse_categorical_accuracy: 0.5810\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 7s - loss: 1.1749 - sparse_categorical_accuracy: 0.5838 - val_loss: 1.0948 - val_sparse_categorical_accuracy: 0.6146\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 7s - loss: 1.1383 - sparse_categorical_accuracy: 0.5978 - val_loss: 1.1102 - val_sparse_categorical_accuracy: 0.6142\n",
            "Epoch 8/50\n",
            "196/196 - 7s - loss: 1.0939 - sparse_categorical_accuracy: 0.6130 - val_loss: 1.0126 - val_sparse_categorical_accuracy: 0.6473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 7s - loss: 1.0560 - sparse_categorical_accuracy: 0.6280 - val_loss: 1.0179 - val_sparse_categorical_accuracy: 0.6434\n",
            "Epoch 10/50\n",
            "196/196 - 7s - loss: 1.0298 - sparse_categorical_accuracy: 0.6366 - val_loss: 0.9907 - val_sparse_categorical_accuracy: 0.6542\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 7s - loss: 0.9966 - sparse_categorical_accuracy: 0.6495 - val_loss: 0.9823 - val_sparse_categorical_accuracy: 0.6557\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 7s - loss: 0.9841 - sparse_categorical_accuracy: 0.6547 - val_loss: 0.8955 - val_sparse_categorical_accuracy: 0.6890\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 7s - loss: 0.9563 - sparse_categorical_accuracy: 0.6630 - val_loss: 0.9823 - val_sparse_categorical_accuracy: 0.6654\n",
            "Epoch 14/50\n",
            "196/196 - 7s - loss: 0.9391 - sparse_categorical_accuracy: 0.6675 - val_loss: 0.9128 - val_sparse_categorical_accuracy: 0.6837\n",
            "Epoch 15/50\n",
            "196/196 - 7s - loss: 0.9192 - sparse_categorical_accuracy: 0.6766 - val_loss: 0.8747 - val_sparse_categorical_accuracy: 0.6980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 7s - loss: 0.9065 - sparse_categorical_accuracy: 0.6815 - val_loss: 0.8513 - val_sparse_categorical_accuracy: 0.7006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 - 7s - loss: 0.8862 - sparse_categorical_accuracy: 0.6877 - val_loss: 0.8699 - val_sparse_categorical_accuracy: 0.6974\n",
            "Epoch 18/50\n",
            "196/196 - 7s - loss: 0.8746 - sparse_categorical_accuracy: 0.6905 - val_loss: 0.8459 - val_sparse_categorical_accuracy: 0.7032\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 - 7s - loss: 0.8519 - sparse_categorical_accuracy: 0.7018 - val_loss: 0.8546 - val_sparse_categorical_accuracy: 0.7040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 - 7s - loss: 0.8483 - sparse_categorical_accuracy: 0.7028 - val_loss: 0.8071 - val_sparse_categorical_accuracy: 0.7163\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 7s - loss: 0.8337 - sparse_categorical_accuracy: 0.7070 - val_loss: 0.8316 - val_sparse_categorical_accuracy: 0.7141\n",
            "Epoch 22/50\n",
            "196/196 - 7s - loss: 0.8201 - sparse_categorical_accuracy: 0.7106 - val_loss: 0.8020 - val_sparse_categorical_accuracy: 0.7259\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 - 7s - loss: 0.8136 - sparse_categorical_accuracy: 0.7125 - val_loss: 0.8296 - val_sparse_categorical_accuracy: 0.7247\n",
            "Epoch 24/50\n",
            "196/196 - 7s - loss: 0.7994 - sparse_categorical_accuracy: 0.7172 - val_loss: 0.7692 - val_sparse_categorical_accuracy: 0.7329\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 - 7s - loss: 0.7920 - sparse_categorical_accuracy: 0.7220 - val_loss: 0.7920 - val_sparse_categorical_accuracy: 0.7249\n",
            "Epoch 26/50\n",
            "196/196 - 7s - loss: 0.7862 - sparse_categorical_accuracy: 0.7235 - val_loss: 0.7629 - val_sparse_categorical_accuracy: 0.7366\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 - 7s - loss: 0.7729 - sparse_categorical_accuracy: 0.7282 - val_loss: 0.8171 - val_sparse_categorical_accuracy: 0.7249\n",
            "Epoch 28/50\n",
            "196/196 - 7s - loss: 0.7738 - sparse_categorical_accuracy: 0.7277 - val_loss: 0.7606 - val_sparse_categorical_accuracy: 0.7375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 - 7s - loss: 0.7544 - sparse_categorical_accuracy: 0.7350 - val_loss: 0.7754 - val_sparse_categorical_accuracy: 0.7360\n",
            "Epoch 30/50\n",
            "196/196 - 7s - loss: 0.7577 - sparse_categorical_accuracy: 0.7354 - val_loss: 0.8011 - val_sparse_categorical_accuracy: 0.7392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 - 7s - loss: 0.7495 - sparse_categorical_accuracy: 0.7371 - val_loss: 0.7626 - val_sparse_categorical_accuracy: 0.7405\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32/50\n",
            "196/196 - 7s - loss: 0.7337 - sparse_categorical_accuracy: 0.7409 - val_loss: 0.7836 - val_sparse_categorical_accuracy: 0.7379\n",
            "Epoch 33/50\n",
            "196/196 - 7s - loss: 0.7241 - sparse_categorical_accuracy: 0.7428 - val_loss: 0.7867 - val_sparse_categorical_accuracy: 0.7368\n",
            "Epoch 34/50\n",
            "196/196 - 7s - loss: 0.7183 - sparse_categorical_accuracy: 0.7479 - val_loss: 0.7489 - val_sparse_categorical_accuracy: 0.7442\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 - 7s - loss: 0.7165 - sparse_categorical_accuracy: 0.7498 - val_loss: 0.7396 - val_sparse_categorical_accuracy: 0.7441\n",
            "Epoch 36/50\n",
            "196/196 - 7s - loss: 0.7095 - sparse_categorical_accuracy: 0.7505 - val_loss: 0.7583 - val_sparse_categorical_accuracy: 0.7591\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37/50\n",
            "196/196 - 7s - loss: 0.6987 - sparse_categorical_accuracy: 0.7527 - val_loss: 0.7640 - val_sparse_categorical_accuracy: 0.7486\n",
            "Epoch 38/50\n",
            "196/196 - 7s - loss: 0.6974 - sparse_categorical_accuracy: 0.7537 - val_loss: 0.7119 - val_sparse_categorical_accuracy: 0.7593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39/50\n",
            "196/196 - 7s - loss: 0.6883 - sparse_categorical_accuracy: 0.7583 - val_loss: 0.7542 - val_sparse_categorical_accuracy: 0.7548\n",
            "Epoch 40/50\n",
            "196/196 - 7s - loss: 0.6866 - sparse_categorical_accuracy: 0.7572 - val_loss: 0.7592 - val_sparse_categorical_accuracy: 0.7464\n",
            "Epoch 41/50\n",
            "196/196 - 7s - loss: 0.6782 - sparse_categorical_accuracy: 0.7597 - val_loss: 0.7766 - val_sparse_categorical_accuracy: 0.7550\n",
            "Epoch 42/50\n",
            "196/196 - 7s - loss: 0.6759 - sparse_categorical_accuracy: 0.7624 - val_loss: 0.7124 - val_sparse_categorical_accuracy: 0.7624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43/50\n",
            "196/196 - 7s - loss: 0.6733 - sparse_categorical_accuracy: 0.7601 - val_loss: 0.7443 - val_sparse_categorical_accuracy: 0.7533\n",
            "Epoch 44/50\n",
            "196/196 - 7s - loss: 0.6643 - sparse_categorical_accuracy: 0.7659 - val_loss: 0.7162 - val_sparse_categorical_accuracy: 0.7559\n",
            "Epoch 45/50\n",
            "196/196 - 7s - loss: 0.6586 - sparse_categorical_accuracy: 0.7668 - val_loss: 0.7029 - val_sparse_categorical_accuracy: 0.7605\n",
            "Epoch 46/50\n",
            "196/196 - 7s - loss: 0.6562 - sparse_categorical_accuracy: 0.7678 - val_loss: 0.6879 - val_sparse_categorical_accuracy: 0.7610\n",
            "Epoch 47/50\n",
            "196/196 - 7s - loss: 0.6505 - sparse_categorical_accuracy: 0.7700 - val_loss: 0.7987 - val_sparse_categorical_accuracy: 0.7553\n",
            "Epoch 48/50\n",
            "196/196 - 7s - loss: 0.6487 - sparse_categorical_accuracy: 0.7717 - val_loss: 0.7172 - val_sparse_categorical_accuracy: 0.7595\n",
            "Epoch 49/50\n",
            "196/196 - 7s - loss: 0.6476 - sparse_categorical_accuracy: 0.7695 - val_loss: 0.7100 - val_sparse_categorical_accuracy: 0.7609\n",
            "Epoch 50/50\n",
            "196/196 - 7s - loss: 0.6438 - sparse_categorical_accuracy: 0.7744 - val_loss: 0.7422 - val_sparse_categorical_accuracy: 0.7641\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_36_layer_call_fn, conv2d_36_layer_call_and_return_conditional_losses, conv2d_37_layer_call_fn, conv2d_37_layer_call_and_return_conditional_losses, conv2d_38_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total training time 427.0395154953003 seconds\n",
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        7624      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        9416      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,212,474\n",
            "Trainable params: 1,212,474\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 11s - loss: 1.9601 - sparse_categorical_accuracy: 0.2791 - val_loss: 1.6459 - val_sparse_categorical_accuracy: 0.4076\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 9s - loss: 1.5480 - sparse_categorical_accuracy: 0.4395 - val_loss: 1.4200 - val_sparse_categorical_accuracy: 0.4839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 9s - loss: 1.4058 - sparse_categorical_accuracy: 0.4963 - val_loss: 1.3069 - val_sparse_categorical_accuracy: 0.5267\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 9s - loss: 1.3023 - sparse_categorical_accuracy: 0.5349 - val_loss: 1.2579 - val_sparse_categorical_accuracy: 0.5395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 9s - loss: 1.2247 - sparse_categorical_accuracy: 0.5650 - val_loss: 1.1152 - val_sparse_categorical_accuracy: 0.6051\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 9s - loss: 1.1693 - sparse_categorical_accuracy: 0.5856 - val_loss: 1.0896 - val_sparse_categorical_accuracy: 0.6167\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 9s - loss: 1.1218 - sparse_categorical_accuracy: 0.5993 - val_loss: 1.0544 - val_sparse_categorical_accuracy: 0.6259\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 9s - loss: 1.0859 - sparse_categorical_accuracy: 0.6146 - val_loss: 1.0046 - val_sparse_categorical_accuracy: 0.6375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 9s - loss: 1.0647 - sparse_categorical_accuracy: 0.6225 - val_loss: 1.0144 - val_sparse_categorical_accuracy: 0.6407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 9s - loss: 1.0421 - sparse_categorical_accuracy: 0.6291 - val_loss: 0.9624 - val_sparse_categorical_accuracy: 0.6593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 9s - loss: 1.0214 - sparse_categorical_accuracy: 0.6386 - val_loss: 0.9947 - val_sparse_categorical_accuracy: 0.6451\n",
            "Epoch 12/50\n",
            "196/196 - 9s - loss: 1.0012 - sparse_categorical_accuracy: 0.6463 - val_loss: 0.9219 - val_sparse_categorical_accuracy: 0.6781\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 9s - loss: 0.9836 - sparse_categorical_accuracy: 0.6473 - val_loss: 0.9938 - val_sparse_categorical_accuracy: 0.6584\n",
            "Epoch 14/50\n",
            "196/196 - 9s - loss: 0.9703 - sparse_categorical_accuracy: 0.6596 - val_loss: 0.9279 - val_sparse_categorical_accuracy: 0.6716\n",
            "Epoch 15/50\n",
            "196/196 - 9s - loss: 0.9408 - sparse_categorical_accuracy: 0.6678 - val_loss: 0.8897 - val_sparse_categorical_accuracy: 0.6844\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 9s - loss: 0.9300 - sparse_categorical_accuracy: 0.6708 - val_loss: 0.9099 - val_sparse_categorical_accuracy: 0.6834\n",
            "Epoch 17/50\n",
            "196/196 - 9s - loss: 0.9216 - sparse_categorical_accuracy: 0.6737 - val_loss: 0.8552 - val_sparse_categorical_accuracy: 0.6998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 - 9s - loss: 0.9082 - sparse_categorical_accuracy: 0.6785 - val_loss: 0.8335 - val_sparse_categorical_accuracy: 0.7134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 - 9s - loss: 0.8984 - sparse_categorical_accuracy: 0.6844 - val_loss: 0.8481 - val_sparse_categorical_accuracy: 0.7048\n",
            "Epoch 20/50\n",
            "196/196 - 9s - loss: 0.8774 - sparse_categorical_accuracy: 0.6910 - val_loss: 0.8301 - val_sparse_categorical_accuracy: 0.7089\n",
            "Epoch 21/50\n",
            "196/196 - 9s - loss: 0.8703 - sparse_categorical_accuracy: 0.6938 - val_loss: 0.8195 - val_sparse_categorical_accuracy: 0.7125\n",
            "Epoch 22/50\n",
            "196/196 - 9s - loss: 0.8566 - sparse_categorical_accuracy: 0.6987 - val_loss: 0.8009 - val_sparse_categorical_accuracy: 0.7216\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 - 9s - loss: 0.8440 - sparse_categorical_accuracy: 0.7033 - val_loss: 0.7790 - val_sparse_categorical_accuracy: 0.7264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 - 9s - loss: 0.8371 - sparse_categorical_accuracy: 0.7059 - val_loss: 0.8134 - val_sparse_categorical_accuracy: 0.7205\n",
            "Epoch 25/50\n",
            "196/196 - 9s - loss: 0.8272 - sparse_categorical_accuracy: 0.7097 - val_loss: 0.7640 - val_sparse_categorical_accuracy: 0.7354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26/50\n",
            "196/196 - 9s - loss: 0.8186 - sparse_categorical_accuracy: 0.7115 - val_loss: 0.7968 - val_sparse_categorical_accuracy: 0.7277\n",
            "Epoch 27/50\n",
            "196/196 - 9s - loss: 0.8118 - sparse_categorical_accuracy: 0.7168 - val_loss: 0.7810 - val_sparse_categorical_accuracy: 0.7314\n",
            "Epoch 28/50\n",
            "196/196 - 9s - loss: 0.7948 - sparse_categorical_accuracy: 0.7211 - val_loss: 0.8228 - val_sparse_categorical_accuracy: 0.7273\n",
            "Epoch 29/50\n",
            "196/196 - 9s - loss: 0.7965 - sparse_categorical_accuracy: 0.7200 - val_loss: 0.7983 - val_sparse_categorical_accuracy: 0.7299\n",
            "Epoch 30/50\n",
            "196/196 - 9s - loss: 0.7831 - sparse_categorical_accuracy: 0.7247 - val_loss: 0.7533 - val_sparse_categorical_accuracy: 0.7413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 - 9s - loss: 0.7770 - sparse_categorical_accuracy: 0.7236 - val_loss: 0.7514 - val_sparse_categorical_accuracy: 0.7396\n",
            "Epoch 32/50\n",
            "196/196 - 9s - loss: 0.7731 - sparse_categorical_accuracy: 0.7282 - val_loss: 0.7726 - val_sparse_categorical_accuracy: 0.7436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33/50\n",
            "196/196 - 9s - loss: 0.7639 - sparse_categorical_accuracy: 0.7324 - val_loss: 0.7344 - val_sparse_categorical_accuracy: 0.7457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "196/196 - 9s - loss: 0.7684 - sparse_categorical_accuracy: 0.7296 - val_loss: 0.7824 - val_sparse_categorical_accuracy: 0.7460\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 - 9s - loss: 0.7550 - sparse_categorical_accuracy: 0.7344 - val_loss: 0.7492 - val_sparse_categorical_accuracy: 0.7489\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36/50\n",
            "196/196 - 9s - loss: 0.7441 - sparse_categorical_accuracy: 0.7367 - val_loss: 0.7298 - val_sparse_categorical_accuracy: 0.7571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37/50\n",
            "196/196 - 9s - loss: 0.7390 - sparse_categorical_accuracy: 0.7398 - val_loss: 0.7461 - val_sparse_categorical_accuracy: 0.7579\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 - 9s - loss: 0.7286 - sparse_categorical_accuracy: 0.7449 - val_loss: 0.7249 - val_sparse_categorical_accuracy: 0.7528\n",
            "Epoch 39/50\n",
            "196/196 - 9s - loss: 0.7234 - sparse_categorical_accuracy: 0.7452 - val_loss: 0.7479 - val_sparse_categorical_accuracy: 0.7476\n",
            "Epoch 40/50\n",
            "196/196 - 9s - loss: 0.7236 - sparse_categorical_accuracy: 0.7461 - val_loss: 0.7447 - val_sparse_categorical_accuracy: 0.7540\n",
            "Epoch 41/50\n",
            "196/196 - 9s - loss: 0.7132 - sparse_categorical_accuracy: 0.7495 - val_loss: 0.7340 - val_sparse_categorical_accuracy: 0.7510\n",
            "Epoch 42/50\n",
            "196/196 - 9s - loss: 0.7132 - sparse_categorical_accuracy: 0.7497 - val_loss: 0.7336 - val_sparse_categorical_accuracy: 0.7526\n",
            "Epoch 43/50\n",
            "196/196 - 9s - loss: 0.6995 - sparse_categorical_accuracy: 0.7541 - val_loss: 0.7554 - val_sparse_categorical_accuracy: 0.7461\n",
            "Epoch 44/50\n",
            "196/196 - 9s - loss: 0.7047 - sparse_categorical_accuracy: 0.7510 - val_loss: 0.7406 - val_sparse_categorical_accuracy: 0.7502\n",
            "Epoch 45/50\n",
            "196/196 - 9s - loss: 0.7018 - sparse_categorical_accuracy: 0.7531 - val_loss: 0.7062 - val_sparse_categorical_accuracy: 0.7642\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 46/50\n",
            "196/196 - 9s - loss: 0.6951 - sparse_categorical_accuracy: 0.7539 - val_loss: 0.7014 - val_sparse_categorical_accuracy: 0.7608\n",
            "Epoch 47/50\n",
            "196/196 - 9s - loss: 0.6825 - sparse_categorical_accuracy: 0.7600 - val_loss: 0.7546 - val_sparse_categorical_accuracy: 0.7530\n",
            "Epoch 48/50\n",
            "196/196 - 9s - loss: 0.6814 - sparse_categorical_accuracy: 0.7597 - val_loss: 0.7492 - val_sparse_categorical_accuracy: 0.7524\n",
            "Epoch 49/50\n",
            "196/196 - 9s - loss: 0.6759 - sparse_categorical_accuracy: 0.7619 - val_loss: 0.6983 - val_sparse_categorical_accuracy: 0.7609\n",
            "Epoch 50/50\n",
            "196/196 - 9s - loss: 0.6728 - sparse_categorical_accuracy: 0.7630 - val_loss: 0.7006 - val_sparse_categorical_accuracy: 0.7645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, conv2d_43_layer_call_fn, conv2d_43_layer_call_and_return_conditional_losses, conv2d_44_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total training time 501.56356859207153 seconds\n",
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        1401      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        2073      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,198,908\n",
            "Trainable params: 1,198,908\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 7s - loss: 1.9717 - sparse_categorical_accuracy: 0.2740 - val_loss: 1.6619 - val_sparse_categorical_accuracy: 0.4045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 5s - loss: 1.6004 - sparse_categorical_accuracy: 0.4193 - val_loss: 1.4925 - val_sparse_categorical_accuracy: 0.4638\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 5s - loss: 1.4495 - sparse_categorical_accuracy: 0.4773 - val_loss: 1.3643 - val_sparse_categorical_accuracy: 0.5029\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 5s - loss: 1.3459 - sparse_categorical_accuracy: 0.5166 - val_loss: 1.2653 - val_sparse_categorical_accuracy: 0.5456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 5s - loss: 1.2680 - sparse_categorical_accuracy: 0.5497 - val_loss: 1.3418 - val_sparse_categorical_accuracy: 0.5304\n",
            "Epoch 6/50\n",
            "196/196 - 5s - loss: 1.2085 - sparse_categorical_accuracy: 0.5668 - val_loss: 1.1234 - val_sparse_categorical_accuracy: 0.6064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 5s - loss: 1.1574 - sparse_categorical_accuracy: 0.5852 - val_loss: 1.0734 - val_sparse_categorical_accuracy: 0.6187\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 5s - loss: 1.1166 - sparse_categorical_accuracy: 0.6029 - val_loss: 1.0513 - val_sparse_categorical_accuracy: 0.6288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 5s - loss: 1.0865 - sparse_categorical_accuracy: 0.6131 - val_loss: 1.0373 - val_sparse_categorical_accuracy: 0.6363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 5s - loss: 1.0624 - sparse_categorical_accuracy: 0.6246 - val_loss: 0.9887 - val_sparse_categorical_accuracy: 0.6487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 5s - loss: 1.0408 - sparse_categorical_accuracy: 0.6301 - val_loss: 0.9782 - val_sparse_categorical_accuracy: 0.6570\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 5s - loss: 1.0037 - sparse_categorical_accuracy: 0.6425 - val_loss: 0.9596 - val_sparse_categorical_accuracy: 0.6624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 5s - loss: 0.9869 - sparse_categorical_accuracy: 0.6479 - val_loss: 0.9907 - val_sparse_categorical_accuracy: 0.6566\n",
            "Epoch 14/50\n",
            "196/196 - 5s - loss: 0.9701 - sparse_categorical_accuracy: 0.6565 - val_loss: 0.8940 - val_sparse_categorical_accuracy: 0.6870\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 5s - loss: 0.9555 - sparse_categorical_accuracy: 0.6591 - val_loss: 0.9164 - val_sparse_categorical_accuracy: 0.6802\n",
            "Epoch 16/50\n",
            "196/196 - 5s - loss: 0.9365 - sparse_categorical_accuracy: 0.6664 - val_loss: 0.8871 - val_sparse_categorical_accuracy: 0.6934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 - 5s - loss: 0.9237 - sparse_categorical_accuracy: 0.6722 - val_loss: 0.8993 - val_sparse_categorical_accuracy: 0.6879\n",
            "Epoch 18/50\n",
            "196/196 - 5s - loss: 0.9042 - sparse_categorical_accuracy: 0.6793 - val_loss: 0.9069 - val_sparse_categorical_accuracy: 0.6929\n",
            "Epoch 19/50\n",
            "196/196 - 5s - loss: 0.8917 - sparse_categorical_accuracy: 0.6839 - val_loss: 0.8511 - val_sparse_categorical_accuracy: 0.7077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 - 5s - loss: 0.8804 - sparse_categorical_accuracy: 0.6894 - val_loss: 0.8563 - val_sparse_categorical_accuracy: 0.7036\n",
            "Epoch 21/50\n",
            "196/196 - 5s - loss: 0.8612 - sparse_categorical_accuracy: 0.6941 - val_loss: 0.8291 - val_sparse_categorical_accuracy: 0.7123\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 - 5s - loss: 0.8477 - sparse_categorical_accuracy: 0.6974 - val_loss: 0.8409 - val_sparse_categorical_accuracy: 0.7135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 - 5s - loss: 0.8440 - sparse_categorical_accuracy: 0.7020 - val_loss: 0.8128 - val_sparse_categorical_accuracy: 0.7183\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 - 5s - loss: 0.8360 - sparse_categorical_accuracy: 0.7024 - val_loss: 0.8512 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 25/50\n",
            "196/196 - 5s - loss: 0.8219 - sparse_categorical_accuracy: 0.7073 - val_loss: 0.8558 - val_sparse_categorical_accuracy: 0.7042\n",
            "Epoch 26/50\n",
            "196/196 - 5s - loss: 0.8139 - sparse_categorical_accuracy: 0.7095 - val_loss: 0.7960 - val_sparse_categorical_accuracy: 0.7266\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 - 5s - loss: 0.8019 - sparse_categorical_accuracy: 0.7171 - val_loss: 0.8095 - val_sparse_categorical_accuracy: 0.7211\n",
            "Epoch 28/50\n",
            "196/196 - 5s - loss: 0.7955 - sparse_categorical_accuracy: 0.7165 - val_loss: 0.7843 - val_sparse_categorical_accuracy: 0.7321\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 - 5s - loss: 0.7856 - sparse_categorical_accuracy: 0.7213 - val_loss: 0.7730 - val_sparse_categorical_accuracy: 0.7339\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n",
            "196/196 - 5s - loss: 0.7757 - sparse_categorical_accuracy: 0.7240 - val_loss: 0.7668 - val_sparse_categorical_accuracy: 0.7379\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 - 5s - loss: 0.7740 - sparse_categorical_accuracy: 0.7235 - val_loss: 0.8010 - val_sparse_categorical_accuracy: 0.7329\n",
            "Epoch 32/50\n",
            "196/196 - 5s - loss: 0.7623 - sparse_categorical_accuracy: 0.7269 - val_loss: 0.7615 - val_sparse_categorical_accuracy: 0.7417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33/50\n",
            "196/196 - 5s - loss: 0.7553 - sparse_categorical_accuracy: 0.7303 - val_loss: 0.8151 - val_sparse_categorical_accuracy: 0.7387\n",
            "Epoch 34/50\n",
            "196/196 - 5s - loss: 0.7572 - sparse_categorical_accuracy: 0.7310 - val_loss: 0.7959 - val_sparse_categorical_accuracy: 0.7331\n",
            "Epoch 35/50\n",
            "196/196 - 5s - loss: 0.7535 - sparse_categorical_accuracy: 0.7323 - val_loss: 0.7895 - val_sparse_categorical_accuracy: 0.7423\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36/50\n",
            "196/196 - 5s - loss: 0.7410 - sparse_categorical_accuracy: 0.7358 - val_loss: 0.7711 - val_sparse_categorical_accuracy: 0.7374\n",
            "Epoch 37/50\n",
            "196/196 - 5s - loss: 0.7391 - sparse_categorical_accuracy: 0.7369 - val_loss: 0.7578 - val_sparse_categorical_accuracy: 0.7435\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 - 5s - loss: 0.7321 - sparse_categorical_accuracy: 0.7386 - val_loss: 0.7517 - val_sparse_categorical_accuracy: 0.7445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39/50\n",
            "196/196 - 5s - loss: 0.7241 - sparse_categorical_accuracy: 0.7429 - val_loss: 0.7578 - val_sparse_categorical_accuracy: 0.7444\n",
            "Epoch 40/50\n",
            "196/196 - 5s - loss: 0.7231 - sparse_categorical_accuracy: 0.7429 - val_loss: 0.7327 - val_sparse_categorical_accuracy: 0.7518\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 41/50\n",
            "196/196 - 5s - loss: 0.7135 - sparse_categorical_accuracy: 0.7451 - val_loss: 0.7624 - val_sparse_categorical_accuracy: 0.7403\n",
            "Epoch 42/50\n",
            "196/196 - 5s - loss: 0.7033 - sparse_categorical_accuracy: 0.7490 - val_loss: 0.7795 - val_sparse_categorical_accuracy: 0.7356\n",
            "Epoch 43/50\n",
            "196/196 - 5s - loss: 0.7104 - sparse_categorical_accuracy: 0.7486 - val_loss: 0.7493 - val_sparse_categorical_accuracy: 0.7448\n",
            "Epoch 44/50\n",
            "196/196 - 5s - loss: 0.7064 - sparse_categorical_accuracy: 0.7465 - val_loss: 0.7505 - val_sparse_categorical_accuracy: 0.7460\n",
            "Epoch 45/50\n",
            "196/196 - 5s - loss: 0.6948 - sparse_categorical_accuracy: 0.7518 - val_loss: 0.7526 - val_sparse_categorical_accuracy: 0.7470\n",
            "Epoch 46/50\n",
            "196/196 - 5s - loss: 0.6919 - sparse_categorical_accuracy: 0.7513 - val_loss: 0.7386 - val_sparse_categorical_accuracy: 0.7543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47/50\n",
            "196/196 - 5s - loss: 0.6892 - sparse_categorical_accuracy: 0.7534 - val_loss: 0.7293 - val_sparse_categorical_accuracy: 0.7579\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 48/50\n",
            "196/196 - 5s - loss: 0.6753 - sparse_categorical_accuracy: 0.7560 - val_loss: 0.7150 - val_sparse_categorical_accuracy: 0.7591\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_48_layer_call_fn, conv2d_48_layer_call_and_return_conditional_losses, conv2d_49_layer_call_fn, conv2d_49_layer_call_and_return_conditional_losses, conv2d_50_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 49/50\n",
            "196/196 - 5s - loss: 0.6799 - sparse_categorical_accuracy: 0.7564 - val_loss: 0.7488 - val_sparse_categorical_accuracy: 0.7478\n",
            "Epoch 50/50\n",
            "196/196 - 5s - loss: 0.6793 - sparse_categorical_accuracy: 0.7573 - val_loss: 0.7161 - val_sparse_categorical_accuracy: 0.7564\n",
            "Total training time 305.8742718696594 seconds\n",
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_12 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        2290      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        3122      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,200,846\n",
            "Trainable params: 1,200,846\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 8s - loss: 1.9780 - sparse_categorical_accuracy: 0.2752 - val_loss: 1.6667 - val_sparse_categorical_accuracy: 0.3907\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 6s - loss: 1.5893 - sparse_categorical_accuracy: 0.4215 - val_loss: 1.5629 - val_sparse_categorical_accuracy: 0.4209\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 6s - loss: 1.4244 - sparse_categorical_accuracy: 0.4871 - val_loss: 1.3489 - val_sparse_categorical_accuracy: 0.5107\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 6s - loss: 1.3285 - sparse_categorical_accuracy: 0.5261 - val_loss: 1.3113 - val_sparse_categorical_accuracy: 0.5333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 6s - loss: 1.2496 - sparse_categorical_accuracy: 0.5557 - val_loss: 1.3991 - val_sparse_categorical_accuracy: 0.5206\n",
            "Epoch 6/50\n",
            "196/196 - 6s - loss: 1.1928 - sparse_categorical_accuracy: 0.5773 - val_loss: 1.1978 - val_sparse_categorical_accuracy: 0.5836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 6s - loss: 1.1472 - sparse_categorical_accuracy: 0.5944 - val_loss: 1.1684 - val_sparse_categorical_accuracy: 0.5906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 6s - loss: 1.1178 - sparse_categorical_accuracy: 0.6041 - val_loss: 1.1495 - val_sparse_categorical_accuracy: 0.6011\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 6s - loss: 1.0721 - sparse_categorical_accuracy: 0.6220 - val_loss: 1.0376 - val_sparse_categorical_accuracy: 0.6388\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 6s - loss: 1.0534 - sparse_categorical_accuracy: 0.6282 - val_loss: 0.9871 - val_sparse_categorical_accuracy: 0.6544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 6s - loss: 1.0273 - sparse_categorical_accuracy: 0.6387 - val_loss: 1.0236 - val_sparse_categorical_accuracy: 0.6639\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 6s - loss: 0.9948 - sparse_categorical_accuracy: 0.6487 - val_loss: 0.9896 - val_sparse_categorical_accuracy: 0.6621\n",
            "Epoch 13/50\n",
            "196/196 - 6s - loss: 0.9730 - sparse_categorical_accuracy: 0.6590 - val_loss: 1.0100 - val_sparse_categorical_accuracy: 0.6640\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 - 6s - loss: 0.9508 - sparse_categorical_accuracy: 0.6656 - val_loss: 0.9567 - val_sparse_categorical_accuracy: 0.6781\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 6s - loss: 0.9310 - sparse_categorical_accuracy: 0.6721 - val_loss: 0.9722 - val_sparse_categorical_accuracy: 0.6996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 6s - loss: 0.9116 - sparse_categorical_accuracy: 0.6779 - val_loss: 0.9366 - val_sparse_categorical_accuracy: 0.6919\n",
            "Epoch 17/50\n",
            "196/196 - 6s - loss: 0.8991 - sparse_categorical_accuracy: 0.6835 - val_loss: 0.8528 - val_sparse_categorical_accuracy: 0.7105\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 - 6s - loss: 0.8818 - sparse_categorical_accuracy: 0.6897 - val_loss: 0.8438 - val_sparse_categorical_accuracy: 0.7050\n",
            "Epoch 19/50\n",
            "196/196 - 6s - loss: 0.8615 - sparse_categorical_accuracy: 0.6962 - val_loss: 0.9328 - val_sparse_categorical_accuracy: 0.6926\n",
            "Epoch 20/50\n",
            "196/196 - 6s - loss: 0.8516 - sparse_categorical_accuracy: 0.7017 - val_loss: 0.8685 - val_sparse_categorical_accuracy: 0.7070\n",
            "Epoch 21/50\n",
            "196/196 - 6s - loss: 0.8395 - sparse_categorical_accuracy: 0.7054 - val_loss: 0.8124 - val_sparse_categorical_accuracy: 0.7267\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 - 6s - loss: 0.8244 - sparse_categorical_accuracy: 0.7093 - val_loss: 0.8045 - val_sparse_categorical_accuracy: 0.7269\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 - 6s - loss: 0.8211 - sparse_categorical_accuracy: 0.7128 - val_loss: 0.8055 - val_sparse_categorical_accuracy: 0.7219\n",
            "Epoch 24/50\n",
            "196/196 - 6s - loss: 0.8053 - sparse_categorical_accuracy: 0.7183 - val_loss: 0.7877 - val_sparse_categorical_accuracy: 0.7316\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 - 6s - loss: 0.7925 - sparse_categorical_accuracy: 0.7208 - val_loss: 0.7892 - val_sparse_categorical_accuracy: 0.7366\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26/50\n",
            "196/196 - 6s - loss: 0.7749 - sparse_categorical_accuracy: 0.7273 - val_loss: 0.8070 - val_sparse_categorical_accuracy: 0.7336\n",
            "Epoch 27/50\n",
            "196/196 - 6s - loss: 0.7702 - sparse_categorical_accuracy: 0.7290 - val_loss: 0.8116 - val_sparse_categorical_accuracy: 0.7381\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 - 6s - loss: 0.7707 - sparse_categorical_accuracy: 0.7298 - val_loss: 0.7969 - val_sparse_categorical_accuracy: 0.7325\n",
            "Epoch 29/50\n",
            "196/196 - 6s - loss: 0.7663 - sparse_categorical_accuracy: 0.7312 - val_loss: 0.7615 - val_sparse_categorical_accuracy: 0.7370\n",
            "Epoch 30/50\n",
            "196/196 - 6s - loss: 0.7578 - sparse_categorical_accuracy: 0.7354 - val_loss: 0.8437 - val_sparse_categorical_accuracy: 0.7232\n",
            "Epoch 31/50\n",
            "196/196 - 6s - loss: 0.7517 - sparse_categorical_accuracy: 0.7351 - val_loss: 0.8149 - val_sparse_categorical_accuracy: 0.7445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32/50\n",
            "196/196 - 6s - loss: 0.7386 - sparse_categorical_accuracy: 0.7411 - val_loss: 0.7694 - val_sparse_categorical_accuracy: 0.7403\n",
            "Epoch 33/50\n",
            "196/196 - 6s - loss: 0.7315 - sparse_categorical_accuracy: 0.7411 - val_loss: 0.7760 - val_sparse_categorical_accuracy: 0.7437\n",
            "Epoch 34/50\n",
            "196/196 - 6s - loss: 0.7242 - sparse_categorical_accuracy: 0.7454 - val_loss: 0.7843 - val_sparse_categorical_accuracy: 0.7458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 - 6s - loss: 0.7183 - sparse_categorical_accuracy: 0.7459 - val_loss: 0.7687 - val_sparse_categorical_accuracy: 0.7510\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36/50\n",
            "196/196 - 6s - loss: 0.7093 - sparse_categorical_accuracy: 0.7488 - val_loss: 0.7552 - val_sparse_categorical_accuracy: 0.7420\n",
            "Epoch 37/50\n",
            "196/196 - 6s - loss: 0.7096 - sparse_categorical_accuracy: 0.7507 - val_loss: 0.7988 - val_sparse_categorical_accuracy: 0.7470\n",
            "Epoch 38/50\n",
            "196/196 - 6s - loss: 0.7019 - sparse_categorical_accuracy: 0.7495 - val_loss: 0.7847 - val_sparse_categorical_accuracy: 0.7469\n",
            "Epoch 39/50\n",
            "196/196 - 6s - loss: 0.6988 - sparse_categorical_accuracy: 0.7521 - val_loss: 0.7362 - val_sparse_categorical_accuracy: 0.7570\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40/50\n",
            "196/196 - 6s - loss: 0.6953 - sparse_categorical_accuracy: 0.7555 - val_loss: 0.7879 - val_sparse_categorical_accuracy: 0.7459\n",
            "Epoch 41/50\n",
            "196/196 - 6s - loss: 0.6844 - sparse_categorical_accuracy: 0.7591 - val_loss: 0.7795 - val_sparse_categorical_accuracy: 0.7509\n",
            "Epoch 42/50\n",
            "196/196 - 6s - loss: 0.6836 - sparse_categorical_accuracy: 0.7590 - val_loss: 0.7543 - val_sparse_categorical_accuracy: 0.7572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43/50\n",
            "196/196 - 6s - loss: 0.6763 - sparse_categorical_accuracy: 0.7602 - val_loss: 0.8417 - val_sparse_categorical_accuracy: 0.7443\n",
            "Epoch 44/50\n",
            "196/196 - 6s - loss: 0.6759 - sparse_categorical_accuracy: 0.7605 - val_loss: 0.7417 - val_sparse_categorical_accuracy: 0.7524\n",
            "Epoch 45/50\n",
            "196/196 - 6s - loss: 0.6682 - sparse_categorical_accuracy: 0.7639 - val_loss: 0.7887 - val_sparse_categorical_accuracy: 0.7492\n",
            "Epoch 46/50\n",
            "196/196 - 6s - loss: 0.6725 - sparse_categorical_accuracy: 0.7628 - val_loss: 0.8232 - val_sparse_categorical_accuracy: 0.7497\n",
            "Epoch 47/50\n",
            "196/196 - 6s - loss: 0.6683 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.8274 - val_sparse_categorical_accuracy: 0.7479\n",
            "Epoch 48/50\n",
            "196/196 - 6s - loss: 0.6577 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.7819 - val_sparse_categorical_accuracy: 0.7557\n",
            "Epoch 49/50\n",
            "196/196 - 6s - loss: 0.6538 - sparse_categorical_accuracy: 0.7683 - val_loss: 0.8958 - val_sparse_categorical_accuracy: 0.7450\n",
            "Epoch 50/50\n",
            "196/196 - 6s - loss: 0.6524 - sparse_categorical_accuracy: 0.7692 - val_loss: 0.7058 - val_sparse_categorical_accuracy: 0.7642\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_54_layer_call_fn, conv2d_54_layer_call_and_return_conditional_losses, conv2d_55_layer_call_fn, conv2d_55_layer_call_and_return_conditional_losses, conv2d_56_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total training time 363.6853120326996 seconds\n",
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_13 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        4068      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        5220      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,204,722\n",
            "Trainable params: 1,204,722\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 9s - loss: 1.9627 - sparse_categorical_accuracy: 0.2718 - val_loss: 1.7683 - val_sparse_categorical_accuracy: 0.3514\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 7s - loss: 1.5489 - sparse_categorical_accuracy: 0.4396 - val_loss: 1.4718 - val_sparse_categorical_accuracy: 0.4648\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 7s - loss: 1.3873 - sparse_categorical_accuracy: 0.5037 - val_loss: 1.4437 - val_sparse_categorical_accuracy: 0.5017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 7s - loss: 1.2864 - sparse_categorical_accuracy: 0.5445 - val_loss: 1.4587 - val_sparse_categorical_accuracy: 0.5011\n",
            "Epoch 5/50\n",
            "196/196 - 7s - loss: 1.2133 - sparse_categorical_accuracy: 0.5678 - val_loss: 1.2513 - val_sparse_categorical_accuracy: 0.5597\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 7s - loss: 1.1541 - sparse_categorical_accuracy: 0.5899 - val_loss: 1.1846 - val_sparse_categorical_accuracy: 0.5896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 7s - loss: 1.1055 - sparse_categorical_accuracy: 0.6094 - val_loss: 1.1011 - val_sparse_categorical_accuracy: 0.6226\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 7s - loss: 1.0708 - sparse_categorical_accuracy: 0.6227 - val_loss: 1.2018 - val_sparse_categorical_accuracy: 0.5921\n",
            "Epoch 9/50\n",
            "196/196 - 6s - loss: 1.0403 - sparse_categorical_accuracy: 0.6347 - val_loss: 1.0634 - val_sparse_categorical_accuracy: 0.6365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 7s - loss: 1.0138 - sparse_categorical_accuracy: 0.6453 - val_loss: 1.1469 - val_sparse_categorical_accuracy: 0.6134\n",
            "Epoch 11/50\n",
            "196/196 - 7s - loss: 0.9917 - sparse_categorical_accuracy: 0.6508 - val_loss: 1.0105 - val_sparse_categorical_accuracy: 0.6558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 7s - loss: 0.9617 - sparse_categorical_accuracy: 0.6595 - val_loss: 0.9663 - val_sparse_categorical_accuracy: 0.6647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 7s - loss: 0.9540 - sparse_categorical_accuracy: 0.6645 - val_loss: 0.9726 - val_sparse_categorical_accuracy: 0.6594\n",
            "Epoch 14/50\n",
            "196/196 - 7s - loss: 0.9270 - sparse_categorical_accuracy: 0.6719 - val_loss: 0.9380 - val_sparse_categorical_accuracy: 0.6763\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 7s - loss: 0.9040 - sparse_categorical_accuracy: 0.6789 - val_loss: 0.9240 - val_sparse_categorical_accuracy: 0.6859\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 7s - loss: 0.8996 - sparse_categorical_accuracy: 0.6833 - val_loss: 0.8897 - val_sparse_categorical_accuracy: 0.6944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 - 7s - loss: 0.8787 - sparse_categorical_accuracy: 0.6891 - val_loss: 0.8898 - val_sparse_categorical_accuracy: 0.6914\n",
            "Epoch 18/50\n",
            "196/196 - 7s - loss: 0.8707 - sparse_categorical_accuracy: 0.6938 - val_loss: 0.9370 - val_sparse_categorical_accuracy: 0.7009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 - 7s - loss: 0.8551 - sparse_categorical_accuracy: 0.6991 - val_loss: 0.8564 - val_sparse_categorical_accuracy: 0.7054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 - 7s - loss: 0.8487 - sparse_categorical_accuracy: 0.7024 - val_loss: 0.8296 - val_sparse_categorical_accuracy: 0.7143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 7s - loss: 0.8350 - sparse_categorical_accuracy: 0.7064 - val_loss: 0.9676 - val_sparse_categorical_accuracy: 0.6857\n",
            "Epoch 22/50\n",
            "196/196 - 7s - loss: 0.8193 - sparse_categorical_accuracy: 0.7113 - val_loss: 0.9065 - val_sparse_categorical_accuracy: 0.7005\n",
            "Epoch 23/50\n",
            "196/196 - 7s - loss: 0.8077 - sparse_categorical_accuracy: 0.7157 - val_loss: 0.8517 - val_sparse_categorical_accuracy: 0.7101\n",
            "Epoch 24/50\n",
            "196/196 - 7s - loss: 0.8037 - sparse_categorical_accuracy: 0.7183 - val_loss: 0.8814 - val_sparse_categorical_accuracy: 0.7006\n",
            "Epoch 25/50\n",
            "196/196 - 7s - loss: 0.7917 - sparse_categorical_accuracy: 0.7216 - val_loss: 0.8208 - val_sparse_categorical_accuracy: 0.7243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26/50\n",
            "196/196 - 7s - loss: 0.7788 - sparse_categorical_accuracy: 0.7254 - val_loss: 0.8103 - val_sparse_categorical_accuracy: 0.7219\n",
            "Epoch 27/50\n",
            "196/196 - 6s - loss: 0.7763 - sparse_categorical_accuracy: 0.7237 - val_loss: 0.8198 - val_sparse_categorical_accuracy: 0.7192\n",
            "Epoch 28/50\n",
            "196/196 - 6s - loss: 0.7552 - sparse_categorical_accuracy: 0.7332 - val_loss: 0.8402 - val_sparse_categorical_accuracy: 0.7237\n",
            "Epoch 29/50\n",
            "196/196 - 6s - loss: 0.7571 - sparse_categorical_accuracy: 0.7320 - val_loss: 0.8250 - val_sparse_categorical_accuracy: 0.7226\n",
            "Epoch 30/50\n",
            "196/196 - 6s - loss: 0.7525 - sparse_categorical_accuracy: 0.7339 - val_loss: 0.8177 - val_sparse_categorical_accuracy: 0.7289\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 - 7s - loss: 0.7471 - sparse_categorical_accuracy: 0.7362 - val_loss: 0.7717 - val_sparse_categorical_accuracy: 0.7377\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32/50\n",
            "196/196 - 7s - loss: 0.7386 - sparse_categorical_accuracy: 0.7396 - val_loss: 0.8792 - val_sparse_categorical_accuracy: 0.7190\n",
            "Epoch 33/50\n",
            "196/196 - 6s - loss: 0.7251 - sparse_categorical_accuracy: 0.7445 - val_loss: 0.8345 - val_sparse_categorical_accuracy: 0.7276\n",
            "Epoch 34/50\n",
            "196/196 - 7s - loss: 0.7199 - sparse_categorical_accuracy: 0.7456 - val_loss: 0.8208 - val_sparse_categorical_accuracy: 0.7285\n",
            "Epoch 35/50\n",
            "196/196 - 7s - loss: 0.7114 - sparse_categorical_accuracy: 0.7499 - val_loss: 0.7513 - val_sparse_categorical_accuracy: 0.7557\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_60_layer_call_fn, conv2d_60_layer_call_and_return_conditional_losses, conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, conv2d_62_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36/50\n",
            "196/196 - 7s - loss: 0.7057 - sparse_categorical_accuracy: 0.7520 - val_loss: 0.8716 - val_sparse_categorical_accuracy: 0.7342\n",
            "Epoch 37/50\n",
            "196/196 - 7s - loss: 0.6980 - sparse_categorical_accuracy: 0.7540 - val_loss: 0.7797 - val_sparse_categorical_accuracy: 0.7456\n",
            "Epoch 38/50\n",
            "196/196 - 7s - loss: 0.6863 - sparse_categorical_accuracy: 0.7569 - val_loss: 0.8275 - val_sparse_categorical_accuracy: 0.7305\n",
            "Epoch 39/50\n",
            "196/196 - 6s - loss: 0.6853 - sparse_categorical_accuracy: 0.7593 - val_loss: 0.8379 - val_sparse_categorical_accuracy: 0.7347\n",
            "Epoch 40/50\n",
            "196/196 - 6s - loss: 0.6798 - sparse_categorical_accuracy: 0.7607 - val_loss: 0.7856 - val_sparse_categorical_accuracy: 0.7457\n",
            "Epoch 41/50\n",
            "196/196 - 6s - loss: 0.6651 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.8091 - val_sparse_categorical_accuracy: 0.7539\n",
            "Epoch 42/50\n",
            "196/196 - 6s - loss: 0.6685 - sparse_categorical_accuracy: 0.7620 - val_loss: 0.8191 - val_sparse_categorical_accuracy: 0.7544\n",
            "Epoch 43/50\n",
            "196/196 - 6s - loss: 0.6652 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.8734 - val_sparse_categorical_accuracy: 0.7420\n",
            "Epoch 44/50\n",
            "196/196 - 6s - loss: 0.6602 - sparse_categorical_accuracy: 0.7689 - val_loss: 0.8962 - val_sparse_categorical_accuracy: 0.7309\n",
            "Epoch 45/50\n",
            "196/196 - 6s - loss: 0.6523 - sparse_categorical_accuracy: 0.7685 - val_loss: 0.7437 - val_sparse_categorical_accuracy: 0.7530\n",
            "Epoch 46/50\n",
            "196/196 - 6s - loss: 0.6477 - sparse_categorical_accuracy: 0.7709 - val_loss: 0.8057 - val_sparse_categorical_accuracy: 0.7428\n",
            "Epoch 47/50\n",
            "196/196 - 6s - loss: 0.6377 - sparse_categorical_accuracy: 0.7735 - val_loss: 0.7876 - val_sparse_categorical_accuracy: 0.7551\n",
            "Epoch 48/50\n",
            "196/196 - 6s - loss: 0.6333 - sparse_categorical_accuracy: 0.7770 - val_loss: 0.8176 - val_sparse_categorical_accuracy: 0.7387\n",
            "Epoch 49/50\n",
            "196/196 - 6s - loss: 0.6360 - sparse_categorical_accuracy: 0.7753 - val_loss: 0.8796 - val_sparse_categorical_accuracy: 0.7330\n",
            "Epoch 50/50\n",
            "196/196 - 7s - loss: 0.6359 - sparse_categorical_accuracy: 0.7755 - val_loss: 0.8433 - val_sparse_categorical_accuracy: 0.7291\n",
            "Total training time 379.8512051105499 seconds\n",
            "Model: \"model_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_14 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        7624      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        9416      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,212,474\n",
            "Trainable params: 1,212,474\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 10s - loss: 1.9267 - sparse_categorical_accuracy: 0.2938 - val_loss: 1.7535 - val_sparse_categorical_accuracy: 0.3407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 8s - loss: 1.5459 - sparse_categorical_accuracy: 0.4378 - val_loss: 1.5344 - val_sparse_categorical_accuracy: 0.4434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 8s - loss: 1.3935 - sparse_categorical_accuracy: 0.4980 - val_loss: 1.3578 - val_sparse_categorical_accuracy: 0.5195\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 8s - loss: 1.2863 - sparse_categorical_accuracy: 0.5412 - val_loss: 1.2533 - val_sparse_categorical_accuracy: 0.5539\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 8s - loss: 1.2152 - sparse_categorical_accuracy: 0.5685 - val_loss: 1.1621 - val_sparse_categorical_accuracy: 0.5900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 8s - loss: 1.1554 - sparse_categorical_accuracy: 0.5919 - val_loss: 1.1118 - val_sparse_categorical_accuracy: 0.6101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 8s - loss: 1.1165 - sparse_categorical_accuracy: 0.6029 - val_loss: 1.1586 - val_sparse_categorical_accuracy: 0.6045\n",
            "Epoch 8/50\n",
            "196/196 - 8s - loss: 1.0793 - sparse_categorical_accuracy: 0.6185 - val_loss: 1.0024 - val_sparse_categorical_accuracy: 0.6413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 8s - loss: 1.0505 - sparse_categorical_accuracy: 0.6263 - val_loss: 0.9880 - val_sparse_categorical_accuracy: 0.6466\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 8s - loss: 1.0191 - sparse_categorical_accuracy: 0.6363 - val_loss: 0.9665 - val_sparse_categorical_accuracy: 0.6556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 8s - loss: 0.9986 - sparse_categorical_accuracy: 0.6489 - val_loss: 0.9914 - val_sparse_categorical_accuracy: 0.6497\n",
            "Epoch 12/50\n",
            "196/196 - 8s - loss: 0.9769 - sparse_categorical_accuracy: 0.6531 - val_loss: 0.9325 - val_sparse_categorical_accuracy: 0.6680\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 8s - loss: 0.9507 - sparse_categorical_accuracy: 0.6622 - val_loss: 0.9107 - val_sparse_categorical_accuracy: 0.6776\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 - 8s - loss: 0.9272 - sparse_categorical_accuracy: 0.6708 - val_loss: 0.8983 - val_sparse_categorical_accuracy: 0.6874\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 8s - loss: 0.9168 - sparse_categorical_accuracy: 0.6749 - val_loss: 0.8634 - val_sparse_categorical_accuracy: 0.6973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 8s - loss: 0.8990 - sparse_categorical_accuracy: 0.6839 - val_loss: 0.8708 - val_sparse_categorical_accuracy: 0.6917\n",
            "Epoch 17/50\n",
            "196/196 - 8s - loss: 0.8873 - sparse_categorical_accuracy: 0.6864 - val_loss: 0.8537 - val_sparse_categorical_accuracy: 0.7043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 - 8s - loss: 0.8671 - sparse_categorical_accuracy: 0.6944 - val_loss: 0.8359 - val_sparse_categorical_accuracy: 0.7136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 - 8s - loss: 0.8619 - sparse_categorical_accuracy: 0.6960 - val_loss: 0.8224 - val_sparse_categorical_accuracy: 0.7089\n",
            "Epoch 20/50\n",
            "196/196 - 8s - loss: 0.8442 - sparse_categorical_accuracy: 0.7000 - val_loss: 0.8449 - val_sparse_categorical_accuracy: 0.7117\n",
            "Epoch 21/50\n",
            "196/196 - 8s - loss: 0.8396 - sparse_categorical_accuracy: 0.7040 - val_loss: 0.8327 - val_sparse_categorical_accuracy: 0.7084\n",
            "Epoch 22/50\n",
            "196/196 - 8s - loss: 0.8326 - sparse_categorical_accuracy: 0.7047 - val_loss: 0.8109 - val_sparse_categorical_accuracy: 0.7213\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 - 8s - loss: 0.8122 - sparse_categorical_accuracy: 0.7113 - val_loss: 0.8240 - val_sparse_categorical_accuracy: 0.7159\n",
            "Epoch 24/50\n",
            "196/196 - 8s - loss: 0.8061 - sparse_categorical_accuracy: 0.7125 - val_loss: 0.7873 - val_sparse_categorical_accuracy: 0.7275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 - 8s - loss: 0.8010 - sparse_categorical_accuracy: 0.7172 - val_loss: 0.8137 - val_sparse_categorical_accuracy: 0.7193\n",
            "Epoch 26/50\n",
            "196/196 - 8s - loss: 0.7850 - sparse_categorical_accuracy: 0.7216 - val_loss: 0.7827 - val_sparse_categorical_accuracy: 0.7310\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 - 8s - loss: 0.7810 - sparse_categorical_accuracy: 0.7241 - val_loss: 0.8202 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 28/50\n",
            "196/196 - 8s - loss: 0.7721 - sparse_categorical_accuracy: 0.7243 - val_loss: 0.7853 - val_sparse_categorical_accuracy: 0.7319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 - 8s - loss: 0.7573 - sparse_categorical_accuracy: 0.7294 - val_loss: 0.7622 - val_sparse_categorical_accuracy: 0.7377\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n",
            "196/196 - 8s - loss: 0.7567 - sparse_categorical_accuracy: 0.7316 - val_loss: 0.7615 - val_sparse_categorical_accuracy: 0.7383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 - 8s - loss: 0.7454 - sparse_categorical_accuracy: 0.7343 - val_loss: 0.7455 - val_sparse_categorical_accuracy: 0.7467\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32/50\n",
            "196/196 - 8s - loss: 0.7379 - sparse_categorical_accuracy: 0.7363 - val_loss: 0.7408 - val_sparse_categorical_accuracy: 0.7416\n",
            "Epoch 33/50\n",
            "196/196 - 8s - loss: 0.7347 - sparse_categorical_accuracy: 0.7377 - val_loss: 0.7555 - val_sparse_categorical_accuracy: 0.7435\n",
            "Epoch 34/50\n",
            "196/196 - 8s - loss: 0.7348 - sparse_categorical_accuracy: 0.7392 - val_loss: 0.7590 - val_sparse_categorical_accuracy: 0.7396\n",
            "Epoch 35/50\n",
            "196/196 - 8s - loss: 0.7184 - sparse_categorical_accuracy: 0.7455 - val_loss: 0.7747 - val_sparse_categorical_accuracy: 0.7364\n",
            "Epoch 36/50\n",
            "196/196 - 8s - loss: 0.7198 - sparse_categorical_accuracy: 0.7422 - val_loss: 0.7603 - val_sparse_categorical_accuracy: 0.7420\n",
            "Epoch 37/50\n",
            "196/196 - 8s - loss: 0.7076 - sparse_categorical_accuracy: 0.7474 - val_loss: 0.7399 - val_sparse_categorical_accuracy: 0.7468\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 - 8s - loss: 0.7099 - sparse_categorical_accuracy: 0.7478 - val_loss: 0.7454 - val_sparse_categorical_accuracy: 0.7451\n",
            "Epoch 39/50\n",
            "196/196 - 8s - loss: 0.6972 - sparse_categorical_accuracy: 0.7516 - val_loss: 0.7498 - val_sparse_categorical_accuracy: 0.7459\n",
            "Epoch 40/50\n",
            "196/196 - 8s - loss: 0.6841 - sparse_categorical_accuracy: 0.7543 - val_loss: 0.7212 - val_sparse_categorical_accuracy: 0.7532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 41/50\n",
            "196/196 - 8s - loss: 0.6912 - sparse_categorical_accuracy: 0.7560 - val_loss: 0.7466 - val_sparse_categorical_accuracy: 0.7469\n",
            "Epoch 42/50\n",
            "196/196 - 8s - loss: 0.6844 - sparse_categorical_accuracy: 0.7544 - val_loss: 0.7588 - val_sparse_categorical_accuracy: 0.7490\n",
            "Epoch 43/50\n",
            "196/196 - 8s - loss: 0.6789 - sparse_categorical_accuracy: 0.7595 - val_loss: 0.7188 - val_sparse_categorical_accuracy: 0.7593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 44/50\n",
            "196/196 - 8s - loss: 0.6824 - sparse_categorical_accuracy: 0.7574 - val_loss: 0.7140 - val_sparse_categorical_accuracy: 0.7585\n",
            "Epoch 45/50\n",
            "196/196 - 8s - loss: 0.6778 - sparse_categorical_accuracy: 0.7564 - val_loss: 0.7270 - val_sparse_categorical_accuracy: 0.7582\n",
            "Epoch 46/50\n",
            "196/196 - 8s - loss: 0.6636 - sparse_categorical_accuracy: 0.7636 - val_loss: 0.7390 - val_sparse_categorical_accuracy: 0.7593\n",
            "Epoch 47/50\n",
            "196/196 - 8s - loss: 0.6640 - sparse_categorical_accuracy: 0.7619 - val_loss: 0.7161 - val_sparse_categorical_accuracy: 0.7595\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 48/50\n",
            "196/196 - 8s - loss: 0.6516 - sparse_categorical_accuracy: 0.7672 - val_loss: 0.7313 - val_sparse_categorical_accuracy: 0.7637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_66_layer_call_fn, conv2d_66_layer_call_and_return_conditional_losses, conv2d_67_layer_call_fn, conv2d_67_layer_call_and_return_conditional_losses, conv2d_68_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_4hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 49/50\n",
            "196/196 - 8s - loss: 0.6594 - sparse_categorical_accuracy: 0.7657 - val_loss: 0.7241 - val_sparse_categorical_accuracy: 0.7617\n",
            "Epoch 50/50\n",
            "196/196 - 8s - loss: 0.6442 - sparse_categorical_accuracy: 0.7698 - val_loss: 0.7230 - val_sparse_categorical_accuracy: 0.7584\n",
            "Total training time 452.5747101306915 seconds\n",
            "Model: \"model_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_15 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        1273      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        1817      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,198,524\n",
            "Trainable params: 1,198,524\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 6s - loss: 1.9632 - sparse_categorical_accuracy: 0.2787 - val_loss: 1.7233 - val_sparse_categorical_accuracy: 0.3712\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 4s - loss: 1.6087 - sparse_categorical_accuracy: 0.4195 - val_loss: 1.5020 - val_sparse_categorical_accuracy: 0.4617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 5s - loss: 1.4405 - sparse_categorical_accuracy: 0.4828 - val_loss: 1.4100 - val_sparse_categorical_accuracy: 0.4998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 4s - loss: 1.3354 - sparse_categorical_accuracy: 0.5243 - val_loss: 1.3196 - val_sparse_categorical_accuracy: 0.5449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 4s - loss: 1.2531 - sparse_categorical_accuracy: 0.5532 - val_loss: 1.1821 - val_sparse_categorical_accuracy: 0.5820\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 4s - loss: 1.1919 - sparse_categorical_accuracy: 0.5757 - val_loss: 1.1607 - val_sparse_categorical_accuracy: 0.5839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 4s - loss: 1.1391 - sparse_categorical_accuracy: 0.5964 - val_loss: 1.0840 - val_sparse_categorical_accuracy: 0.6188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 4s - loss: 1.0933 - sparse_categorical_accuracy: 0.6139 - val_loss: 1.0185 - val_sparse_categorical_accuracy: 0.6410\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 5s - loss: 1.0539 - sparse_categorical_accuracy: 0.6297 - val_loss: 0.9961 - val_sparse_categorical_accuracy: 0.6532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 5s - loss: 1.0297 - sparse_categorical_accuracy: 0.6360 - val_loss: 0.9975 - val_sparse_categorical_accuracy: 0.6551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 5s - loss: 1.0020 - sparse_categorical_accuracy: 0.6470 - val_loss: 0.9826 - val_sparse_categorical_accuracy: 0.6575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 5s - loss: 0.9780 - sparse_categorical_accuracy: 0.6550 - val_loss: 0.9267 - val_sparse_categorical_accuracy: 0.6809\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 4s - loss: 0.9530 - sparse_categorical_accuracy: 0.6646 - val_loss: 0.9031 - val_sparse_categorical_accuracy: 0.6902\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 - 5s - loss: 0.9386 - sparse_categorical_accuracy: 0.6699 - val_loss: 0.8881 - val_sparse_categorical_accuracy: 0.6964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 5s - loss: 0.9180 - sparse_categorical_accuracy: 0.6775 - val_loss: 0.9186 - val_sparse_categorical_accuracy: 0.6827\n",
            "Epoch 16/50\n",
            "196/196 - 4s - loss: 0.9111 - sparse_categorical_accuracy: 0.6787 - val_loss: 0.8768 - val_sparse_categorical_accuracy: 0.6977\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 - 4s - loss: 0.8878 - sparse_categorical_accuracy: 0.6868 - val_loss: 0.9026 - val_sparse_categorical_accuracy: 0.6894\n",
            "Epoch 18/50\n",
            "196/196 - 4s - loss: 0.8779 - sparse_categorical_accuracy: 0.6912 - val_loss: 0.8653 - val_sparse_categorical_accuracy: 0.7014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 - 5s - loss: 0.8607 - sparse_categorical_accuracy: 0.6962 - val_loss: 0.8905 - val_sparse_categorical_accuracy: 0.6965\n",
            "Epoch 20/50\n",
            "196/196 - 4s - loss: 0.8431 - sparse_categorical_accuracy: 0.7062 - val_loss: 0.8424 - val_sparse_categorical_accuracy: 0.7115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 4s - loss: 0.8334 - sparse_categorical_accuracy: 0.7063 - val_loss: 0.9085 - val_sparse_categorical_accuracy: 0.6955\n",
            "Epoch 22/50\n",
            "196/196 - 4s - loss: 0.8259 - sparse_categorical_accuracy: 0.7075 - val_loss: 0.8956 - val_sparse_categorical_accuracy: 0.6924\n",
            "Epoch 23/50\n",
            "196/196 - 4s - loss: 0.8189 - sparse_categorical_accuracy: 0.7100 - val_loss: 0.8356 - val_sparse_categorical_accuracy: 0.7116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 - 4s - loss: 0.7989 - sparse_categorical_accuracy: 0.7192 - val_loss: 0.7875 - val_sparse_categorical_accuracy: 0.7285\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 - 4s - loss: 0.7981 - sparse_categorical_accuracy: 0.7185 - val_loss: 0.8216 - val_sparse_categorical_accuracy: 0.7147\n",
            "Epoch 26/50\n",
            "196/196 - 4s - loss: 0.7883 - sparse_categorical_accuracy: 0.7215 - val_loss: 0.7947 - val_sparse_categorical_accuracy: 0.7283\n",
            "Epoch 27/50\n",
            "196/196 - 4s - loss: 0.7732 - sparse_categorical_accuracy: 0.7265 - val_loss: 0.7891 - val_sparse_categorical_accuracy: 0.7309\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 - 4s - loss: 0.7687 - sparse_categorical_accuracy: 0.7293 - val_loss: 0.8193 - val_sparse_categorical_accuracy: 0.7167\n",
            "Epoch 29/50\n",
            "196/196 - 4s - loss: 0.7641 - sparse_categorical_accuracy: 0.7306 - val_loss: 0.8001 - val_sparse_categorical_accuracy: 0.7276\n",
            "Epoch 30/50\n",
            "196/196 - 4s - loss: 0.7565 - sparse_categorical_accuracy: 0.7316 - val_loss: 0.8595 - val_sparse_categorical_accuracy: 0.7111\n",
            "Epoch 31/50\n",
            "196/196 - 4s - loss: 0.7475 - sparse_categorical_accuracy: 0.7363 - val_loss: 0.7943 - val_sparse_categorical_accuracy: 0.7278\n",
            "Epoch 32/50\n",
            "196/196 - 4s - loss: 0.7438 - sparse_categorical_accuracy: 0.7371 - val_loss: 0.8143 - val_sparse_categorical_accuracy: 0.7241\n",
            "Epoch 33/50\n",
            "196/196 - 4s - loss: 0.7381 - sparse_categorical_accuracy: 0.7411 - val_loss: 0.7748 - val_sparse_categorical_accuracy: 0.7364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "196/196 - 4s - loss: 0.7348 - sparse_categorical_accuracy: 0.7404 - val_loss: 0.7631 - val_sparse_categorical_accuracy: 0.7408\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 - 4s - loss: 0.7234 - sparse_categorical_accuracy: 0.7436 - val_loss: 0.8121 - val_sparse_categorical_accuracy: 0.7209\n",
            "Epoch 36/50\n",
            "196/196 - 4s - loss: 0.7120 - sparse_categorical_accuracy: 0.7477 - val_loss: 0.7641 - val_sparse_categorical_accuracy: 0.7366\n",
            "Epoch 37/50\n",
            "196/196 - 4s - loss: 0.7062 - sparse_categorical_accuracy: 0.7515 - val_loss: 0.7463 - val_sparse_categorical_accuracy: 0.7468\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 - 4s - loss: 0.7024 - sparse_categorical_accuracy: 0.7520 - val_loss: 0.7927 - val_sparse_categorical_accuracy: 0.7409\n",
            "Epoch 39/50\n",
            "196/196 - 4s - loss: 0.7038 - sparse_categorical_accuracy: 0.7504 - val_loss: 0.7497 - val_sparse_categorical_accuracy: 0.7435\n",
            "Epoch 40/50\n",
            "196/196 - 4s - loss: 0.6985 - sparse_categorical_accuracy: 0.7540 - val_loss: 0.7644 - val_sparse_categorical_accuracy: 0.7433\n",
            "Epoch 41/50\n",
            "196/196 - 4s - loss: 0.6975 - sparse_categorical_accuracy: 0.7528 - val_loss: 0.8252 - val_sparse_categorical_accuracy: 0.7376\n",
            "Epoch 42/50\n",
            "196/196 - 4s - loss: 0.6842 - sparse_categorical_accuracy: 0.7567 - val_loss: 0.7371 - val_sparse_categorical_accuracy: 0.7566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43/50\n",
            "196/196 - 4s - loss: 0.6785 - sparse_categorical_accuracy: 0.7597 - val_loss: 0.7235 - val_sparse_categorical_accuracy: 0.7507\n",
            "Epoch 44/50\n",
            "196/196 - 4s - loss: 0.6771 - sparse_categorical_accuracy: 0.7613 - val_loss: 0.7460 - val_sparse_categorical_accuracy: 0.7460\n",
            "Epoch 45/50\n",
            "196/196 - 4s - loss: 0.6630 - sparse_categorical_accuracy: 0.7632 - val_loss: 0.7724 - val_sparse_categorical_accuracy: 0.7511\n",
            "Epoch 46/50\n",
            "196/196 - 4s - loss: 0.6669 - sparse_categorical_accuracy: 0.7620 - val_loss: 0.7774 - val_sparse_categorical_accuracy: 0.7458\n",
            "Epoch 47/50\n",
            "196/196 - 4s - loss: 0.6602 - sparse_categorical_accuracy: 0.7648 - val_loss: 0.7464 - val_sparse_categorical_accuracy: 0.7527\n",
            "Epoch 48/50\n",
            "196/196 - 4s - loss: 0.6601 - sparse_categorical_accuracy: 0.7647 - val_loss: 0.7515 - val_sparse_categorical_accuracy: 0.7541\n",
            "Epoch 49/50\n",
            "196/196 - 4s - loss: 0.6538 - sparse_categorical_accuracy: 0.7677 - val_loss: 0.7351 - val_sparse_categorical_accuracy: 0.7577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_72_layer_call_fn, conv2d_72_layer_call_and_return_conditional_losses, conv2d_73_layer_call_fn, conv2d_73_layer_call_and_return_conditional_losses, conv2d_74_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50/50\n",
            "196/196 - 5s - loss: 0.6524 - sparse_categorical_accuracy: 0.7685 - val_loss: 0.7681 - val_sparse_categorical_accuracy: 0.7552\n",
            "Total training time 290.45313692092896 seconds\n",
            "Model: \"model_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_16 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        2290      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        3122      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,200,846\n",
            "Trainable params: 1,200,846\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 8s - loss: 2.0412 - sparse_categorical_accuracy: 0.2419 - val_loss: 1.8762 - val_sparse_categorical_accuracy: 0.2939\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 6s - loss: 1.6810 - sparse_categorical_accuracy: 0.3797 - val_loss: 1.6290 - val_sparse_categorical_accuracy: 0.3976\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 6s - loss: 1.5086 - sparse_categorical_accuracy: 0.4503 - val_loss: 1.4018 - val_sparse_categorical_accuracy: 0.4844\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 6s - loss: 1.3985 - sparse_categorical_accuracy: 0.4966 - val_loss: 1.4941 - val_sparse_categorical_accuracy: 0.4717\n",
            "Epoch 5/50\n",
            "196/196 - 6s - loss: 1.3160 - sparse_categorical_accuracy: 0.5289 - val_loss: 1.3345 - val_sparse_categorical_accuracy: 0.5310\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 6s - loss: 1.2541 - sparse_categorical_accuracy: 0.5547 - val_loss: 1.3186 - val_sparse_categorical_accuracy: 0.5381\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 6s - loss: 1.2002 - sparse_categorical_accuracy: 0.5749 - val_loss: 1.1971 - val_sparse_categorical_accuracy: 0.5775\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 6s - loss: 1.1564 - sparse_categorical_accuracy: 0.5899 - val_loss: 1.1779 - val_sparse_categorical_accuracy: 0.5936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 6s - loss: 1.1217 - sparse_categorical_accuracy: 0.6017 - val_loss: 1.2734 - val_sparse_categorical_accuracy: 0.5669\n",
            "Epoch 10/50\n",
            "196/196 - 6s - loss: 1.0912 - sparse_categorical_accuracy: 0.6109 - val_loss: 1.0378 - val_sparse_categorical_accuracy: 0.6270\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 6s - loss: 1.0636 - sparse_categorical_accuracy: 0.6263 - val_loss: 1.0721 - val_sparse_categorical_accuracy: 0.6284\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 6s - loss: 1.0391 - sparse_categorical_accuracy: 0.6321 - val_loss: 1.1659 - val_sparse_categorical_accuracy: 0.5909\n",
            "Epoch 13/50\n",
            "196/196 - 6s - loss: 1.0175 - sparse_categorical_accuracy: 0.6407 - val_loss: 1.1328 - val_sparse_categorical_accuracy: 0.6246\n",
            "Epoch 14/50\n",
            "196/196 - 6s - loss: 0.9993 - sparse_categorical_accuracy: 0.6453 - val_loss: 1.0924 - val_sparse_categorical_accuracy: 0.6292\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 6s - loss: 0.9823 - sparse_categorical_accuracy: 0.6503 - val_loss: 1.1132 - val_sparse_categorical_accuracy: 0.6314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 6s - loss: 0.9682 - sparse_categorical_accuracy: 0.6579 - val_loss: 1.0726 - val_sparse_categorical_accuracy: 0.6459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 - 6s - loss: 0.9605 - sparse_categorical_accuracy: 0.6629 - val_loss: 1.1163 - val_sparse_categorical_accuracy: 0.6357\n",
            "Epoch 18/50\n",
            "196/196 - 6s - loss: 0.9346 - sparse_categorical_accuracy: 0.6680 - val_loss: 1.1181 - val_sparse_categorical_accuracy: 0.6392\n",
            "Epoch 19/50\n",
            "196/196 - 6s - loss: 0.9338 - sparse_categorical_accuracy: 0.6686 - val_loss: 1.1951 - val_sparse_categorical_accuracy: 0.6258\n",
            "Epoch 20/50\n",
            "196/196 - 6s - loss: 0.9200 - sparse_categorical_accuracy: 0.6769 - val_loss: 1.0391 - val_sparse_categorical_accuracy: 0.6528\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 6s - loss: 0.9079 - sparse_categorical_accuracy: 0.6795 - val_loss: 1.0178 - val_sparse_categorical_accuracy: 0.6657\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 - 6s - loss: 0.8944 - sparse_categorical_accuracy: 0.6836 - val_loss: 0.9698 - val_sparse_categorical_accuracy: 0.6730\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 - 6s - loss: 0.8862 - sparse_categorical_accuracy: 0.6893 - val_loss: 1.0200 - val_sparse_categorical_accuracy: 0.6617\n",
            "Epoch 24/50\n",
            "196/196 - 6s - loss: 0.8715 - sparse_categorical_accuracy: 0.6924 - val_loss: 1.0850 - val_sparse_categorical_accuracy: 0.6484\n",
            "Epoch 25/50\n",
            "196/196 - 6s - loss: 0.8634 - sparse_categorical_accuracy: 0.6952 - val_loss: 1.0332 - val_sparse_categorical_accuracy: 0.6543\n",
            "Epoch 26/50\n",
            "196/196 - 6s - loss: 0.8492 - sparse_categorical_accuracy: 0.6983 - val_loss: 1.0238 - val_sparse_categorical_accuracy: 0.6678\n",
            "Epoch 27/50\n",
            "196/196 - 6s - loss: 0.8441 - sparse_categorical_accuracy: 0.7015 - val_loss: 1.0075 - val_sparse_categorical_accuracy: 0.6763\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 - 6s - loss: 0.8430 - sparse_categorical_accuracy: 0.7005 - val_loss: 1.0170 - val_sparse_categorical_accuracy: 0.6862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 - 6s - loss: 0.8345 - sparse_categorical_accuracy: 0.7051 - val_loss: 1.0391 - val_sparse_categorical_accuracy: 0.6717\n",
            "Epoch 30/50\n",
            "196/196 - 6s - loss: 0.8282 - sparse_categorical_accuracy: 0.7067 - val_loss: 1.2057 - val_sparse_categorical_accuracy: 0.6613\n",
            "Epoch 31/50\n",
            "196/196 - 6s - loss: 0.8151 - sparse_categorical_accuracy: 0.7109 - val_loss: 1.1646 - val_sparse_categorical_accuracy: 0.6624\n",
            "Epoch 32/50\n",
            "196/196 - 6s - loss: 0.8141 - sparse_categorical_accuracy: 0.7111 - val_loss: 1.0366 - val_sparse_categorical_accuracy: 0.6793\n",
            "Epoch 33/50\n",
            "196/196 - 6s - loss: 0.7993 - sparse_categorical_accuracy: 0.7159 - val_loss: 1.1637 - val_sparse_categorical_accuracy: 0.6617\n",
            "Epoch 34/50\n",
            "196/196 - 6s - loss: 0.8007 - sparse_categorical_accuracy: 0.7152 - val_loss: 0.8881 - val_sparse_categorical_accuracy: 0.7025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 - 6s - loss: 0.7902 - sparse_categorical_accuracy: 0.7208 - val_loss: 0.9193 - val_sparse_categorical_accuracy: 0.7008\n",
            "Epoch 36/50\n",
            "196/196 - 6s - loss: 0.7837 - sparse_categorical_accuracy: 0.7223 - val_loss: 0.9874 - val_sparse_categorical_accuracy: 0.6933\n",
            "Epoch 37/50\n",
            "196/196 - 6s - loss: 0.7814 - sparse_categorical_accuracy: 0.7223 - val_loss: 0.9875 - val_sparse_categorical_accuracy: 0.6947\n",
            "Epoch 38/50\n",
            "196/196 - 6s - loss: 0.7860 - sparse_categorical_accuracy: 0.7208 - val_loss: 0.9848 - val_sparse_categorical_accuracy: 0.6920\n",
            "Epoch 39/50\n",
            "196/196 - 6s - loss: 0.7674 - sparse_categorical_accuracy: 0.7258 - val_loss: 1.0563 - val_sparse_categorical_accuracy: 0.6875\n",
            "Epoch 40/50\n",
            "196/196 - 6s - loss: 0.7650 - sparse_categorical_accuracy: 0.7271 - val_loss: 0.9576 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 41/50\n",
            "196/196 - 6s - loss: 0.7650 - sparse_categorical_accuracy: 0.7251 - val_loss: 1.0301 - val_sparse_categorical_accuracy: 0.6824\n",
            "Epoch 42/50\n",
            "196/196 - 6s - loss: 0.7534 - sparse_categorical_accuracy: 0.7323 - val_loss: 0.9742 - val_sparse_categorical_accuracy: 0.6898\n",
            "Epoch 43/50\n",
            "196/196 - 6s - loss: 0.7530 - sparse_categorical_accuracy: 0.7351 - val_loss: 1.1866 - val_sparse_categorical_accuracy: 0.6614\n",
            "Epoch 44/50\n",
            "196/196 - 6s - loss: 0.7433 - sparse_categorical_accuracy: 0.7344 - val_loss: 0.9993 - val_sparse_categorical_accuracy: 0.6852\n",
            "Epoch 45/50\n",
            "196/196 - 6s - loss: 0.7356 - sparse_categorical_accuracy: 0.7368 - val_loss: 0.8694 - val_sparse_categorical_accuracy: 0.7179\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_78_layer_call_fn, conv2d_78_layer_call_and_return_conditional_losses, conv2d_79_layer_call_fn, conv2d_79_layer_call_and_return_conditional_losses, conv2d_80_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 46/50\n",
            "196/196 - 6s - loss: 0.7347 - sparse_categorical_accuracy: 0.7389 - val_loss: 0.9438 - val_sparse_categorical_accuracy: 0.6998\n",
            "Epoch 47/50\n",
            "196/196 - 6s - loss: 0.7328 - sparse_categorical_accuracy: 0.7392 - val_loss: 1.0583 - val_sparse_categorical_accuracy: 0.6966\n",
            "Epoch 48/50\n",
            "196/196 - 6s - loss: 0.7252 - sparse_categorical_accuracy: 0.7453 - val_loss: 0.9671 - val_sparse_categorical_accuracy: 0.7041\n",
            "Epoch 49/50\n",
            "196/196 - 6s - loss: 0.7157 - sparse_categorical_accuracy: 0.7449 - val_loss: 1.0223 - val_sparse_categorical_accuracy: 0.7150\n",
            "Epoch 50/50\n",
            "196/196 - 6s - loss: 0.7163 - sparse_categorical_accuracy: 0.7458 - val_loss: 0.8752 - val_sparse_categorical_accuracy: 0.7168\n",
            "Total training time 357.4018998146057 seconds\n",
            "Model: \"model_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_17 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        4324      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        5732      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,205,490\n",
            "Trainable params: 1,205,490\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 9s - loss: 1.9065 - sparse_categorical_accuracy: 0.3059 - val_loss: 1.9563 - val_sparse_categorical_accuracy: 0.3142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 7s - loss: 1.5074 - sparse_categorical_accuracy: 0.4532 - val_loss: 2.1824 - val_sparse_categorical_accuracy: 0.3418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 7s - loss: 1.3643 - sparse_categorical_accuracy: 0.5132 - val_loss: 2.0305 - val_sparse_categorical_accuracy: 0.3885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 7s - loss: 1.2741 - sparse_categorical_accuracy: 0.5496 - val_loss: 1.6791 - val_sparse_categorical_accuracy: 0.4597\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 7s - loss: 1.2049 - sparse_categorical_accuracy: 0.5741 - val_loss: 2.3724 - val_sparse_categorical_accuracy: 0.3768\n",
            "Epoch 6/50\n",
            "196/196 - 7s - loss: 1.1520 - sparse_categorical_accuracy: 0.5948 - val_loss: 3.0174 - val_sparse_categorical_accuracy: 0.3642\n",
            "Epoch 7/50\n",
            "196/196 - 7s - loss: 1.1065 - sparse_categorical_accuracy: 0.6116 - val_loss: 1.8017 - val_sparse_categorical_accuracy: 0.4691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 7s - loss: 1.0599 - sparse_categorical_accuracy: 0.6280 - val_loss: 1.6924 - val_sparse_categorical_accuracy: 0.4815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 7s - loss: 1.0200 - sparse_categorical_accuracy: 0.6393 - val_loss: 2.1014 - val_sparse_categorical_accuracy: 0.4803\n",
            "Epoch 10/50\n",
            "196/196 - 7s - loss: 0.9830 - sparse_categorical_accuracy: 0.6549 - val_loss: 1.5823 - val_sparse_categorical_accuracy: 0.5260\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 7s - loss: 0.9600 - sparse_categorical_accuracy: 0.6671 - val_loss: 1.8752 - val_sparse_categorical_accuracy: 0.4856\n",
            "Epoch 12/50\n",
            "196/196 - 7s - loss: 0.9281 - sparse_categorical_accuracy: 0.6737 - val_loss: 1.3113 - val_sparse_categorical_accuracy: 0.6074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 7s - loss: 0.8997 - sparse_categorical_accuracy: 0.6852 - val_loss: 1.3377 - val_sparse_categorical_accuracy: 0.5913\n",
            "Epoch 14/50\n",
            "196/196 - 7s - loss: 0.8755 - sparse_categorical_accuracy: 0.6962 - val_loss: 1.1940 - val_sparse_categorical_accuracy: 0.6066\n",
            "Epoch 15/50\n",
            "196/196 - 7s - loss: 0.8536 - sparse_categorical_accuracy: 0.6997 - val_loss: 1.1927 - val_sparse_categorical_accuracy: 0.6227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 7s - loss: 0.8387 - sparse_categorical_accuracy: 0.7056 - val_loss: 1.3886 - val_sparse_categorical_accuracy: 0.5860\n",
            "Epoch 17/50\n",
            "196/196 - 7s - loss: 0.8159 - sparse_categorical_accuracy: 0.7168 - val_loss: 1.3905 - val_sparse_categorical_accuracy: 0.6064\n",
            "Epoch 18/50\n",
            "196/196 - 7s - loss: 0.8027 - sparse_categorical_accuracy: 0.7181 - val_loss: 1.3434 - val_sparse_categorical_accuracy: 0.6187\n",
            "Epoch 19/50\n",
            "196/196 - 7s - loss: 0.7809 - sparse_categorical_accuracy: 0.7275 - val_loss: 1.3100 - val_sparse_categorical_accuracy: 0.6346\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 - 7s - loss: 0.7700 - sparse_categorical_accuracy: 0.7313 - val_loss: 1.2866 - val_sparse_categorical_accuracy: 0.6351\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 7s - loss: 0.7586 - sparse_categorical_accuracy: 0.7372 - val_loss: 1.2780 - val_sparse_categorical_accuracy: 0.6440\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 - 7s - loss: 0.7408 - sparse_categorical_accuracy: 0.7414 - val_loss: 1.6199 - val_sparse_categorical_accuracy: 0.6061\n",
            "Epoch 23/50\n",
            "196/196 - 7s - loss: 0.7356 - sparse_categorical_accuracy: 0.7425 - val_loss: 0.9403 - val_sparse_categorical_accuracy: 0.6888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 - 7s - loss: 0.7274 - sparse_categorical_accuracy: 0.7468 - val_loss: 1.3469 - val_sparse_categorical_accuracy: 0.6382\n",
            "Epoch 25/50\n",
            "196/196 - 7s - loss: 0.7077 - sparse_categorical_accuracy: 0.7517 - val_loss: 1.9719 - val_sparse_categorical_accuracy: 0.6016\n",
            "Epoch 26/50\n",
            "196/196 - 7s - loss: 0.6914 - sparse_categorical_accuracy: 0.7588 - val_loss: 1.2563 - val_sparse_categorical_accuracy: 0.6577\n",
            "Epoch 27/50\n",
            "196/196 - 7s - loss: 0.6869 - sparse_categorical_accuracy: 0.7620 - val_loss: 1.3254 - val_sparse_categorical_accuracy: 0.6537\n",
            "Epoch 28/50\n",
            "196/196 - 7s - loss: 0.6795 - sparse_categorical_accuracy: 0.7617 - val_loss: 1.3751 - val_sparse_categorical_accuracy: 0.6780\n",
            "Epoch 29/50\n",
            "196/196 - 7s - loss: 0.6730 - sparse_categorical_accuracy: 0.7634 - val_loss: 1.4978 - val_sparse_categorical_accuracy: 0.6570\n",
            "Epoch 30/50\n",
            "196/196 - 7s - loss: 0.6582 - sparse_categorical_accuracy: 0.7689 - val_loss: 1.3422 - val_sparse_categorical_accuracy: 0.6494\n",
            "Epoch 31/50\n",
            "196/196 - 7s - loss: 0.6544 - sparse_categorical_accuracy: 0.7707 - val_loss: 1.3581 - val_sparse_categorical_accuracy: 0.6778\n",
            "Epoch 32/50\n",
            "196/196 - 7s - loss: 0.6426 - sparse_categorical_accuracy: 0.7758 - val_loss: 1.6501 - val_sparse_categorical_accuracy: 0.6182\n",
            "Epoch 33/50\n",
            "196/196 - 7s - loss: 0.6395 - sparse_categorical_accuracy: 0.7772 - val_loss: 1.5008 - val_sparse_categorical_accuracy: 0.6504\n",
            "Epoch 34/50\n",
            "196/196 - 7s - loss: 0.6274 - sparse_categorical_accuracy: 0.7792 - val_loss: 1.4047 - val_sparse_categorical_accuracy: 0.6616\n",
            "Epoch 35/50\n",
            "196/196 - 7s - loss: 0.6183 - sparse_categorical_accuracy: 0.7823 - val_loss: 1.5698 - val_sparse_categorical_accuracy: 0.6446\n",
            "Epoch 36/50\n",
            "196/196 - 7s - loss: 0.6099 - sparse_categorical_accuracy: 0.7845 - val_loss: 1.2275 - val_sparse_categorical_accuracy: 0.6964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37/50\n",
            "196/196 - 7s - loss: 0.6060 - sparse_categorical_accuracy: 0.7866 - val_loss: 1.1932 - val_sparse_categorical_accuracy: 0.6919\n",
            "Epoch 38/50\n",
            "196/196 - 7s - loss: 0.5992 - sparse_categorical_accuracy: 0.7895 - val_loss: 1.6604 - val_sparse_categorical_accuracy: 0.6524\n",
            "Epoch 39/50\n",
            "196/196 - 7s - loss: 0.5956 - sparse_categorical_accuracy: 0.7889 - val_loss: 1.4674 - val_sparse_categorical_accuracy: 0.6581\n",
            "Epoch 40/50\n",
            "196/196 - 7s - loss: 0.5931 - sparse_categorical_accuracy: 0.7945 - val_loss: 1.3003 - val_sparse_categorical_accuracy: 0.6857\n",
            "Epoch 41/50\n",
            "196/196 - 7s - loss: 0.5836 - sparse_categorical_accuracy: 0.7964 - val_loss: 1.2093 - val_sparse_categorical_accuracy: 0.7040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses, conv2d_86_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 42/50\n",
            "196/196 - 7s - loss: 0.5719 - sparse_categorical_accuracy: 0.7981 - val_loss: 1.3259 - val_sparse_categorical_accuracy: 0.6987\n",
            "Epoch 43/50\n",
            "196/196 - 7s - loss: 0.5676 - sparse_categorical_accuracy: 0.7986 - val_loss: 1.5123 - val_sparse_categorical_accuracy: 0.6909\n",
            "Epoch 44/50\n",
            "196/196 - 7s - loss: 0.5639 - sparse_categorical_accuracy: 0.8038 - val_loss: 1.8728 - val_sparse_categorical_accuracy: 0.6352\n",
            "Epoch 45/50\n",
            "196/196 - 7s - loss: 0.5566 - sparse_categorical_accuracy: 0.8041 - val_loss: 1.3714 - val_sparse_categorical_accuracy: 0.6797\n",
            "Epoch 46/50\n",
            "196/196 - 7s - loss: 0.5500 - sparse_categorical_accuracy: 0.8064 - val_loss: 1.2821 - val_sparse_categorical_accuracy: 0.6849\n",
            "Epoch 47/50\n",
            "196/196 - 7s - loss: 0.5499 - sparse_categorical_accuracy: 0.8080 - val_loss: 1.3835 - val_sparse_categorical_accuracy: 0.7000\n",
            "Epoch 48/50\n",
            "196/196 - 7s - loss: 0.5472 - sparse_categorical_accuracy: 0.8073 - val_loss: 1.2348 - val_sparse_categorical_accuracy: 0.7019\n",
            "Epoch 49/50\n",
            "196/196 - 7s - loss: 0.5454 - sparse_categorical_accuracy: 0.8058 - val_loss: 1.2918 - val_sparse_categorical_accuracy: 0.6875\n",
            "Epoch 50/50\n",
            "196/196 - 7s - loss: 0.5467 - sparse_categorical_accuracy: 0.8056 - val_loss: 1.6191 - val_sparse_categorical_accuracy: 0.6789\n",
            "Total training time 373.98096084594727 seconds\n",
            "Model: \"model_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_18 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        8392      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        10952     \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,214,778\n",
            "Trainable params: 1,214,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 11s - loss: 1.9702 - sparse_categorical_accuracy: 0.2662 - val_loss: 1.7326 - val_sparse_categorical_accuracy: 0.3566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 8s - loss: 1.5670 - sparse_categorical_accuracy: 0.4311 - val_loss: 1.7967 - val_sparse_categorical_accuracy: 0.3830\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 8s - loss: 1.3966 - sparse_categorical_accuracy: 0.4970 - val_loss: 2.0085 - val_sparse_categorical_accuracy: 0.4160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 8s - loss: 1.2960 - sparse_categorical_accuracy: 0.5368 - val_loss: 1.4226 - val_sparse_categorical_accuracy: 0.5126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 8s - loss: 1.2207 - sparse_categorical_accuracy: 0.5689 - val_loss: 1.3767 - val_sparse_categorical_accuracy: 0.5319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 8s - loss: 1.1737 - sparse_categorical_accuracy: 0.5867 - val_loss: 1.3888 - val_sparse_categorical_accuracy: 0.5477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 8s - loss: 1.1127 - sparse_categorical_accuracy: 0.6068 - val_loss: 1.3215 - val_sparse_categorical_accuracy: 0.5580\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 8s - loss: 1.0787 - sparse_categorical_accuracy: 0.6164 - val_loss: 1.1997 - val_sparse_categorical_accuracy: 0.5931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 8s - loss: 1.0478 - sparse_categorical_accuracy: 0.6301 - val_loss: 1.4705 - val_sparse_categorical_accuracy: 0.5734\n",
            "Epoch 10/50\n",
            "196/196 - 8s - loss: 1.0140 - sparse_categorical_accuracy: 0.6402 - val_loss: 1.2920 - val_sparse_categorical_accuracy: 0.5940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 8s - loss: 0.9970 - sparse_categorical_accuracy: 0.6499 - val_loss: 1.2451 - val_sparse_categorical_accuracy: 0.6056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 8s - loss: 0.9587 - sparse_categorical_accuracy: 0.6612 - val_loss: 1.1004 - val_sparse_categorical_accuracy: 0.6395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 8s - loss: 0.9434 - sparse_categorical_accuracy: 0.6675 - val_loss: 1.1388 - val_sparse_categorical_accuracy: 0.6288\n",
            "Epoch 14/50\n",
            "196/196 - 8s - loss: 0.9203 - sparse_categorical_accuracy: 0.6753 - val_loss: 1.0698 - val_sparse_categorical_accuracy: 0.6543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 8s - loss: 0.9114 - sparse_categorical_accuracy: 0.6781 - val_loss: 1.0830 - val_sparse_categorical_accuracy: 0.6606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 8s - loss: 0.8848 - sparse_categorical_accuracy: 0.6885 - val_loss: 1.1023 - val_sparse_categorical_accuracy: 0.6560\n",
            "Epoch 17/50\n",
            "196/196 - 8s - loss: 0.8692 - sparse_categorical_accuracy: 0.6943 - val_loss: 1.2374 - val_sparse_categorical_accuracy: 0.6499\n",
            "Epoch 18/50\n",
            "196/196 - 8s - loss: 0.8607 - sparse_categorical_accuracy: 0.6989 - val_loss: 1.2343 - val_sparse_categorical_accuracy: 0.6529\n",
            "Epoch 19/50\n",
            "196/196 - 8s - loss: 0.8392 - sparse_categorical_accuracy: 0.7046 - val_loss: 1.0696 - val_sparse_categorical_accuracy: 0.6676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 - 8s - loss: 0.8226 - sparse_categorical_accuracy: 0.7103 - val_loss: 1.0566 - val_sparse_categorical_accuracy: 0.6685\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 8s - loss: 0.8095 - sparse_categorical_accuracy: 0.7129 - val_loss: 0.9454 - val_sparse_categorical_accuracy: 0.7091\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 - 8s - loss: 0.8039 - sparse_categorical_accuracy: 0.7174 - val_loss: 1.1030 - val_sparse_categorical_accuracy: 0.6744\n",
            "Epoch 23/50\n",
            "196/196 - 8s - loss: 0.7891 - sparse_categorical_accuracy: 0.7206 - val_loss: 0.9911 - val_sparse_categorical_accuracy: 0.6943\n",
            "Epoch 24/50\n",
            "196/196 - 8s - loss: 0.7757 - sparse_categorical_accuracy: 0.7256 - val_loss: 1.1122 - val_sparse_categorical_accuracy: 0.6891\n",
            "Epoch 25/50\n",
            "196/196 - 8s - loss: 0.7692 - sparse_categorical_accuracy: 0.7308 - val_loss: 0.9762 - val_sparse_categorical_accuracy: 0.6989\n",
            "Epoch 26/50\n",
            "196/196 - 8s - loss: 0.7600 - sparse_categorical_accuracy: 0.7323 - val_loss: 0.9626 - val_sparse_categorical_accuracy: 0.7121\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 - 8s - loss: 0.7415 - sparse_categorical_accuracy: 0.7382 - val_loss: 1.0666 - val_sparse_categorical_accuracy: 0.6983\n",
            "Epoch 28/50\n",
            "196/196 - 8s - loss: 0.7377 - sparse_categorical_accuracy: 0.7402 - val_loss: 0.8454 - val_sparse_categorical_accuracy: 0.7213\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 - 8s - loss: 0.7351 - sparse_categorical_accuracy: 0.7414 - val_loss: 0.9747 - val_sparse_categorical_accuracy: 0.6977\n",
            "Epoch 30/50\n",
            "196/196 - 8s - loss: 0.7212 - sparse_categorical_accuracy: 0.7463 - val_loss: 1.0484 - val_sparse_categorical_accuracy: 0.7120\n",
            "Epoch 31/50\n",
            "196/196 - 8s - loss: 0.7151 - sparse_categorical_accuracy: 0.7473 - val_loss: 1.1572 - val_sparse_categorical_accuracy: 0.7028\n",
            "Epoch 32/50\n",
            "196/196 - 8s - loss: 0.7033 - sparse_categorical_accuracy: 0.7534 - val_loss: 0.9458 - val_sparse_categorical_accuracy: 0.7244\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33/50\n",
            "196/196 - 8s - loss: 0.7016 - sparse_categorical_accuracy: 0.7538 - val_loss: 0.9155 - val_sparse_categorical_accuracy: 0.7148\n",
            "Epoch 34/50\n",
            "196/196 - 8s - loss: 0.6869 - sparse_categorical_accuracy: 0.7570 - val_loss: 1.1105 - val_sparse_categorical_accuracy: 0.7028\n",
            "Epoch 35/50\n",
            "196/196 - 8s - loss: 0.6817 - sparse_categorical_accuracy: 0.7577 - val_loss: 0.8939 - val_sparse_categorical_accuracy: 0.7347\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36/50\n",
            "196/196 - 8s - loss: 0.6796 - sparse_categorical_accuracy: 0.7603 - val_loss: 1.1050 - val_sparse_categorical_accuracy: 0.7074\n",
            "Epoch 37/50\n",
            "196/196 - 8s - loss: 0.6724 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.9041 - val_sparse_categorical_accuracy: 0.7323\n",
            "Epoch 38/50\n",
            "196/196 - 8s - loss: 0.6596 - sparse_categorical_accuracy: 0.7668 - val_loss: 0.9643 - val_sparse_categorical_accuracy: 0.7224\n",
            "Epoch 39/50\n",
            "196/196 - 8s - loss: 0.6491 - sparse_categorical_accuracy: 0.7695 - val_loss: 0.9597 - val_sparse_categorical_accuracy: 0.7269\n",
            "Epoch 40/50\n",
            "196/196 - 8s - loss: 0.6586 - sparse_categorical_accuracy: 0.7666 - val_loss: 0.9180 - val_sparse_categorical_accuracy: 0.7356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 41/50\n",
            "196/196 - 8s - loss: 0.6445 - sparse_categorical_accuracy: 0.7751 - val_loss: 0.8421 - val_sparse_categorical_accuracy: 0.7571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_90_layer_call_fn, conv2d_90_layer_call_and_return_conditional_losses, conv2d_91_layer_call_fn, conv2d_91_layer_call_and_return_conditional_losses, conv2d_92_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_1udim_8hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 42/50\n",
            "196/196 - 8s - loss: 0.6457 - sparse_categorical_accuracy: 0.7727 - val_loss: 0.8829 - val_sparse_categorical_accuracy: 0.7426\n",
            "Epoch 43/50\n",
            "196/196 - 8s - loss: 0.6310 - sparse_categorical_accuracy: 0.7766 - val_loss: 0.8580 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 44/50\n",
            "196/196 - 8s - loss: 0.6336 - sparse_categorical_accuracy: 0.7747 - val_loss: 0.9855 - val_sparse_categorical_accuracy: 0.7349\n",
            "Epoch 45/50\n",
            "196/196 - 8s - loss: 0.6259 - sparse_categorical_accuracy: 0.7797 - val_loss: 0.8953 - val_sparse_categorical_accuracy: 0.7462\n",
            "Epoch 46/50\n",
            "196/196 - 8s - loss: 0.6189 - sparse_categorical_accuracy: 0.7826 - val_loss: 1.0701 - val_sparse_categorical_accuracy: 0.7373\n",
            "Epoch 47/50\n",
            "196/196 - 8s - loss: 0.6115 - sparse_categorical_accuracy: 0.7826 - val_loss: 0.8347 - val_sparse_categorical_accuracy: 0.7527\n",
            "Epoch 48/50\n",
            "196/196 - 8s - loss: 0.6120 - sparse_categorical_accuracy: 0.7831 - val_loss: 1.0544 - val_sparse_categorical_accuracy: 0.7232\n",
            "Epoch 49/50\n",
            "196/196 - 8s - loss: 0.6042 - sparse_categorical_accuracy: 0.7856 - val_loss: 0.8751 - val_sparse_categorical_accuracy: 0.7430\n",
            "Epoch 50/50\n",
            "196/196 - 8s - loss: 0.6062 - sparse_categorical_accuracy: 0.7852 - val_loss: 1.0938 - val_sparse_categorical_accuracy: 0.7214\n",
            "Total training time 445.89512753486633 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGScdXR7HZXv"
      },
      "source": [
        "##Exp 4 - 2 Heads, X u dim, Y Key dim\n",
        "Model : LAMBDA  \n",
        "Preprocessing : False  \n",
        "Batch size : 256  \n",
        "Key dim : 1,2,4,8  \n",
        "u dim : 2,4,8  \n",
        "Heads : 2  \n",
        "Pos. emb. size : 14  \n",
        "LambdaConv : False  \n",
        "\n",
        "---\n",
        "<pre>\n",
        "Q = k * hd       =  2, 4, 8,16|  2,  4,  8, 16|  2,  4,  8, 16|\n",
        "K = k * u        =  2, 4, 8,16|  4,  8, 16, 32|  8, 16, 32, 64|\n",
        "V = 64 // hd * u = 64,64,64,64|128,128,128,128|256,256,256,256|\n",
        "</pre>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCKaERn-PBme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cbd15e9-a136-4bed-8fcf-e3286141079f"
      },
      "source": [
        "hds = 2\n",
        "for u_dim in [2,4,8]:\n",
        "  for k_dim in [1,2,4,8]:\n",
        "    model = LambdaNetwork(num_classes=num_classes, \n",
        "                          image_height=image_height, \n",
        "                          image_width=image_width,\n",
        "                          k_dim=k_dim,\n",
        "                          u_dim=u_dim,\n",
        "                          num_heads=hds,\n",
        "                          n_r_size=14,\n",
        "                          local_contexts=False,\n",
        "                          preprocess=False).model()\n",
        "    model.summary()\n",
        "    if 'LAMBDA' not in history:\n",
        "      history['LAMBDA'] = dict()\n",
        "    if hds not in history['LAMBDA']:\n",
        "      history['LAMBDA'][hds] = dict()\n",
        "    if u_dim not in history['LAMBDA'][hds]:\n",
        "      history['LAMBDA'][hds][u_dim] = dict()\n",
        "    history['LAMBDA'][hds][u_dim][k_dim]= train_and_eval(model, SAVE_PATH + f'_LAMBDA_{k_dim}kdim_{u_dim}udim_{hds}hds', verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_19 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        3634      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        5810      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,204,878\n",
            "Trainable params: 1,204,878\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 8s - loss: 1.9848 - sparse_categorical_accuracy: 0.2683 - val_loss: 1.9983 - val_sparse_categorical_accuracy: 0.2794\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 6s - loss: 1.6183 - sparse_categorical_accuracy: 0.4104 - val_loss: 1.7271 - val_sparse_categorical_accuracy: 0.3780\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 6s - loss: 1.4622 - sparse_categorical_accuracy: 0.4720 - val_loss: 1.5954 - val_sparse_categorical_accuracy: 0.4461\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 6s - loss: 1.3537 - sparse_categorical_accuracy: 0.5106 - val_loss: 1.4314 - val_sparse_categorical_accuracy: 0.5074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 6s - loss: 1.2667 - sparse_categorical_accuracy: 0.5466 - val_loss: 1.3796 - val_sparse_categorical_accuracy: 0.5258\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 6s - loss: 1.2016 - sparse_categorical_accuracy: 0.5757 - val_loss: 1.2957 - val_sparse_categorical_accuracy: 0.5535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 6s - loss: 1.1465 - sparse_categorical_accuracy: 0.5911 - val_loss: 1.1711 - val_sparse_categorical_accuracy: 0.5924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 6s - loss: 1.1100 - sparse_categorical_accuracy: 0.6053 - val_loss: 1.0981 - val_sparse_categorical_accuracy: 0.6153\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 6s - loss: 1.0696 - sparse_categorical_accuracy: 0.6202 - val_loss: 1.1221 - val_sparse_categorical_accuracy: 0.6087\n",
            "Epoch 10/50\n",
            "196/196 - 6s - loss: 1.0400 - sparse_categorical_accuracy: 0.6328 - val_loss: 1.0946 - val_sparse_categorical_accuracy: 0.6122\n",
            "Epoch 11/50\n",
            "196/196 - 6s - loss: 1.0267 - sparse_categorical_accuracy: 0.6331 - val_loss: 1.0729 - val_sparse_categorical_accuracy: 0.6287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 6s - loss: 0.9956 - sparse_categorical_accuracy: 0.6482 - val_loss: 1.0319 - val_sparse_categorical_accuracy: 0.6391\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 6s - loss: 0.9809 - sparse_categorical_accuracy: 0.6520 - val_loss: 1.0238 - val_sparse_categorical_accuracy: 0.6498\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 - 6s - loss: 0.9528 - sparse_categorical_accuracy: 0.6614 - val_loss: 0.9767 - val_sparse_categorical_accuracy: 0.6654\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 6s - loss: 0.9379 - sparse_categorical_accuracy: 0.6667 - val_loss: 1.0253 - val_sparse_categorical_accuracy: 0.6512\n",
            "Epoch 16/50\n",
            "196/196 - 6s - loss: 0.9212 - sparse_categorical_accuracy: 0.6734 - val_loss: 0.9360 - val_sparse_categorical_accuracy: 0.6704\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 - 6s - loss: 0.9141 - sparse_categorical_accuracy: 0.6755 - val_loss: 0.8907 - val_sparse_categorical_accuracy: 0.6924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 - 6s - loss: 0.8915 - sparse_categorical_accuracy: 0.6850 - val_loss: 0.9173 - val_sparse_categorical_accuracy: 0.6863\n",
            "Epoch 19/50\n",
            "196/196 - 6s - loss: 0.8800 - sparse_categorical_accuracy: 0.6832 - val_loss: 0.9635 - val_sparse_categorical_accuracy: 0.6695\n",
            "Epoch 20/50\n",
            "196/196 - 6s - loss: 0.8676 - sparse_categorical_accuracy: 0.6927 - val_loss: 0.8796 - val_sparse_categorical_accuracy: 0.6924\n",
            "Epoch 21/50\n",
            "196/196 - 6s - loss: 0.8543 - sparse_categorical_accuracy: 0.6955 - val_loss: 0.8564 - val_sparse_categorical_accuracy: 0.7031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 - 6s - loss: 0.8473 - sparse_categorical_accuracy: 0.6958 - val_loss: 0.8434 - val_sparse_categorical_accuracy: 0.7072\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 - 6s - loss: 0.8308 - sparse_categorical_accuracy: 0.7046 - val_loss: 0.9117 - val_sparse_categorical_accuracy: 0.6879\n",
            "Epoch 24/50\n",
            "196/196 - 6s - loss: 0.8278 - sparse_categorical_accuracy: 0.7050 - val_loss: 0.8207 - val_sparse_categorical_accuracy: 0.7126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 - 6s - loss: 0.8074 - sparse_categorical_accuracy: 0.7128 - val_loss: 0.8289 - val_sparse_categorical_accuracy: 0.7114\n",
            "Epoch 26/50\n",
            "196/196 - 6s - loss: 0.8030 - sparse_categorical_accuracy: 0.7150 - val_loss: 0.8573 - val_sparse_categorical_accuracy: 0.7148\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 - 6s - loss: 0.7924 - sparse_categorical_accuracy: 0.7194 - val_loss: 0.8417 - val_sparse_categorical_accuracy: 0.7154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 - 6s - loss: 0.7821 - sparse_categorical_accuracy: 0.7225 - val_loss: 0.8425 - val_sparse_categorical_accuracy: 0.7161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 - 6s - loss: 0.7804 - sparse_categorical_accuracy: 0.7244 - val_loss: 0.8035 - val_sparse_categorical_accuracy: 0.7261\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n",
            "196/196 - 6s - loss: 0.7720 - sparse_categorical_accuracy: 0.7266 - val_loss: 0.8004 - val_sparse_categorical_accuracy: 0.7287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 - 6s - loss: 0.7622 - sparse_categorical_accuracy: 0.7304 - val_loss: 0.8396 - val_sparse_categorical_accuracy: 0.7161\n",
            "Epoch 32/50\n",
            "196/196 - 6s - loss: 0.7583 - sparse_categorical_accuracy: 0.7300 - val_loss: 0.7870 - val_sparse_categorical_accuracy: 0.7296\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33/50\n",
            "196/196 - 6s - loss: 0.7494 - sparse_categorical_accuracy: 0.7325 - val_loss: 0.7922 - val_sparse_categorical_accuracy: 0.7266\n",
            "Epoch 34/50\n",
            "196/196 - 6s - loss: 0.7459 - sparse_categorical_accuracy: 0.7359 - val_loss: 0.7820 - val_sparse_categorical_accuracy: 0.7336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 - 6s - loss: 0.7385 - sparse_categorical_accuracy: 0.7376 - val_loss: 0.8054 - val_sparse_categorical_accuracy: 0.7289\n",
            "Epoch 36/50\n",
            "196/196 - 6s - loss: 0.7290 - sparse_categorical_accuracy: 0.7414 - val_loss: 0.8155 - val_sparse_categorical_accuracy: 0.7309\n",
            "Epoch 37/50\n",
            "196/196 - 6s - loss: 0.7178 - sparse_categorical_accuracy: 0.7453 - val_loss: 0.8026 - val_sparse_categorical_accuracy: 0.7354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 - 6s - loss: 0.7183 - sparse_categorical_accuracy: 0.7439 - val_loss: 0.7772 - val_sparse_categorical_accuracy: 0.7351\n",
            "Epoch 39/50\n",
            "196/196 - 6s - loss: 0.7112 - sparse_categorical_accuracy: 0.7468 - val_loss: 0.7542 - val_sparse_categorical_accuracy: 0.7409\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40/50\n",
            "196/196 - 6s - loss: 0.7030 - sparse_categorical_accuracy: 0.7497 - val_loss: 0.7572 - val_sparse_categorical_accuracy: 0.7442\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 41/50\n",
            "196/196 - 6s - loss: 0.7047 - sparse_categorical_accuracy: 0.7501 - val_loss: 0.7693 - val_sparse_categorical_accuracy: 0.7353\n",
            "Epoch 42/50\n",
            "196/196 - 6s - loss: 0.6992 - sparse_categorical_accuracy: 0.7504 - val_loss: 0.7852 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 43/50\n",
            "196/196 - 6s - loss: 0.6838 - sparse_categorical_accuracy: 0.7564 - val_loss: 0.7463 - val_sparse_categorical_accuracy: 0.7464\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 44/50\n",
            "196/196 - 6s - loss: 0.6878 - sparse_categorical_accuracy: 0.7541 - val_loss: 0.7321 - val_sparse_categorical_accuracy: 0.7524\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 45/50\n",
            "196/196 - 6s - loss: 0.6777 - sparse_categorical_accuracy: 0.7568 - val_loss: 0.7703 - val_sparse_categorical_accuracy: 0.7396\n",
            "Epoch 46/50\n",
            "196/196 - 6s - loss: 0.6758 - sparse_categorical_accuracy: 0.7586 - val_loss: 0.7478 - val_sparse_categorical_accuracy: 0.7511\n",
            "Epoch 47/50\n",
            "196/196 - 6s - loss: 0.6719 - sparse_categorical_accuracy: 0.7610 - val_loss: 0.7373 - val_sparse_categorical_accuracy: 0.7561\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 48/50\n",
            "196/196 - 6s - loss: 0.6664 - sparse_categorical_accuracy: 0.7625 - val_loss: 0.7278 - val_sparse_categorical_accuracy: 0.7578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_96_layer_call_fn, conv2d_96_layer_call_and_return_conditional_losses, conv2d_97_layer_call_fn, conv2d_97_layer_call_and_return_conditional_losses, conv2d_98_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 49/50\n",
            "196/196 - 6s - loss: 0.6719 - sparse_categorical_accuracy: 0.7611 - val_loss: 0.7589 - val_sparse_categorical_accuracy: 0.7481\n",
            "Epoch 50/50\n",
            "196/196 - 6s - loss: 0.6529 - sparse_categorical_accuracy: 0.7680 - val_loss: 0.7436 - val_sparse_categorical_accuracy: 0.7562\n",
            "Total training time 368.6171123981476 seconds\n",
            "Model: \"model_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_20 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        5220      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        7524      \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,208,178\n",
            "Trainable params: 1,208,178\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 9s - loss: 1.9643 - sparse_categorical_accuracy: 0.2841 - val_loss: 1.6988 - val_sparse_categorical_accuracy: 0.3904\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 7s - loss: 1.5951 - sparse_categorical_accuracy: 0.4228 - val_loss: 1.6030 - val_sparse_categorical_accuracy: 0.4023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 7s - loss: 1.4448 - sparse_categorical_accuracy: 0.4794 - val_loss: 1.4249 - val_sparse_categorical_accuracy: 0.4856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 7s - loss: 1.3392 - sparse_categorical_accuracy: 0.5207 - val_loss: 1.2847 - val_sparse_categorical_accuracy: 0.5414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 7s - loss: 1.2544 - sparse_categorical_accuracy: 0.5508 - val_loss: 1.1614 - val_sparse_categorical_accuracy: 0.5882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 7s - loss: 1.1889 - sparse_categorical_accuracy: 0.5801 - val_loss: 1.1723 - val_sparse_categorical_accuracy: 0.5822\n",
            "Epoch 7/50\n",
            "196/196 - 7s - loss: 1.1344 - sparse_categorical_accuracy: 0.5975 - val_loss: 1.0781 - val_sparse_categorical_accuracy: 0.6200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 7s - loss: 1.0977 - sparse_categorical_accuracy: 0.6129 - val_loss: 1.0403 - val_sparse_categorical_accuracy: 0.6304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 7s - loss: 1.0581 - sparse_categorical_accuracy: 0.6263 - val_loss: 1.0748 - val_sparse_categorical_accuracy: 0.6198\n",
            "Epoch 10/50\n",
            "196/196 - 7s - loss: 1.0412 - sparse_categorical_accuracy: 0.6333 - val_loss: 1.0621 - val_sparse_categorical_accuracy: 0.6291\n",
            "Epoch 11/50\n",
            "196/196 - 7s - loss: 1.0105 - sparse_categorical_accuracy: 0.6437 - val_loss: 0.9866 - val_sparse_categorical_accuracy: 0.6500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 7s - loss: 0.9817 - sparse_categorical_accuracy: 0.6535 - val_loss: 0.9385 - val_sparse_categorical_accuracy: 0.6725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 7s - loss: 0.9630 - sparse_categorical_accuracy: 0.6606 - val_loss: 0.9193 - val_sparse_categorical_accuracy: 0.6807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 - 7s - loss: 0.9440 - sparse_categorical_accuracy: 0.6669 - val_loss: 0.9351 - val_sparse_categorical_accuracy: 0.6727\n",
            "Epoch 15/50\n",
            "196/196 - 7s - loss: 0.9306 - sparse_categorical_accuracy: 0.6712 - val_loss: 0.9329 - val_sparse_categorical_accuracy: 0.6749\n",
            "Epoch 16/50\n",
            "196/196 - 7s - loss: 0.9017 - sparse_categorical_accuracy: 0.6822 - val_loss: 0.9609 - val_sparse_categorical_accuracy: 0.6649\n",
            "Epoch 17/50\n",
            "196/196 - 7s - loss: 0.8933 - sparse_categorical_accuracy: 0.6819 - val_loss: 0.9146 - val_sparse_categorical_accuracy: 0.6832\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 - 7s - loss: 0.8811 - sparse_categorical_accuracy: 0.6910 - val_loss: 0.9027 - val_sparse_categorical_accuracy: 0.6857\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 - 7s - loss: 0.8604 - sparse_categorical_accuracy: 0.6960 - val_loss: 0.8556 - val_sparse_categorical_accuracy: 0.7031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 - 7s - loss: 0.8468 - sparse_categorical_accuracy: 0.7003 - val_loss: 0.8653 - val_sparse_categorical_accuracy: 0.6969\n",
            "Epoch 21/50\n",
            "196/196 - 7s - loss: 0.8380 - sparse_categorical_accuracy: 0.7047 - val_loss: 0.8466 - val_sparse_categorical_accuracy: 0.7097\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 - 7s - loss: 0.8259 - sparse_categorical_accuracy: 0.7076 - val_loss: 0.8803 - val_sparse_categorical_accuracy: 0.6937\n",
            "Epoch 23/50\n",
            "196/196 - 7s - loss: 0.8169 - sparse_categorical_accuracy: 0.7094 - val_loss: 0.8382 - val_sparse_categorical_accuracy: 0.7171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 - 7s - loss: 0.7996 - sparse_categorical_accuracy: 0.7171 - val_loss: 0.8458 - val_sparse_categorical_accuracy: 0.7097\n",
            "Epoch 25/50\n",
            "196/196 - 7s - loss: 0.7933 - sparse_categorical_accuracy: 0.7164 - val_loss: 0.8328 - val_sparse_categorical_accuracy: 0.7217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26/50\n",
            "196/196 - 7s - loss: 0.7861 - sparse_categorical_accuracy: 0.7211 - val_loss: 0.8743 - val_sparse_categorical_accuracy: 0.7030\n",
            "Epoch 27/50\n",
            "196/196 - 7s - loss: 0.7732 - sparse_categorical_accuracy: 0.7267 - val_loss: 0.9131 - val_sparse_categorical_accuracy: 0.7096\n",
            "Epoch 28/50\n",
            "196/196 - 7s - loss: 0.7616 - sparse_categorical_accuracy: 0.7294 - val_loss: 0.7902 - val_sparse_categorical_accuracy: 0.7345\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 - 7s - loss: 0.7524 - sparse_categorical_accuracy: 0.7346 - val_loss: 0.8075 - val_sparse_categorical_accuracy: 0.7295\n",
            "Epoch 30/50\n",
            "196/196 - 7s - loss: 0.7409 - sparse_categorical_accuracy: 0.7371 - val_loss: 0.7857 - val_sparse_categorical_accuracy: 0.7278\n",
            "Epoch 31/50\n",
            "196/196 - 7s - loss: 0.7370 - sparse_categorical_accuracy: 0.7375 - val_loss: 0.8163 - val_sparse_categorical_accuracy: 0.7284\n",
            "Epoch 32/50\n",
            "196/196 - 7s - loss: 0.7290 - sparse_categorical_accuracy: 0.7428 - val_loss: 0.7700 - val_sparse_categorical_accuracy: 0.7354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33/50\n",
            "196/196 - 7s - loss: 0.7204 - sparse_categorical_accuracy: 0.7460 - val_loss: 0.7849 - val_sparse_categorical_accuracy: 0.7341\n",
            "Epoch 34/50\n",
            "196/196 - 7s - loss: 0.7187 - sparse_categorical_accuracy: 0.7449 - val_loss: 0.8553 - val_sparse_categorical_accuracy: 0.7262\n",
            "Epoch 35/50\n",
            "196/196 - 7s - loss: 0.7106 - sparse_categorical_accuracy: 0.7471 - val_loss: 0.8084 - val_sparse_categorical_accuracy: 0.7405\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36/50\n",
            "196/196 - 7s - loss: 0.7104 - sparse_categorical_accuracy: 0.7472 - val_loss: 0.7577 - val_sparse_categorical_accuracy: 0.7487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37/50\n",
            "196/196 - 7s - loss: 0.7020 - sparse_categorical_accuracy: 0.7496 - val_loss: 0.7350 - val_sparse_categorical_accuracy: 0.7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 - 7s - loss: 0.6884 - sparse_categorical_accuracy: 0.7557 - val_loss: 0.7533 - val_sparse_categorical_accuracy: 0.7400\n",
            "Epoch 39/50\n",
            "196/196 - 7s - loss: 0.6849 - sparse_categorical_accuracy: 0.7553 - val_loss: 0.7902 - val_sparse_categorical_accuracy: 0.7457\n",
            "Epoch 40/50\n",
            "196/196 - 7s - loss: 0.6827 - sparse_categorical_accuracy: 0.7587 - val_loss: 0.7786 - val_sparse_categorical_accuracy: 0.7406\n",
            "Epoch 41/50\n",
            "196/196 - 7s - loss: 0.6761 - sparse_categorical_accuracy: 0.7610 - val_loss: 0.7236 - val_sparse_categorical_accuracy: 0.7518\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 42/50\n",
            "196/196 - 7s - loss: 0.6675 - sparse_categorical_accuracy: 0.7633 - val_loss: 0.7792 - val_sparse_categorical_accuracy: 0.7479\n",
            "Epoch 43/50\n",
            "196/196 - 7s - loss: 0.6648 - sparse_categorical_accuracy: 0.7654 - val_loss: 0.7326 - val_sparse_categorical_accuracy: 0.7559\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_102_layer_call_fn, conv2d_102_layer_call_and_return_conditional_losses, conv2d_103_layer_call_fn, conv2d_103_layer_call_and_return_conditional_losses, conv2d_104_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 44/50\n",
            "196/196 - 7s - loss: 0.6544 - sparse_categorical_accuracy: 0.7686 - val_loss: 0.7894 - val_sparse_categorical_accuracy: 0.7509\n",
            "Epoch 45/50\n",
            "196/196 - 7s - loss: 0.6487 - sparse_categorical_accuracy: 0.7719 - val_loss: 0.7702 - val_sparse_categorical_accuracy: 0.7505\n",
            "Epoch 46/50\n",
            "196/196 - 7s - loss: 0.6444 - sparse_categorical_accuracy: 0.7724 - val_loss: 0.7636 - val_sparse_categorical_accuracy: 0.7531\n",
            "Epoch 47/50\n",
            "196/196 - 7s - loss: 0.6503 - sparse_categorical_accuracy: 0.7678 - val_loss: 0.7784 - val_sparse_categorical_accuracy: 0.7463\n",
            "Epoch 48/50\n",
            "196/196 - 7s - loss: 0.6375 - sparse_categorical_accuracy: 0.7756 - val_loss: 0.7818 - val_sparse_categorical_accuracy: 0.7527\n",
            "Epoch 49/50\n",
            "196/196 - 7s - loss: 0.6233 - sparse_categorical_accuracy: 0.7768 - val_loss: 0.8595 - val_sparse_categorical_accuracy: 0.7398\n",
            "Epoch 50/50\n",
            "196/196 - 7s - loss: 0.6254 - sparse_categorical_accuracy: 0.7753 - val_loss: 0.7752 - val_sparse_categorical_accuracy: 0.7559\n",
            "Total training time 424.50683975219727 seconds\n",
            "Model: \"model_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_21 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        8392      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        10952     \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,214,778\n",
            "Trainable params: 1,214,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 11s - loss: 2.0076 - sparse_categorical_accuracy: 0.2560 - val_loss: 1.6901 - val_sparse_categorical_accuracy: 0.3653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 9s - loss: 1.5929 - sparse_categorical_accuracy: 0.4182 - val_loss: 1.5473 - val_sparse_categorical_accuracy: 0.4325\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 9s - loss: 1.3793 - sparse_categorical_accuracy: 0.5012 - val_loss: 1.6058 - val_sparse_categorical_accuracy: 0.4365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 9s - loss: 1.2399 - sparse_categorical_accuracy: 0.5582 - val_loss: 1.1615 - val_sparse_categorical_accuracy: 0.5923\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 9s - loss: 1.1502 - sparse_categorical_accuracy: 0.5905 - val_loss: 1.0925 - val_sparse_categorical_accuracy: 0.6141\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 9s - loss: 1.0831 - sparse_categorical_accuracy: 0.6186 - val_loss: 1.1183 - val_sparse_categorical_accuracy: 0.6090\n",
            "Epoch 7/50\n",
            "196/196 - 9s - loss: 1.0246 - sparse_categorical_accuracy: 0.6358 - val_loss: 1.0486 - val_sparse_categorical_accuracy: 0.6393\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 9s - loss: 0.9881 - sparse_categorical_accuracy: 0.6520 - val_loss: 0.9992 - val_sparse_categorical_accuracy: 0.6669\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 9s - loss: 0.9583 - sparse_categorical_accuracy: 0.6647 - val_loss: 0.9517 - val_sparse_categorical_accuracy: 0.6759\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 9s - loss: 0.9060 - sparse_categorical_accuracy: 0.6816 - val_loss: 0.8395 - val_sparse_categorical_accuracy: 0.7070\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 9s - loss: 0.8780 - sparse_categorical_accuracy: 0.6936 - val_loss: 0.8404 - val_sparse_categorical_accuracy: 0.7224\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 9s - loss: 0.8567 - sparse_categorical_accuracy: 0.7004 - val_loss: 0.8965 - val_sparse_categorical_accuracy: 0.6956\n",
            "Epoch 13/50\n",
            "196/196 - 9s - loss: 0.8328 - sparse_categorical_accuracy: 0.7103 - val_loss: 0.8115 - val_sparse_categorical_accuracy: 0.7214\n",
            "Epoch 14/50\n",
            "196/196 - 9s - loss: 0.8065 - sparse_categorical_accuracy: 0.7180 - val_loss: 0.8685 - val_sparse_categorical_accuracy: 0.7254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 9s - loss: 0.7853 - sparse_categorical_accuracy: 0.7249 - val_loss: 0.7831 - val_sparse_categorical_accuracy: 0.7498\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 9s - loss: 0.7738 - sparse_categorical_accuracy: 0.7315 - val_loss: 0.8010 - val_sparse_categorical_accuracy: 0.7437\n",
            "Epoch 17/50\n",
            "196/196 - 9s - loss: 0.7539 - sparse_categorical_accuracy: 0.7363 - val_loss: 0.8123 - val_sparse_categorical_accuracy: 0.7408\n",
            "Epoch 18/50\n",
            "196/196 - 9s - loss: 0.7380 - sparse_categorical_accuracy: 0.7427 - val_loss: 0.8181 - val_sparse_categorical_accuracy: 0.7482\n",
            "Epoch 19/50\n",
            "196/196 - 9s - loss: 0.7294 - sparse_categorical_accuracy: 0.7451 - val_loss: 0.8463 - val_sparse_categorical_accuracy: 0.7470\n",
            "Epoch 20/50\n",
            "196/196 - 9s - loss: 0.7209 - sparse_categorical_accuracy: 0.7481 - val_loss: 0.7539 - val_sparse_categorical_accuracy: 0.7573\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 9s - loss: 0.7023 - sparse_categorical_accuracy: 0.7562 - val_loss: 0.7041 - val_sparse_categorical_accuracy: 0.7539\n",
            "Epoch 22/50\n",
            "196/196 - 9s - loss: 0.6956 - sparse_categorical_accuracy: 0.7577 - val_loss: 0.8055 - val_sparse_categorical_accuracy: 0.7493\n",
            "Epoch 23/50\n",
            "196/196 - 8s - loss: 0.6811 - sparse_categorical_accuracy: 0.7634 - val_loss: 0.8187 - val_sparse_categorical_accuracy: 0.7629\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 - 9s - loss: 0.6688 - sparse_categorical_accuracy: 0.7673 - val_loss: 0.8677 - val_sparse_categorical_accuracy: 0.7389\n",
            "Epoch 25/50\n",
            "196/196 - 9s - loss: 0.6538 - sparse_categorical_accuracy: 0.7703 - val_loss: 0.7902 - val_sparse_categorical_accuracy: 0.7598\n",
            "Epoch 26/50\n",
            "196/196 - 9s - loss: 0.6521 - sparse_categorical_accuracy: 0.7739 - val_loss: 0.7338 - val_sparse_categorical_accuracy: 0.7638\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 - 9s - loss: 0.6412 - sparse_categorical_accuracy: 0.7777 - val_loss: 0.7214 - val_sparse_categorical_accuracy: 0.7652\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 - 9s - loss: 0.6286 - sparse_categorical_accuracy: 0.7808 - val_loss: 0.7600 - val_sparse_categorical_accuracy: 0.7495\n",
            "Epoch 29/50\n",
            "196/196 - 9s - loss: 0.6234 - sparse_categorical_accuracy: 0.7808 - val_loss: 0.7181 - val_sparse_categorical_accuracy: 0.7573\n",
            "Epoch 30/50\n",
            "196/196 - 9s - loss: 0.6262 - sparse_categorical_accuracy: 0.7825 - val_loss: 0.8340 - val_sparse_categorical_accuracy: 0.7681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 - 9s - loss: 0.6162 - sparse_categorical_accuracy: 0.7855 - val_loss: 0.7083 - val_sparse_categorical_accuracy: 0.7719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32/50\n",
            "196/196 - 9s - loss: 0.6003 - sparse_categorical_accuracy: 0.7895 - val_loss: 0.7667 - val_sparse_categorical_accuracy: 0.7691\n",
            "Epoch 33/50\n",
            "196/196 - 9s - loss: 0.5893 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.7362 - val_sparse_categorical_accuracy: 0.7770\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "196/196 - 9s - loss: 0.5838 - sparse_categorical_accuracy: 0.7941 - val_loss: 0.6982 - val_sparse_categorical_accuracy: 0.7774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 - 9s - loss: 0.5852 - sparse_categorical_accuracy: 0.7944 - val_loss: 0.7508 - val_sparse_categorical_accuracy: 0.7698\n",
            "Epoch 36/50\n",
            "196/196 - 9s - loss: 0.5780 - sparse_categorical_accuracy: 0.7990 - val_loss: 0.8184 - val_sparse_categorical_accuracy: 0.7716\n",
            "Epoch 37/50\n",
            "196/196 - 9s - loss: 0.5718 - sparse_categorical_accuracy: 0.7995 - val_loss: 0.7890 - val_sparse_categorical_accuracy: 0.7762\n",
            "Epoch 38/50\n",
            "196/196 - 9s - loss: 0.5581 - sparse_categorical_accuracy: 0.8056 - val_loss: 0.7626 - val_sparse_categorical_accuracy: 0.7833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39/50\n",
            "196/196 - 9s - loss: 0.5546 - sparse_categorical_accuracy: 0.8058 - val_loss: 0.8521 - val_sparse_categorical_accuracy: 0.7590\n",
            "Epoch 40/50\n",
            "196/196 - 9s - loss: 0.5573 - sparse_categorical_accuracy: 0.8043 - val_loss: 0.7390 - val_sparse_categorical_accuracy: 0.7737\n",
            "Epoch 41/50\n",
            "196/196 - 9s - loss: 0.5488 - sparse_categorical_accuracy: 0.8070 - val_loss: 0.7230 - val_sparse_categorical_accuracy: 0.7778\n",
            "Epoch 42/50\n",
            "196/196 - 8s - loss: 0.5403 - sparse_categorical_accuracy: 0.8105 - val_loss: 0.6800 - val_sparse_categorical_accuracy: 0.7851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43/50\n",
            "196/196 - 9s - loss: 0.5399 - sparse_categorical_accuracy: 0.8112 - val_loss: 0.7669 - val_sparse_categorical_accuracy: 0.7742\n",
            "Epoch 44/50\n",
            "196/196 - 9s - loss: 0.5349 - sparse_categorical_accuracy: 0.8119 - val_loss: 0.6584 - val_sparse_categorical_accuracy: 0.7876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 45/50\n",
            "196/196 - 9s - loss: 0.5305 - sparse_categorical_accuracy: 0.8143 - val_loss: 0.7441 - val_sparse_categorical_accuracy: 0.7831\n",
            "Epoch 46/50\n",
            "196/196 - 8s - loss: 0.5289 - sparse_categorical_accuracy: 0.8152 - val_loss: 0.7193 - val_sparse_categorical_accuracy: 0.7798\n",
            "Epoch 47/50\n",
            "196/196 - 9s - loss: 0.5245 - sparse_categorical_accuracy: 0.8154 - val_loss: 0.7647 - val_sparse_categorical_accuracy: 0.7846\n",
            "Epoch 48/50\n",
            "196/196 - 8s - loss: 0.5149 - sparse_categorical_accuracy: 0.8209 - val_loss: 0.7246 - val_sparse_categorical_accuracy: 0.7821\n",
            "Epoch 49/50\n",
            "196/196 - 9s - loss: 0.5080 - sparse_categorical_accuracy: 0.8219 - val_loss: 0.8421 - val_sparse_categorical_accuracy: 0.7831\n",
            "Epoch 50/50\n",
            "196/196 - 9s - loss: 0.5100 - sparse_categorical_accuracy: 0.8203 - val_loss: 0.7327 - val_sparse_categorical_accuracy: 0.7898\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_108_layer_call_fn, conv2d_108_layer_call_and_return_conditional_losses, conv2d_109_layer_call_fn, conv2d_109_layer_call_and_return_conditional_losses, conv2d_110_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total training time 492.0526487827301 seconds\n",
            "Model: \"model_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_22 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        14736     \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        17808     \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,227,978\n",
            "Trainable params: 1,227,978\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 13s - loss: 2.0118 - sparse_categorical_accuracy: 0.2476 - val_loss: 1.7639 - val_sparse_categorical_accuracy: 0.3641\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 11s - loss: 1.6055 - sparse_categorical_accuracy: 0.4141 - val_loss: 1.6726 - val_sparse_categorical_accuracy: 0.3914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 11s - loss: 1.4277 - sparse_categorical_accuracy: 0.4865 - val_loss: 1.4346 - val_sparse_categorical_accuracy: 0.4835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 11s - loss: 1.2973 - sparse_categorical_accuracy: 0.5359 - val_loss: 1.2785 - val_sparse_categorical_accuracy: 0.5442\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 11s - loss: 1.2091 - sparse_categorical_accuracy: 0.5703 - val_loss: 1.2084 - val_sparse_categorical_accuracy: 0.5790\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 11s - loss: 1.1327 - sparse_categorical_accuracy: 0.5989 - val_loss: 1.0955 - val_sparse_categorical_accuracy: 0.6066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 11s - loss: 1.0919 - sparse_categorical_accuracy: 0.6109 - val_loss: 1.0200 - val_sparse_categorical_accuracy: 0.6439\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 11s - loss: 1.0428 - sparse_categorical_accuracy: 0.6319 - val_loss: 1.0667 - val_sparse_categorical_accuracy: 0.6306\n",
            "Epoch 9/50\n",
            "196/196 - 11s - loss: 1.0082 - sparse_categorical_accuracy: 0.6441 - val_loss: 0.9835 - val_sparse_categorical_accuracy: 0.6613\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 11s - loss: 0.9729 - sparse_categorical_accuracy: 0.6558 - val_loss: 0.8985 - val_sparse_categorical_accuracy: 0.6878\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 11s - loss: 0.9471 - sparse_categorical_accuracy: 0.6678 - val_loss: 1.0902 - val_sparse_categorical_accuracy: 0.6472\n",
            "Epoch 12/50\n",
            "196/196 - 11s - loss: 0.9202 - sparse_categorical_accuracy: 0.6754 - val_loss: 0.9075 - val_sparse_categorical_accuracy: 0.6864\n",
            "Epoch 13/50\n",
            "196/196 - 11s - loss: 0.8963 - sparse_categorical_accuracy: 0.6844 - val_loss: 0.9587 - val_sparse_categorical_accuracy: 0.6802\n",
            "Epoch 14/50\n",
            "196/196 - 11s - loss: 0.8772 - sparse_categorical_accuracy: 0.6908 - val_loss: 0.8483 - val_sparse_categorical_accuracy: 0.7112\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 11s - loss: 0.8554 - sparse_categorical_accuracy: 0.6988 - val_loss: 0.8750 - val_sparse_categorical_accuracy: 0.7155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 11s - loss: 0.8362 - sparse_categorical_accuracy: 0.7057 - val_loss: 0.8992 - val_sparse_categorical_accuracy: 0.7094\n",
            "Epoch 17/50\n",
            "196/196 - 11s - loss: 0.8272 - sparse_categorical_accuracy: 0.7103 - val_loss: 0.8962 - val_sparse_categorical_accuracy: 0.7110\n",
            "Epoch 18/50\n",
            "196/196 - 11s - loss: 0.8141 - sparse_categorical_accuracy: 0.7150 - val_loss: 0.8056 - val_sparse_categorical_accuracy: 0.7289\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 - 11s - loss: 0.7845 - sparse_categorical_accuracy: 0.7247 - val_loss: 0.8019 - val_sparse_categorical_accuracy: 0.7262\n",
            "Epoch 20/50\n",
            "196/196 - 11s - loss: 0.7878 - sparse_categorical_accuracy: 0.7212 - val_loss: 0.8433 - val_sparse_categorical_accuracy: 0.7328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 11s - loss: 0.7747 - sparse_categorical_accuracy: 0.7257 - val_loss: 0.8321 - val_sparse_categorical_accuracy: 0.7355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 - 11s - loss: 0.7629 - sparse_categorical_accuracy: 0.7307 - val_loss: 0.8194 - val_sparse_categorical_accuracy: 0.7302\n",
            "Epoch 23/50\n",
            "196/196 - 11s - loss: 0.7524 - sparse_categorical_accuracy: 0.7329 - val_loss: 0.8575 - val_sparse_categorical_accuracy: 0.7358\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 - 11s - loss: 0.7432 - sparse_categorical_accuracy: 0.7403 - val_loss: 0.8684 - val_sparse_categorical_accuracy: 0.7449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 - 11s - loss: 0.7289 - sparse_categorical_accuracy: 0.7455 - val_loss: 0.7641 - val_sparse_categorical_accuracy: 0.7413\n",
            "Epoch 26/50\n",
            "196/196 - 11s - loss: 0.7176 - sparse_categorical_accuracy: 0.7494 - val_loss: 0.7689 - val_sparse_categorical_accuracy: 0.7512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 - 11s - loss: 0.7125 - sparse_categorical_accuracy: 0.7516 - val_loss: 0.8088 - val_sparse_categorical_accuracy: 0.7503\n",
            "Epoch 28/50\n",
            "196/196 - 11s - loss: 0.7050 - sparse_categorical_accuracy: 0.7507 - val_loss: 0.8314 - val_sparse_categorical_accuracy: 0.7507\n",
            "Epoch 29/50\n",
            "196/196 - 11s - loss: 0.6848 - sparse_categorical_accuracy: 0.7580 - val_loss: 0.8873 - val_sparse_categorical_accuracy: 0.7416\n",
            "Epoch 30/50\n",
            "196/196 - 11s - loss: 0.6831 - sparse_categorical_accuracy: 0.7600 - val_loss: 0.7864 - val_sparse_categorical_accuracy: 0.7520\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 - 11s - loss: 0.6751 - sparse_categorical_accuracy: 0.7625 - val_loss: 0.7630 - val_sparse_categorical_accuracy: 0.7564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32/50\n",
            "196/196 - 11s - loss: 0.6722 - sparse_categorical_accuracy: 0.7636 - val_loss: 0.8794 - val_sparse_categorical_accuracy: 0.7509\n",
            "Epoch 33/50\n",
            "196/196 - 11s - loss: 0.6583 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.7935 - val_sparse_categorical_accuracy: 0.7579\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "196/196 - 11s - loss: 0.6558 - sparse_categorical_accuracy: 0.7689 - val_loss: 0.7698 - val_sparse_categorical_accuracy: 0.7603\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 - 11s - loss: 0.6523 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.7964 - val_sparse_categorical_accuracy: 0.7500\n",
            "Epoch 36/50\n",
            "196/196 - 11s - loss: 0.6500 - sparse_categorical_accuracy: 0.7712 - val_loss: 0.8875 - val_sparse_categorical_accuracy: 0.7598\n",
            "Epoch 37/50\n",
            "196/196 - 11s - loss: 0.6384 - sparse_categorical_accuracy: 0.7725 - val_loss: 0.8131 - val_sparse_categorical_accuracy: 0.7627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 - 11s - loss: 0.6336 - sparse_categorical_accuracy: 0.7767 - val_loss: 0.7714 - val_sparse_categorical_accuracy: 0.7706\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39/50\n",
            "196/196 - 11s - loss: 0.6277 - sparse_categorical_accuracy: 0.7791 - val_loss: 0.8198 - val_sparse_categorical_accuracy: 0.7563\n",
            "Epoch 40/50\n",
            "196/196 - 11s - loss: 0.6225 - sparse_categorical_accuracy: 0.7783 - val_loss: 0.8909 - val_sparse_categorical_accuracy: 0.7533\n",
            "Epoch 41/50\n",
            "196/196 - 11s - loss: 0.6096 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.8474 - val_sparse_categorical_accuracy: 0.7611\n",
            "Epoch 42/50\n",
            "196/196 - 11s - loss: 0.6159 - sparse_categorical_accuracy: 0.7837 - val_loss: 0.7685 - val_sparse_categorical_accuracy: 0.7646\n",
            "Epoch 43/50\n",
            "196/196 - 11s - loss: 0.6055 - sparse_categorical_accuracy: 0.7870 - val_loss: 0.9693 - val_sparse_categorical_accuracy: 0.7457\n",
            "Epoch 44/50\n",
            "196/196 - 11s - loss: 0.5943 - sparse_categorical_accuracy: 0.7892 - val_loss: 0.8167 - val_sparse_categorical_accuracy: 0.7630\n",
            "Epoch 45/50\n",
            "196/196 - 11s - loss: 0.5945 - sparse_categorical_accuracy: 0.7883 - val_loss: 0.8373 - val_sparse_categorical_accuracy: 0.7628\n",
            "Epoch 46/50\n",
            "196/196 - 11s - loss: 0.5921 - sparse_categorical_accuracy: 0.7923 - val_loss: 0.8583 - val_sparse_categorical_accuracy: 0.7606\n",
            "Epoch 47/50\n",
            "196/196 - 11s - loss: 0.5809 - sparse_categorical_accuracy: 0.7955 - val_loss: 0.8272 - val_sparse_categorical_accuracy: 0.7718\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_114_layer_call_fn, conv2d_114_layer_call_and_return_conditional_losses, conv2d_115_layer_call_fn, conv2d_115_layer_call_and_return_conditional_losses, conv2d_116_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_2udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 48/50\n",
            "196/196 - 11s - loss: 0.5802 - sparse_categorical_accuracy: 0.7948 - val_loss: 0.8389 - val_sparse_categorical_accuracy: 0.7643\n",
            "Epoch 49/50\n",
            "196/196 - 11s - loss: 0.5840 - sparse_categorical_accuracy: 0.7934 - val_loss: 0.8203 - val_sparse_categorical_accuracy: 0.7707\n",
            "Epoch 50/50\n",
            "196/196 - 11s - loss: 0.5705 - sparse_categorical_accuracy: 0.7985 - val_loss: 0.8195 - val_sparse_categorical_accuracy: 0.7681\n",
            "Total training time 625.7674057483673 seconds\n",
            "Model: \"model_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_23 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        7204      \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        11492     \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,214,130\n",
            "Trainable params: 1,214,130\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 9s - loss: 1.9606 - sparse_categorical_accuracy: 0.2870 - val_loss: 1.8034 - val_sparse_categorical_accuracy: 0.3373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 7s - loss: 1.5720 - sparse_categorical_accuracy: 0.4324 - val_loss: 1.4871 - val_sparse_categorical_accuracy: 0.4632\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 7s - loss: 1.4312 - sparse_categorical_accuracy: 0.4865 - val_loss: 1.3744 - val_sparse_categorical_accuracy: 0.4999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 7s - loss: 1.3039 - sparse_categorical_accuracy: 0.5329 - val_loss: 1.2330 - val_sparse_categorical_accuracy: 0.5671\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 7s - loss: 1.2211 - sparse_categorical_accuracy: 0.5669 - val_loss: 1.1406 - val_sparse_categorical_accuracy: 0.5918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 7s - loss: 1.1431 - sparse_categorical_accuracy: 0.5969 - val_loss: 1.1664 - val_sparse_categorical_accuracy: 0.5833\n",
            "Epoch 7/50\n",
            "196/196 - 7s - loss: 1.0915 - sparse_categorical_accuracy: 0.6132 - val_loss: 1.0583 - val_sparse_categorical_accuracy: 0.6307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 7s - loss: 1.0302 - sparse_categorical_accuracy: 0.6336 - val_loss: 1.0207 - val_sparse_categorical_accuracy: 0.6486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 7s - loss: 0.9940 - sparse_categorical_accuracy: 0.6460 - val_loss: 0.9578 - val_sparse_categorical_accuracy: 0.6653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 7s - loss: 0.9573 - sparse_categorical_accuracy: 0.6619 - val_loss: 0.8919 - val_sparse_categorical_accuracy: 0.6843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 7s - loss: 0.9331 - sparse_categorical_accuracy: 0.6742 - val_loss: 1.0074 - val_sparse_categorical_accuracy: 0.6602\n",
            "Epoch 12/50\n",
            "196/196 - 7s - loss: 0.9001 - sparse_categorical_accuracy: 0.6824 - val_loss: 0.8672 - val_sparse_categorical_accuracy: 0.6956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 7s - loss: 0.8754 - sparse_categorical_accuracy: 0.6903 - val_loss: 0.8227 - val_sparse_categorical_accuracy: 0.7155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 - 7s - loss: 0.8527 - sparse_categorical_accuracy: 0.7011 - val_loss: 0.8558 - val_sparse_categorical_accuracy: 0.7071\n",
            "Epoch 15/50\n",
            "196/196 - 7s - loss: 0.8274 - sparse_categorical_accuracy: 0.7078 - val_loss: 0.8607 - val_sparse_categorical_accuracy: 0.6996\n",
            "Epoch 16/50\n",
            "196/196 - 7s - loss: 0.8099 - sparse_categorical_accuracy: 0.7148 - val_loss: 0.9052 - val_sparse_categorical_accuracy: 0.7156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 - 7s - loss: 0.7892 - sparse_categorical_accuracy: 0.7225 - val_loss: 0.8444 - val_sparse_categorical_accuracy: 0.7233\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 - 7s - loss: 0.7754 - sparse_categorical_accuracy: 0.7270 - val_loss: 0.8466 - val_sparse_categorical_accuracy: 0.7230\n",
            "Epoch 19/50\n",
            "196/196 - 7s - loss: 0.7626 - sparse_categorical_accuracy: 0.7295 - val_loss: 0.7749 - val_sparse_categorical_accuracy: 0.7310\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 - 7s - loss: 0.7451 - sparse_categorical_accuracy: 0.7383 - val_loss: 0.7618 - val_sparse_categorical_accuracy: 0.7384\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 7s - loss: 0.7314 - sparse_categorical_accuracy: 0.7443 - val_loss: 0.7981 - val_sparse_categorical_accuracy: 0.7334\n",
            "Epoch 22/50\n",
            "196/196 - 7s - loss: 0.7127 - sparse_categorical_accuracy: 0.7486 - val_loss: 0.7895 - val_sparse_categorical_accuracy: 0.7369\n",
            "Epoch 23/50\n",
            "196/196 - 7s - loss: 0.7069 - sparse_categorical_accuracy: 0.7505 - val_loss: 0.7285 - val_sparse_categorical_accuracy: 0.7537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 - 7s - loss: 0.6961 - sparse_categorical_accuracy: 0.7538 - val_loss: 0.7435 - val_sparse_categorical_accuracy: 0.7482\n",
            "Epoch 25/50\n",
            "196/196 - 7s - loss: 0.6791 - sparse_categorical_accuracy: 0.7599 - val_loss: 0.7645 - val_sparse_categorical_accuracy: 0.7490\n",
            "Epoch 26/50\n",
            "196/196 - 7s - loss: 0.6604 - sparse_categorical_accuracy: 0.7679 - val_loss: 0.7894 - val_sparse_categorical_accuracy: 0.7453\n",
            "Epoch 27/50\n",
            "196/196 - 7s - loss: 0.6597 - sparse_categorical_accuracy: 0.7669 - val_loss: 0.7294 - val_sparse_categorical_accuracy: 0.7582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 - 7s - loss: 0.6511 - sparse_categorical_accuracy: 0.7662 - val_loss: 0.7664 - val_sparse_categorical_accuracy: 0.7519\n",
            "Epoch 29/50\n",
            "196/196 - 7s - loss: 0.6448 - sparse_categorical_accuracy: 0.7730 - val_loss: 0.8068 - val_sparse_categorical_accuracy: 0.7528\n",
            "Epoch 30/50\n",
            "196/196 - 7s - loss: 0.6384 - sparse_categorical_accuracy: 0.7754 - val_loss: 0.7140 - val_sparse_categorical_accuracy: 0.7653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 - 7s - loss: 0.6252 - sparse_categorical_accuracy: 0.7812 - val_loss: 0.7557 - val_sparse_categorical_accuracy: 0.7605\n",
            "Epoch 32/50\n",
            "196/196 - 7s - loss: 0.6138 - sparse_categorical_accuracy: 0.7831 - val_loss: 0.7923 - val_sparse_categorical_accuracy: 0.7629\n",
            "Epoch 33/50\n",
            "196/196 - 7s - loss: 0.6065 - sparse_categorical_accuracy: 0.7861 - val_loss: 0.6847 - val_sparse_categorical_accuracy: 0.7675\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "196/196 - 7s - loss: 0.5999 - sparse_categorical_accuracy: 0.7882 - val_loss: 0.7940 - val_sparse_categorical_accuracy: 0.7598\n",
            "Epoch 35/50\n",
            "196/196 - 7s - loss: 0.5952 - sparse_categorical_accuracy: 0.7891 - val_loss: 0.7164 - val_sparse_categorical_accuracy: 0.7625\n",
            "Epoch 36/50\n",
            "196/196 - 7s - loss: 0.5841 - sparse_categorical_accuracy: 0.7952 - val_loss: 0.7161 - val_sparse_categorical_accuracy: 0.7654\n",
            "Epoch 37/50\n",
            "196/196 - 7s - loss: 0.5779 - sparse_categorical_accuracy: 0.7964 - val_loss: 0.7564 - val_sparse_categorical_accuracy: 0.7610\n",
            "Epoch 38/50\n",
            "196/196 - 7s - loss: 0.5777 - sparse_categorical_accuracy: 0.7963 - val_loss: 0.7326 - val_sparse_categorical_accuracy: 0.7606\n",
            "Epoch 39/50\n",
            "196/196 - 7s - loss: 0.5636 - sparse_categorical_accuracy: 0.8015 - val_loss: 0.7497 - val_sparse_categorical_accuracy: 0.7617\n",
            "Epoch 40/50\n",
            "196/196 - 7s - loss: 0.5684 - sparse_categorical_accuracy: 0.8003 - val_loss: 0.7774 - val_sparse_categorical_accuracy: 0.7627\n",
            "Epoch 41/50\n",
            "196/196 - 7s - loss: 0.5537 - sparse_categorical_accuracy: 0.8031 - val_loss: 0.7379 - val_sparse_categorical_accuracy: 0.7643\n",
            "Epoch 42/50\n",
            "196/196 - 7s - loss: 0.5439 - sparse_categorical_accuracy: 0.8083 - val_loss: 0.8754 - val_sparse_categorical_accuracy: 0.7569\n",
            "Epoch 43/50\n",
            "196/196 - 7s - loss: 0.5389 - sparse_categorical_accuracy: 0.8105 - val_loss: 0.7411 - val_sparse_categorical_accuracy: 0.7730\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 44/50\n",
            "196/196 - 7s - loss: 0.5424 - sparse_categorical_accuracy: 0.8094 - val_loss: 0.7714 - val_sparse_categorical_accuracy: 0.7681\n",
            "Epoch 45/50\n",
            "196/196 - 7s - loss: 0.5291 - sparse_categorical_accuracy: 0.8129 - val_loss: 0.7540 - val_sparse_categorical_accuracy: 0.7719\n",
            "Epoch 46/50\n",
            "196/196 - 7s - loss: 0.5279 - sparse_categorical_accuracy: 0.8133 - val_loss: 0.6925 - val_sparse_categorical_accuracy: 0.7779\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47/50\n",
            "196/196 - 7s - loss: 0.5225 - sparse_categorical_accuracy: 0.8159 - val_loss: 0.6890 - val_sparse_categorical_accuracy: 0.7705\n",
            "Epoch 48/50\n",
            "196/196 - 7s - loss: 0.5249 - sparse_categorical_accuracy: 0.8137 - val_loss: 0.7114 - val_sparse_categorical_accuracy: 0.7701\n",
            "Epoch 49/50\n",
            "196/196 - 7s - loss: 0.5116 - sparse_categorical_accuracy: 0.8177 - val_loss: 0.6783 - val_sparse_categorical_accuracy: 0.7819\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50/50\n",
            "196/196 - 7s - loss: 0.5090 - sparse_categorical_accuracy: 0.8213 - val_loss: 0.7105 - val_sparse_categorical_accuracy: 0.7825\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_120_layer_call_fn, conv2d_120_layer_call_and_return_conditional_losses, conv2d_121_layer_call_fn, conv2d_121_layer_call_and_return_conditional_losses, conv2d_122_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_1kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total training time 421.2387671470642 seconds\n",
            "Model: \"model_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_24 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        10312     \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        14792     \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,220,538\n",
            "Trainable params: 1,220,538\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 11s - loss: 1.9195 - sparse_categorical_accuracy: 0.2978 - val_loss: 1.7421 - val_sparse_categorical_accuracy: 0.4048\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 9s - loss: 1.5296 - sparse_categorical_accuracy: 0.4508 - val_loss: 1.4532 - val_sparse_categorical_accuracy: 0.4764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 9s - loss: 1.3831 - sparse_categorical_accuracy: 0.5066 - val_loss: 1.3110 - val_sparse_categorical_accuracy: 0.5384\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 9s - loss: 1.2766 - sparse_categorical_accuracy: 0.5449 - val_loss: 1.2125 - val_sparse_categorical_accuracy: 0.5803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 9s - loss: 1.1978 - sparse_categorical_accuracy: 0.5796 - val_loss: 1.1181 - val_sparse_categorical_accuracy: 0.6001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 9s - loss: 1.1298 - sparse_categorical_accuracy: 0.6024 - val_loss: 1.0730 - val_sparse_categorical_accuracy: 0.6182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 9s - loss: 1.0721 - sparse_categorical_accuracy: 0.6201 - val_loss: 0.9925 - val_sparse_categorical_accuracy: 0.6491\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 9s - loss: 1.0283 - sparse_categorical_accuracy: 0.6354 - val_loss: 1.0510 - val_sparse_categorical_accuracy: 0.6299\n",
            "Epoch 9/50\n",
            "196/196 - 9s - loss: 0.9909 - sparse_categorical_accuracy: 0.6534 - val_loss: 0.9936 - val_sparse_categorical_accuracy: 0.6579\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 9s - loss: 0.9679 - sparse_categorical_accuracy: 0.6592 - val_loss: 0.8894 - val_sparse_categorical_accuracy: 0.6835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 9s - loss: 0.9309 - sparse_categorical_accuracy: 0.6722 - val_loss: 0.8945 - val_sparse_categorical_accuracy: 0.6906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 - 9s - loss: 0.9056 - sparse_categorical_accuracy: 0.6819 - val_loss: 0.8658 - val_sparse_categorical_accuracy: 0.6919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 9s - loss: 0.8825 - sparse_categorical_accuracy: 0.6890 - val_loss: 0.8618 - val_sparse_categorical_accuracy: 0.6963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 - 9s - loss: 0.8578 - sparse_categorical_accuracy: 0.7006 - val_loss: 0.8726 - val_sparse_categorical_accuracy: 0.6945\n",
            "Epoch 15/50\n",
            "196/196 - 9s - loss: 0.8287 - sparse_categorical_accuracy: 0.7075 - val_loss: 0.8263 - val_sparse_categorical_accuracy: 0.7113\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 - 9s - loss: 0.8193 - sparse_categorical_accuracy: 0.7125 - val_loss: 0.8062 - val_sparse_categorical_accuracy: 0.7255\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 - 9s - loss: 0.8002 - sparse_categorical_accuracy: 0.7199 - val_loss: 0.7680 - val_sparse_categorical_accuracy: 0.7345\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 - 9s - loss: 0.7841 - sparse_categorical_accuracy: 0.7256 - val_loss: 0.7870 - val_sparse_categorical_accuracy: 0.7306\n",
            "Epoch 19/50\n",
            "196/196 - 9s - loss: 0.7718 - sparse_categorical_accuracy: 0.7306 - val_loss: 0.7789 - val_sparse_categorical_accuracy: 0.7328\n",
            "Epoch 20/50\n",
            "196/196 - 9s - loss: 0.7498 - sparse_categorical_accuracy: 0.7368 - val_loss: 0.7554 - val_sparse_categorical_accuracy: 0.7424\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 9s - loss: 0.7363 - sparse_categorical_accuracy: 0.7396 - val_loss: 0.7845 - val_sparse_categorical_accuracy: 0.7314\n",
            "Epoch 22/50\n",
            "196/196 - 9s - loss: 0.7336 - sparse_categorical_accuracy: 0.7450 - val_loss: 0.7678 - val_sparse_categorical_accuracy: 0.7440\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 - 9s - loss: 0.7153 - sparse_categorical_accuracy: 0.7477 - val_loss: 0.7341 - val_sparse_categorical_accuracy: 0.7482\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 - 9s - loss: 0.7081 - sparse_categorical_accuracy: 0.7540 - val_loss: 0.7490 - val_sparse_categorical_accuracy: 0.7475\n",
            "Epoch 25/50\n",
            "196/196 - 9s - loss: 0.6954 - sparse_categorical_accuracy: 0.7544 - val_loss: 0.8784 - val_sparse_categorical_accuracy: 0.7409\n",
            "Epoch 26/50\n",
            "196/196 - 9s - loss: 0.6781 - sparse_categorical_accuracy: 0.7638 - val_loss: 0.7427 - val_sparse_categorical_accuracy: 0.7536\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 - 9s - loss: 0.6658 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.7791 - val_sparse_categorical_accuracy: 0.7535\n",
            "Epoch 28/50\n",
            "196/196 - 9s - loss: 0.6563 - sparse_categorical_accuracy: 0.7701 - val_loss: 0.7680 - val_sparse_categorical_accuracy: 0.7582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 - 9s - loss: 0.6468 - sparse_categorical_accuracy: 0.7727 - val_loss: 0.7382 - val_sparse_categorical_accuracy: 0.7566\n",
            "Epoch 30/50\n",
            "196/196 - 9s - loss: 0.6503 - sparse_categorical_accuracy: 0.7709 - val_loss: 0.7171 - val_sparse_categorical_accuracy: 0.7641\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 - 9s - loss: 0.6413 - sparse_categorical_accuracy: 0.7759 - val_loss: 0.7639 - val_sparse_categorical_accuracy: 0.7689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32/50\n",
            "196/196 - 9s - loss: 0.6296 - sparse_categorical_accuracy: 0.7768 - val_loss: 0.7110 - val_sparse_categorical_accuracy: 0.7690\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33/50\n",
            "196/196 - 9s - loss: 0.6128 - sparse_categorical_accuracy: 0.7856 - val_loss: 0.7232 - val_sparse_categorical_accuracy: 0.7737\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "196/196 - 9s - loss: 0.6103 - sparse_categorical_accuracy: 0.7852 - val_loss: 0.8198 - val_sparse_categorical_accuracy: 0.7619\n",
            "Epoch 35/50\n",
            "196/196 - 9s - loss: 0.6032 - sparse_categorical_accuracy: 0.7872 - val_loss: 0.7558 - val_sparse_categorical_accuracy: 0.7694\n",
            "Epoch 36/50\n",
            "196/196 - 9s - loss: 0.5947 - sparse_categorical_accuracy: 0.7913 - val_loss: 0.7275 - val_sparse_categorical_accuracy: 0.7590\n",
            "Epoch 37/50\n",
            "196/196 - 9s - loss: 0.5942 - sparse_categorical_accuracy: 0.7905 - val_loss: 0.7892 - val_sparse_categorical_accuracy: 0.7638\n",
            "Epoch 38/50\n",
            "196/196 - 9s - loss: 0.5826 - sparse_categorical_accuracy: 0.7959 - val_loss: 0.7017 - val_sparse_categorical_accuracy: 0.7698\n",
            "Epoch 39/50\n",
            "196/196 - 9s - loss: 0.5694 - sparse_categorical_accuracy: 0.7974 - val_loss: 0.8471 - val_sparse_categorical_accuracy: 0.7646\n",
            "Epoch 40/50\n",
            "196/196 - 9s - loss: 0.5711 - sparse_categorical_accuracy: 0.7968 - val_loss: 0.7355 - val_sparse_categorical_accuracy: 0.7726\n",
            "Epoch 41/50\n",
            "196/196 - 9s - loss: 0.5635 - sparse_categorical_accuracy: 0.8031 - val_loss: 0.7424 - val_sparse_categorical_accuracy: 0.7733\n",
            "Epoch 42/50\n",
            "196/196 - 9s - loss: 0.5556 - sparse_categorical_accuracy: 0.8040 - val_loss: 0.7813 - val_sparse_categorical_accuracy: 0.7674\n",
            "Epoch 43/50\n",
            "196/196 - 9s - loss: 0.5508 - sparse_categorical_accuracy: 0.8059 - val_loss: 0.7601 - val_sparse_categorical_accuracy: 0.7671\n",
            "Epoch 44/50\n",
            "196/196 - 9s - loss: 0.5451 - sparse_categorical_accuracy: 0.8083 - val_loss: 0.7662 - val_sparse_categorical_accuracy: 0.7778\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 45/50\n",
            "196/196 - 9s - loss: 0.5404 - sparse_categorical_accuracy: 0.8110 - val_loss: 0.7185 - val_sparse_categorical_accuracy: 0.7793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 46/50\n",
            "196/196 - 9s - loss: 0.5396 - sparse_categorical_accuracy: 0.8115 - val_loss: 0.7255 - val_sparse_categorical_accuracy: 0.7814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_126_layer_call_fn, conv2d_126_layer_call_and_return_conditional_losses, conv2d_127_layer_call_fn, conv2d_127_layer_call_and_return_conditional_losses, conv2d_128_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_2kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47/50\n",
            "196/196 - 9s - loss: 0.5361 - sparse_categorical_accuracy: 0.8107 - val_loss: 0.6773 - val_sparse_categorical_accuracy: 0.7811\n",
            "Epoch 48/50\n",
            "196/196 - 9s - loss: 0.5179 - sparse_categorical_accuracy: 0.8183 - val_loss: 0.9062 - val_sparse_categorical_accuracy: 0.7740\n",
            "Epoch 49/50\n",
            "196/196 - 9s - loss: 0.5100 - sparse_categorical_accuracy: 0.8193 - val_loss: 0.7436 - val_sparse_categorical_accuracy: 0.7770\n",
            "Epoch 50/50\n",
            "196/196 - 9s - loss: 0.5182 - sparse_categorical_accuracy: 0.8173 - val_loss: 0.8550 - val_sparse_categorical_accuracy: 0.7693\n",
            "Total training time 526.223051071167 seconds\n",
            "Model: \"model_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_25 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        16528     \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        21392     \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,233,354\n",
            "Trainable params: 1,233,354\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 13s - loss: 1.8989 - sparse_categorical_accuracy: 0.3100 - val_loss: 1.6458 - val_sparse_categorical_accuracy: 0.3989\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 11s - loss: 1.5259 - sparse_categorical_accuracy: 0.4533 - val_loss: 1.4993 - val_sparse_categorical_accuracy: 0.4707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 11s - loss: 1.3429 - sparse_categorical_accuracy: 0.5231 - val_loss: 1.3651 - val_sparse_categorical_accuracy: 0.5278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 - 12s - loss: 1.2211 - sparse_categorical_accuracy: 0.5697 - val_loss: 1.2612 - val_sparse_categorical_accuracy: 0.5610\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 11s - loss: 1.1301 - sparse_categorical_accuracy: 0.6012 - val_loss: 1.1294 - val_sparse_categorical_accuracy: 0.6081\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 - 11s - loss: 1.0720 - sparse_categorical_accuracy: 0.6229 - val_loss: 1.1495 - val_sparse_categorical_accuracy: 0.6127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 - 11s - loss: 1.0197 - sparse_categorical_accuracy: 0.6411 - val_loss: 1.2869 - val_sparse_categorical_accuracy: 0.5808\n",
            "Epoch 8/50\n",
            "196/196 - 11s - loss: 0.9825 - sparse_categorical_accuracy: 0.6566 - val_loss: 0.9851 - val_sparse_categorical_accuracy: 0.6646\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 12s - loss: 0.9389 - sparse_categorical_accuracy: 0.6695 - val_loss: 0.9456 - val_sparse_categorical_accuracy: 0.6719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 12s - loss: 0.9166 - sparse_categorical_accuracy: 0.6787 - val_loss: 0.8891 - val_sparse_categorical_accuracy: 0.6970\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 12s - loss: 0.8793 - sparse_categorical_accuracy: 0.6924 - val_loss: 0.9503 - val_sparse_categorical_accuracy: 0.6727\n",
            "Epoch 12/50\n",
            "196/196 - 11s - loss: 0.8535 - sparse_categorical_accuracy: 0.6998 - val_loss: 0.8807 - val_sparse_categorical_accuracy: 0.7058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 - 12s - loss: 0.8364 - sparse_categorical_accuracy: 0.7060 - val_loss: 0.9361 - val_sparse_categorical_accuracy: 0.6946\n",
            "Epoch 14/50\n",
            "196/196 - 11s - loss: 0.8008 - sparse_categorical_accuracy: 0.7161 - val_loss: 0.9102 - val_sparse_categorical_accuracy: 0.7012\n",
            "Epoch 15/50\n",
            "196/196 - 11s - loss: 0.7927 - sparse_categorical_accuracy: 0.7238 - val_loss: 1.0360 - val_sparse_categorical_accuracy: 0.6710\n",
            "Epoch 16/50\n",
            "196/196 - 11s - loss: 0.7732 - sparse_categorical_accuracy: 0.7280 - val_loss: 0.8006 - val_sparse_categorical_accuracy: 0.7323\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 - 11s - loss: 0.7623 - sparse_categorical_accuracy: 0.7334 - val_loss: 0.8131 - val_sparse_categorical_accuracy: 0.7322\n",
            "Epoch 18/50\n",
            "196/196 - 11s - loss: 0.7357 - sparse_categorical_accuracy: 0.7428 - val_loss: 0.9031 - val_sparse_categorical_accuracy: 0.7128\n",
            "Epoch 19/50\n",
            "196/196 - 11s - loss: 0.7289 - sparse_categorical_accuracy: 0.7448 - val_loss: 0.9061 - val_sparse_categorical_accuracy: 0.7135\n",
            "Epoch 20/50\n",
            "196/196 - 11s - loss: 0.7100 - sparse_categorical_accuracy: 0.7509 - val_loss: 0.7684 - val_sparse_categorical_accuracy: 0.7452\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 11s - loss: 0.6993 - sparse_categorical_accuracy: 0.7526 - val_loss: 0.9029 - val_sparse_categorical_accuracy: 0.7293\n",
            "Epoch 22/50\n",
            "196/196 - 11s - loss: 0.6893 - sparse_categorical_accuracy: 0.7572 - val_loss: 0.7527 - val_sparse_categorical_accuracy: 0.7499\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 - 11s - loss: 0.6709 - sparse_categorical_accuracy: 0.7629 - val_loss: 0.8247 - val_sparse_categorical_accuracy: 0.7324\n",
            "Epoch 24/50\n",
            "196/196 - 11s - loss: 0.6666 - sparse_categorical_accuracy: 0.7640 - val_loss: 0.7950 - val_sparse_categorical_accuracy: 0.7403\n",
            "Epoch 25/50\n",
            "196/196 - 11s - loss: 0.6533 - sparse_categorical_accuracy: 0.7704 - val_loss: 0.8028 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 26/50\n",
            "196/196 - 11s - loss: 0.6422 - sparse_categorical_accuracy: 0.7741 - val_loss: 0.7277 - val_sparse_categorical_accuracy: 0.7600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 - 11s - loss: 0.6254 - sparse_categorical_accuracy: 0.7788 - val_loss: 0.7268 - val_sparse_categorical_accuracy: 0.7539\n",
            "Epoch 28/50\n",
            "196/196 - 11s - loss: 0.6255 - sparse_categorical_accuracy: 0.7783 - val_loss: 0.7997 - val_sparse_categorical_accuracy: 0.7500\n",
            "Epoch 29/50\n",
            "196/196 - 11s - loss: 0.6124 - sparse_categorical_accuracy: 0.7837 - val_loss: 0.8941 - val_sparse_categorical_accuracy: 0.7451\n",
            "Epoch 30/50\n",
            "196/196 - 11s - loss: 0.6049 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.8792 - val_sparse_categorical_accuracy: 0.7485\n",
            "Epoch 31/50\n",
            "196/196 - 11s - loss: 0.6053 - sparse_categorical_accuracy: 0.7874 - val_loss: 0.7910 - val_sparse_categorical_accuracy: 0.7496\n",
            "Epoch 32/50\n",
            "196/196 - 11s - loss: 0.5919 - sparse_categorical_accuracy: 0.7923 - val_loss: 0.7528 - val_sparse_categorical_accuracy: 0.7590\n",
            "Epoch 33/50\n",
            "196/196 - 11s - loss: 0.5793 - sparse_categorical_accuracy: 0.7935 - val_loss: 0.8518 - val_sparse_categorical_accuracy: 0.7503\n",
            "Epoch 34/50\n",
            "196/196 - 11s - loss: 0.5779 - sparse_categorical_accuracy: 0.7963 - val_loss: 0.6982 - val_sparse_categorical_accuracy: 0.7668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 - 11s - loss: 0.5700 - sparse_categorical_accuracy: 0.7979 - val_loss: 0.8500 - val_sparse_categorical_accuracy: 0.7416\n",
            "Epoch 36/50\n",
            "196/196 - 11s - loss: 0.5656 - sparse_categorical_accuracy: 0.8000 - val_loss: 0.8115 - val_sparse_categorical_accuracy: 0.7697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37/50\n",
            "196/196 - 12s - loss: 0.5526 - sparse_categorical_accuracy: 0.8045 - val_loss: 0.8872 - val_sparse_categorical_accuracy: 0.7556\n",
            "Epoch 38/50\n",
            "196/196 - 11s - loss: 0.5429 - sparse_categorical_accuracy: 0.8078 - val_loss: 0.8320 - val_sparse_categorical_accuracy: 0.7612\n",
            "Epoch 39/50\n",
            "196/196 - 11s - loss: 0.5424 - sparse_categorical_accuracy: 0.8080 - val_loss: 0.7457 - val_sparse_categorical_accuracy: 0.7695\n",
            "Epoch 40/50\n",
            "196/196 - 11s - loss: 0.5379 - sparse_categorical_accuracy: 0.8084 - val_loss: 0.8362 - val_sparse_categorical_accuracy: 0.7575\n",
            "Epoch 41/50\n",
            "196/196 - 11s - loss: 0.5295 - sparse_categorical_accuracy: 0.8131 - val_loss: 0.7884 - val_sparse_categorical_accuracy: 0.7589\n",
            "Epoch 42/50\n",
            "196/196 - 11s - loss: 0.5316 - sparse_categorical_accuracy: 0.8123 - val_loss: 0.7527 - val_sparse_categorical_accuracy: 0.7571\n",
            "Epoch 43/50\n",
            "196/196 - 11s - loss: 0.5245 - sparse_categorical_accuracy: 0.8143 - val_loss: 0.8087 - val_sparse_categorical_accuracy: 0.7658\n",
            "Epoch 44/50\n",
            "196/196 - 11s - loss: 0.5109 - sparse_categorical_accuracy: 0.8173 - val_loss: 0.8109 - val_sparse_categorical_accuracy: 0.7691\n",
            "Epoch 45/50\n",
            "196/196 - 11s - loss: 0.5058 - sparse_categorical_accuracy: 0.8208 - val_loss: 0.7343 - val_sparse_categorical_accuracy: 0.7733\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 46/50\n",
            "196/196 - 11s - loss: 0.5076 - sparse_categorical_accuracy: 0.8226 - val_loss: 0.8641 - val_sparse_categorical_accuracy: 0.7627\n",
            "Epoch 47/50\n",
            "196/196 - 11s - loss: 0.4987 - sparse_categorical_accuracy: 0.8230 - val_loss: 0.8453 - val_sparse_categorical_accuracy: 0.7655\n",
            "Epoch 48/50\n",
            "196/196 - 11s - loss: 0.4932 - sparse_categorical_accuracy: 0.8253 - val_loss: 0.7907 - val_sparse_categorical_accuracy: 0.7650\n",
            "Epoch 49/50\n",
            "196/196 - 11s - loss: 0.4851 - sparse_categorical_accuracy: 0.8265 - val_loss: 0.7802 - val_sparse_categorical_accuracy: 0.7747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50/50\n",
            "196/196 - 11s - loss: 0.4864 - sparse_categorical_accuracy: 0.8283 - val_loss: 0.7190 - val_sparse_categorical_accuracy: 0.7755\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_132_layer_call_fn, conv2d_132_layer_call_and_return_conditional_losses, conv2d_133_layer_call_fn, conv2d_133_layer_call_and_return_conditional_losses, conv2d_134_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_4kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total training time 625.0631093978882 seconds\n",
            "Model: \"model_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_26 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "rescale (Rescaling)          (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "resize (Resizing)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop2 (Dropout)              (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "lamb3 (Lambda)               (None, 14, 14, 64)        28960     \n",
            "_________________________________________________________________\n",
            "lamb4 (Lambda)               (None, 14, 14, 64)        34592     \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "drop4 (Dropout)              (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flat5 (Flatten)              (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dens5 (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "drop5 (Dropout)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "logits (Dense)               (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,258,986\n",
            "Trainable params: 1,258,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 - 18s - loss: 1.9793 - sparse_categorical_accuracy: 0.2702 - val_loss: 1.7654 - val_sparse_categorical_accuracy: 0.3627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 - 16s - loss: 1.4897 - sparse_categorical_accuracy: 0.4645 - val_loss: 1.4609 - val_sparse_categorical_accuracy: 0.4691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 - 16s - loss: 1.2521 - sparse_categorical_accuracy: 0.5572 - val_loss: 1.8857 - val_sparse_categorical_accuracy: 0.4562\n",
            "Epoch 4/50\n",
            "196/196 - 16s - loss: 1.1283 - sparse_categorical_accuracy: 0.6038 - val_loss: 1.0623 - val_sparse_categorical_accuracy: 0.6247\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 - 16s - loss: 1.0438 - sparse_categorical_accuracy: 0.6316 - val_loss: 1.1105 - val_sparse_categorical_accuracy: 0.6199\n",
            "Epoch 6/50\n",
            "196/196 - 16s - loss: 0.9834 - sparse_categorical_accuracy: 0.6544 - val_loss: 1.1843 - val_sparse_categorical_accuracy: 0.6158\n",
            "Epoch 7/50\n",
            "196/196 - 16s - loss: 0.9288 - sparse_categorical_accuracy: 0.6757 - val_loss: 0.9684 - val_sparse_categorical_accuracy: 0.6701\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 - 16s - loss: 0.8845 - sparse_categorical_accuracy: 0.6880 - val_loss: 0.9048 - val_sparse_categorical_accuracy: 0.6855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 - 16s - loss: 0.8484 - sparse_categorical_accuracy: 0.7040 - val_loss: 0.8296 - val_sparse_categorical_accuracy: 0.7147\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 - 16s - loss: 0.8211 - sparse_categorical_accuracy: 0.7122 - val_loss: 0.8289 - val_sparse_categorical_accuracy: 0.7245\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 - 16s - loss: 0.7954 - sparse_categorical_accuracy: 0.7213 - val_loss: 0.8690 - val_sparse_categorical_accuracy: 0.7204\n",
            "Epoch 12/50\n",
            "196/196 - 16s - loss: 0.7642 - sparse_categorical_accuracy: 0.7324 - val_loss: 0.9036 - val_sparse_categorical_accuracy: 0.7084\n",
            "Epoch 13/50\n",
            "196/196 - 16s - loss: 0.7487 - sparse_categorical_accuracy: 0.7386 - val_loss: 0.8667 - val_sparse_categorical_accuracy: 0.7284\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 - 16s - loss: 0.7198 - sparse_categorical_accuracy: 0.7486 - val_loss: 0.7339 - val_sparse_categorical_accuracy: 0.7484\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 - 16s - loss: 0.7016 - sparse_categorical_accuracy: 0.7557 - val_loss: 0.7750 - val_sparse_categorical_accuracy: 0.7339\n",
            "Epoch 16/50\n",
            "196/196 - 16s - loss: 0.6894 - sparse_categorical_accuracy: 0.7610 - val_loss: 0.7873 - val_sparse_categorical_accuracy: 0.7344\n",
            "Epoch 17/50\n",
            "196/196 - 16s - loss: 0.6714 - sparse_categorical_accuracy: 0.7643 - val_loss: 1.0197 - val_sparse_categorical_accuracy: 0.7198\n",
            "Epoch 18/50\n",
            "196/196 - 16s - loss: 0.6617 - sparse_categorical_accuracy: 0.7722 - val_loss: 0.7216 - val_sparse_categorical_accuracy: 0.7551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 - 16s - loss: 0.6430 - sparse_categorical_accuracy: 0.7747 - val_loss: 0.7403 - val_sparse_categorical_accuracy: 0.7444\n",
            "Epoch 20/50\n",
            "196/196 - 16s - loss: 0.6295 - sparse_categorical_accuracy: 0.7801 - val_loss: 0.7155 - val_sparse_categorical_accuracy: 0.7673\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 - 16s - loss: 0.6155 - sparse_categorical_accuracy: 0.7845 - val_loss: 0.7107 - val_sparse_categorical_accuracy: 0.7667\n",
            "Epoch 22/50\n",
            "196/196 - 16s - loss: 0.6005 - sparse_categorical_accuracy: 0.7896 - val_loss: 0.7597 - val_sparse_categorical_accuracy: 0.7645\n",
            "Epoch 23/50\n",
            "196/196 - 16s - loss: 0.6009 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.7898 - val_sparse_categorical_accuracy: 0.7538\n",
            "Epoch 24/50\n",
            "196/196 - 16s - loss: 0.5863 - sparse_categorical_accuracy: 0.7954 - val_loss: 0.8131 - val_sparse_categorical_accuracy: 0.7698\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 - 16s - loss: 0.5872 - sparse_categorical_accuracy: 0.7956 - val_loss: 0.7320 - val_sparse_categorical_accuracy: 0.7621\n",
            "Epoch 26/50\n",
            "196/196 - 16s - loss: 0.5652 - sparse_categorical_accuracy: 0.8044 - val_loss: 0.7855 - val_sparse_categorical_accuracy: 0.7673\n",
            "Epoch 27/50\n",
            "196/196 - 16s - loss: 0.5639 - sparse_categorical_accuracy: 0.8013 - val_loss: 0.8781 - val_sparse_categorical_accuracy: 0.7550\n",
            "Epoch 28/50\n",
            "196/196 - 16s - loss: 0.5461 - sparse_categorical_accuracy: 0.8091 - val_loss: 0.8645 - val_sparse_categorical_accuracy: 0.7646\n",
            "Epoch 29/50\n",
            "196/196 - 16s - loss: 0.5416 - sparse_categorical_accuracy: 0.8116 - val_loss: 0.6931 - val_sparse_categorical_accuracy: 0.7827\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_138_layer_call_fn, conv2d_138_layer_call_and_return_conditional_losses, conv2d_139_layer_call_fn, conv2d_139_layer_call_and_return_conditional_losses, conv2d_140_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_LAMBDA/210323_LAMBDA_8kdim_4udim_2hds/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTM9xv0V-NYT"
      },
      "source": [
        "##Exp 5 - 1 Heads, 1 u dim, Y Key dim\n",
        "Model : LAMBDA  \n",
        "Preprocessing : False  \n",
        "Batch size : 256  \n",
        "Key dim : 16,32,64,128  \n",
        "u dim : 1  \n",
        "Heads : 1  \n",
        "Pos. emb. size : 14  \n",
        "LambdaConv : False  \n",
        "\n",
        "---\n",
        "<pre>\n",
        "Q = k * hd       = 16,32,64,128 \n",
        "K = k * u        = 16,32,64,128\n",
        "V = 64 // hd * u = 64,64,64, 64\n",
        "</pre>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYYOiFFQ_MXi"
      },
      "source": [
        "hds = 1\n",
        "u_dim = 1\n",
        "for k_dim in [16,32,64,128]:\n",
        "  model = LambdaNetwork(num_classes=num_classes, \n",
        "                        image_height=image_height, \n",
        "                        image_width=image_width,\n",
        "                        k_dim=k_dim,\n",
        "                        u_dim=u_dim,\n",
        "                        num_heads=hds,\n",
        "                        n_r_size=14,\n",
        "                        local_contexts=False,\n",
        "                        preprocess=False).model()\n",
        "  model.summary()\n",
        "  if 'LAMBDA' not in history:\n",
        "    history['LAMBDA'] = dict()\n",
        "  if hds not in history['LAMBDA']:\n",
        "    history['LAMBDA'][hds] = dict()\n",
        "  if u_dim not in history['LAMBDA'][hds]:\n",
        "    history['LAMBDA'][hds][u_dim] = dict()\n",
        "  history['LAMBDA'][hds][u_dim][k_dim]= train_and_eval(model, SAVE_PATH + f'_LAMBDA_{k_dim}kdim_{u_dim}udim_{hds}hds', verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwbScRfWNx23"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAZclOphR1JM"
      },
      "source": [
        "Between Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJzxy5waI03G"
      },
      "source": [
        "legends = ['CNN', 'VIT', 'LAMBDA_2hds_1u_8k']\n",
        "histories = [history['CNN'], history['VIT'], history['LAMBDA'][2][1][8]]\n",
        "\n",
        "plot([(i.history['loss'], i.history['val_loss']) for i in histories], \n",
        "     [(i.history['sparse_categorical_accuracy'],\n",
        "       i.history['val_sparse_categorical_accuracy']) for i in histories],\n",
        "     legends,\n",
        "     subplot_title=['Loss', 'Accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_AqYR4cR2k3"
      },
      "source": [
        "X heads, 1 udim, Y kdim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xkdndjf4RQ2m"
      },
      "source": [
        "for hds in [1,2,4,8]:\n",
        "  legends = [f'LAMBDA_{hds}hds_1u_{i}k' for i in [1,2,4,8]]\n",
        "  histories = [history['LAMBDA'][hds][1][i] for i in [1,2,4,8]]\n",
        "\n",
        "plot([(i.history['loss'], i.history['val_loss']) for i in histories], \n",
        "     [(i.history['sparse_categorical_accuracy'],\n",
        "       i.history['val_sparse_categorical_accuracy']) for i in histories],\n",
        "     legends,\n",
        "     subplot_title=['Loss', 'Accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh4Enoo2R6ws"
      },
      "source": [
        "2 heads, X udim, Y kdim\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8pazwcvRzqA"
      },
      "source": [
        "for u in [1,2,4,8]:\n",
        "  legends = [f'LAMBDA_2hds_{u}u_{i}k' for i in [1,2,4,8]]\n",
        "  histories = [history['LAMBDA'][2][u][i] for i in [1,2,4,8]]\n",
        "\n",
        "plot([(i.history['loss'], i.history['val_loss']) for i in histories], \n",
        "     [(i.history['sparse_categorical_accuracy'],\n",
        "       i.history['val_sparse_categorical_accuracy']) for i in histories],\n",
        "     legends,\n",
        "     subplot_title=['Loss', 'Accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxuKL9WY9Wg8"
      },
      "source": [
        "1 heads, 1 udim, X kdim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqdVAZsW9WYK"
      },
      "source": [
        "u = 1\n",
        "hds = 1\n",
        "legends = [f'LAMBDA_{hds}hds_{u}u_{i}k' for i in [1,2,4,8,16,32,64,128]]\n",
        "histories = [history['LAMBDA'][hds][u][i] for i in [1,2,4,8,16,32,64,128]]\n",
        "\n",
        "plot([(i.history['loss'], i.history['val_loss']) for i in histories], \n",
        "     [(i.history['sparse_categorical_accuracy'],\n",
        "       i.history['val_sparse_categorical_accuracy']) for i in histories],\n",
        "     legends,\n",
        "     subplot_title=['Loss', 'Accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}