{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sandbox.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ScWlGBFVBJBe6rfJDdO_-5H2S3tnNkrC",
      "authorship_tag": "ABX9TyMxoPI4+2wXod9do7WnOJ5U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheneeheng/Transformer-TF/blob/colab_dev/CIfar10_Perceiver_Experimental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTYg3rNWKMt7"
      },
      "source": [
        "# **CIFAR10 Classification**\n",
        "\n",
        "Based on [CIFAR10_Keras_GPU.ipynb](https://github.com/katnoria/cifar10-native-vs-colab/blob/master/CIFAR10_Keras_GPU.ipynb) from [katnoria/cifar10-native-vs-colab](https://github.com/katnoria/cifar10-native-vs-colab) .\n",
        "\n",
        "General info about the dataset:\n",
        "- 50K Train, 10K Test\n",
        "- 10 object classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncg-GVYwK1TG"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV1eLcgmDsyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902ef760-f009-46ee-95af-9fd162ff39de"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")\n",
        "\n",
        "# SAVE_PATH = \"/content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327_\"\n",
        "SAVE_PATH = \"work/results/perceiver/210402\"\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import sys\n",
        "from time import time\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnrIX5LiLGqz"
      },
      "source": [
        "# Data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwetmhNHLGPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12827ba6-1840-4d58-8073-30637c823c39"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(x_trn_full, y_trn_full), (x_tst, y_tst) = cifar10.load_data()\n",
        "\n",
        "# remove the last dimension\n",
        "y_trn_full = y_trn_full.reshape(y_trn_full.shape[0],)\n",
        "y_tst = y_tst.reshape(y_tst.shape[0],)\n",
        "\n",
        "# perform in model.\n",
        "# # normalize data to 0..1\n",
        "# x_trn_full, x_tst = x_trn_full / 255.0, x_tst / 255.0\n",
        "\n",
        "# create validation split\n",
        "# split = 0.2\n",
        "# x_trn, x_val, y_trn, y_val = train_test_split(\n",
        "#     x_trn_full, y_trn_full, test_size=split, random_state=1969)\n",
        "x_trn, x_val, y_trn, y_val = x_trn_full, x_tst, y_trn_full, y_tst\n",
        "\n",
        "print(f'x_trn.shape: {x_trn.shape}')\n",
        "print(f'y_trn.shape: {y_trn.shape}')\n",
        "print(f'x_val.shape: {x_val.shape}')\n",
        "print(f'y_val.shape: {y_val.shape}')\n",
        "print(f'x_tst shape: {x_tst.shape}')\n",
        "print(f'y_tst.shape: {y_tst.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 83s 0us/step\n",
            "x_trn.shape: (50000, 32, 32, 3)\n",
            "y_trn.shape: (50000,)\n",
            "x_val.shape: (10000, 32, 32, 3)\n",
            "y_val.shape: (10000,)\n",
            "x_tst shape: (10000, 32, 32, 3)\n",
            "y_tst.shape: (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JqdGbPROPRm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "80d7da0a-50c0-4990-a169-78567ea20c13"
      },
      "source": [
        "# pick 25 random images and plot\n",
        "idxs = np.random.randint(x_trn.shape[0], size=25)\n",
        "images = x_trn[idxs]\n",
        "labels = y_trn[idxs]\n",
        "classnames = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "              'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "fig, axes = plt.subplots(5,5, figsize=(8,9))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "  ax.imshow(images[i])\n",
        "  ax.axis('off')\n",
        "  idx = labels[i]\n",
        "  ax.set_title(classnames[idx])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x648 with 25 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ0WDLT4s00F"
      },
      "source": [
        "class Patches(tf.keras.layers.Layer):\n",
        "  \"\"\"Creates patches from an image. Implemented as a keras layer.\"\"\"\n",
        "  def __init__(self, patch_size: int):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "  def call(self, x):\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=x,\n",
        "        sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "        strides=[1, self.patch_size, self.patch_size, 1],\n",
        "        rates=[1, 1, 1, 1],\n",
        "        padding=\"VALID\",\n",
        "    )\n",
        "    patch_dims = patches.shape[-1]\n",
        "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "    return patches\n",
        "\n",
        "  def get_config(self):\n",
        "    return {\"patch_size\": self.patch_size}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "vRUfSiYstKud",
        "outputId": "c95d4fa1-5614-42c1-fd2a-7567f76091cc"
      },
      "source": [
        "plt.figure(figsize=(4, 4))\n",
        "image = x_trn[np.random.choice(range(x_trn.shape[0]))]\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "image_size = 32\n",
        "patch_size = 4\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image size: 32 X 32\n",
            "Patch size: 4 X 4\n",
            "Patches per image: 64\n",
            "Elements per patch: 48\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 64 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOwAAADnCAYAAAAdFLrXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXCElEQVR4nO2da5CkZXXHn7693T3TM9MzO/edvbMLy4qCMcIiMSQgZZUVNJJCjJZuWAwoSIlyMcErSpHEFGKQkBKMUkQLEyWpFIHIlqySZVlKuSwbdtnZXZiZ3bn09PT0/fr2JR+SfOh+/6frGb9Yx/r/vnHofk4/79tn3tp/n+f8fa1WyxBCdOD/TX8AQog9LFhCFMGCJUQRLFhCFMGCJUQRwW7/84Uf/h2UkP1+/Lag34HxZhOv/zvXXOfrjD370P0wZyaXg2ssppIwnsymYfwLDzzSlvOrN++B+daNjcH3u8Jeam4Vxu+46z7PHr9731dgzonREbhGTwRf15rrwvgVH/pUW84XnnoU5hsZ7Ifvr9bwukvpAoz/wfv3ePb4o+/j+1ip5OEaAc8K//dZSvi67r3lq23vuP+uz8F8rSb+FaRcw+v6gvha337XNz2f8Gu33QQXT56eg2ukZk7CeHZxHsafmM16cvIJS4giWLCEKIIFS4giWLCEKKKr6BQMhmBcEp38Phx361jEQFTqNRgvVSsw3mhhFchxsHjQSTQSgfGeKI47PX3CSvYtnqOjozC+bmgQxqMRfB9WV7Gw1kmjXofxYDAA4+VyGcaXFpes8hljTCKRgPGwg78jThDHo9GoVb7JyUmcL4S/B4UiFr8WF5et8hljTLOGv6vhAL5fsWgMrxPDcQSfsIQoggVLiCJYsIQoggVLiCJYsIQooqtK7BPquWWwulg3uL+sLKiUiJLQblcX/rS0/Ph/CB2EHnp6e2HcL6wbDodh3HGwMoiou/h6ZIX2y2oFX++SoOZ2klzB7ZtSa2IuhxXUmdkZq3zGGJPN4r04Qutfs4Hve28EX+9OioUijPt6sXofE34FiAgqNqJaKsF4Qbh+hQJ+vVu3/4WBT1hCFMGCJUQRLFhCFMGCJUQRLFhCFNFVEgv4sfLZ9Ak9qIL6mSvjPmBETejJDcd6YDwk9B6HTcMqX6+gEq8bHILxurD3TCZjlc8YY06exAeZ/S38mddP4sP0Pp/d39uF+QW87ugwjOcEtTq1smKVzxhjlpexMl0p4fsVEXq/10/gvutO0sL1r1Wxkj4xtg7G43GpV9zL6Dj+bM06/o2iKAxVqGWtU/IJS4gmWLCEKIIFS4giWLCEKIIFS4gifDTDIkQPfMISoggWLCGKYMESoggWLCGK6Nqa+OpPHoGKlCscVM9VcAtiMoN7r66+8TbPQo/edzfM2WjglrZ8Hh8WjgjjMT9xx91tOR9/6G9hvkgUt0IurqzCeDaP9/jZL37Ds8e/uPGjMGcunYJr7DxnB4z7A7hN8lN33tuW86YPvwfmu/SS3fD90V48dnPfs8/D+H3/+K+ePf7RZbthzoAP35fzdp0L45s3TsD43s/d2Zbzob/+AsxXLOL7MjmBW08rLm4P/djNX/fs8a5br4c581l8mH5l/gyMp86chvF/f/UUvXUI0QwLlhBFsGAJUQQLlhBF/FreOi3hHGargk1y64KpLkSYVpjL43/IVwVj3vhg3Cqd1Oi1nMTnORcWsWfM4tKiVT5jjEmlsHBVK+OpehVBzLNvUsMi4czMLIyPCz41kkcPIpHAHjVbN50D48sJfL1TK9jPZ2/Hf7t1u/PP/08mh8Wohh8LeYj5ZfzZalV8HrYuiIQtoc4QfMISoggWLCGKYMESoggWLCGKYMESooju3joBQQ0W5Mmqi9sHXSEO1xYUaLchOa3jLfT22HmySL4z+TJWn7OCuvjGqTmrfMYYc/x1/NpIBE8OHFyHVeWhAeyN08lZW7Ay29uD/WWWFnC+zHLGKp8xxoSk+xVcm2K9nLBT3w+98BKMb9++AcYLJcGV3rFzfDfGmJAfX7/VvDAdUXBsbzp231Vj+IQlRBUsWEIUwYIlRBEsWEIUwYIlRBFdVeKWH6vBrQaOSypYJpux/kBLCdyrm8lh5W3DGD6I3GpiR2/vulj19Yex586WrdtgPBoesMpnjDGjo1MwPjP7JoyfehMfcJ68+CKrfFMbt8C45M0T7onD+FkV+37dDVN4j/l8RniH4JTeb3ddj594A8YHh7FXTr/g1ZRexQo5IuzHinJPGNfBSgoPKKgLveIIPmEJUQQLlhBFsGAJUQQLlhBFsGAJUQS9dQhRBJ+whCiCBUuIIliwhCiCBUuIIrq2Jh7/6b9ARSpTwOM4j506BeNz8/gQ8pe+9ZDnNPPNe66GOZMp3LJ40dvPg/GJ0REYv/rGL7flfPCeO2C+8Q1b4fuPHT0G46+8fATG//mpn3n2eP2Hr4E5T5yYhmusnxqF8csv/0MY//hNt7fl/IdvfA3fxzRu92w0cAvi+ITgc/OZz3v2+MA9X4Y5nzt4CK6xuIAHCZTLuG3v0NGjbTl37zob5jtrB26RnBgdhvF8Hg8uePCH/+bZ4y17/wzmrNWxkDufwN466SSuj1+8+Bq9dQjRDAuWEEWwYAlRBAuWEEV0FZ3qLj5TWixin5umIFbkC9h0GXFGMLf1CZYnNeEzViplq3yRKJ58lxPOyR56/gUYPz2HBQXE7Cw+uzkygs/27tr1FhhfSKxY5Xty389gXLqPfsHfaGxsDMb3fsYbk4yvJzfgKYbVGp6yuLBgNzWxKZynnRaE0FKpAONjI/jzIWqCn0+hhIWrcASfwQ0L5uEIPmEJUQQLlhBFsGAJUQQLlhBFsGAJUURXlbhSxWpXs4kVvWoVT4uT2ssQIcGNOr4uDuM+H/ZqkT67B+E88PT0CRhPZ3IwPjqG2wcRPj++fhOTWIWVHNR/+auXrfIVy/i+BILY00W6v9Mn8FRHxJP/+TSMT07gPYYFT5uh4XVW+WJx7DM0v4A/s/SLxvDwRqt8xhjTFJ536Sz+joSj2DupFehahm3wCUuIIliwhCiCBUuIIliwhCiCBUuIIrrKU67Qp1uvY/fqUhkfbJdUR0QkitXCvj7skVIT1OBW1M7VWtpjKoV7YesuVhfrQTsvH2OMGR3DymcohBumDxw4CONLCTsfGF8D/132+7Ei32zgvUQd+57XXBr3jy8uzsP42Bg+HL9+PT6A3klY+N709mGPpLCDe8iLZbsedGOMyedxP3I6g/vQo3X8GSsuricEn7CEKIIFS4giWLCEKIIFS4giWLCEKILeOoQogk9YQhTBgiVEESxYQhTBgiVEEV1bEw/+4EGoSCVWsSfL4aOvw/iZBPbFefjxn3pOZn/0ysthzkHhAPu6PtxiNiQcaL75rr9vy3nvlz4N8x0+isdjHjl8HMYjEdxWePDItGePH7/qCnxdE9hf5uhrwljU4UkYf/HEsbac77vkMqws4nPxZmAgDuPRCL7W3/3xDzwrXbD9HJhzKYVbE3tjuPV0w0Z8oHz/gUNtOS991ztgvko1g/MJI0cjIfy9+Y/9Bzx7vPTCi/F9XMH1EY0JAwMMbgV95TC9dQhRDQuWEEWwYAlRBAuWEEV099ap4Gl72XQGxiU/mkbD/jysZCZcEj7LQD8WK4quXc435uZgPJFcgnFfECs1sYFBq3zGGLOyis9RvjmLfWSEI7gmFMRT+DrZvm0bjAcCWCiTJlGu5VxzOIQ/mxPEZ0KDAWGiYMOuEy8axutWq9g/yK0Jvji5lFU+Y4xJppZhPFfAOQsVfL2DjqD+AfiEJUQRLFhCFMGCJUQRLFhCFMGCJUQRXVXiQg57hGSE1sSmoAbba2DGlCvCFERBES27WEX0Ww6iW1jG7YDLK7id0q3jv3HNNeyyXMEKZcvgPUZ6BPt5v11OfwB/5uGRYRhPpbBS2mzZq8TxOFbN88JUQieMJzhGhRbCTmK9+NeCdBrvpVrDX5BCASv4iGx2BcYrwtotH74PoTreO4JPWEIUwYIlRBEsWEIUwYIlRBEsWEIU0VUlzglO0mVB6ZN6UKX+YLw2dmtvBrCSls1iD5dG0y5nQFjXEXphfcLfuErF3mVeGlTZ24t9YFyhLzoYslMX80V8jXyCypxMYuW8X+jbRkxO4sP1oSg+BF+t4V8HHMs9Fou4fzeTxv3t/TGsPgcslXdjjKm7uL9d+rVEbou2rw8+YQlRBAuWEEWwYAlRBAuWEEWwYAlRBL11CFEEn7CEKIIFS4giWLCEKKJrp9P3P49tLE4LnTDLwvnZZCYD44/tO+hpK3nPRRfAnE4P7rIJhfBZ0b4+3DX06I+faMt51ft+H+ZbXMQTDGvC+duwgztnnnvxsGePV7z7ErhIrYo7Xup1HO8Rzoo+/dzP23J+5P1XwXx+oTPN75euKb4H337kYc9CUs6i0CUndaY1Grib6Mn9+9tyXvIO/L1ZSizA94+PDMF43cW2GYeOnPTscfNYHNuDCOdhi2XczeUP4PuQLdZo1UGIZliwhCiCBUuIIliwhCiCBUuIIrqqxKVSCcbrdayCSQpbrYbja1nbJyhvrrC25BvTSS6Hp+Tl8zheb2BFz3Gwtwuirw+bBldDeC+uYK7TF4tZ5Qs5+Gzv+Pg4XldQg6V7g5DmK66m8cRNv3AONSB4GXUinUcOBvFXXPquDg/GrfIZY8z4KJ46mRG+U5L6Xi7js7xwDetXEkJ+47BgCVEEC5YQRbBgCVEEC5YQRfxaKrGovAkq4lqmJvp8WElrtSRXcLx2RfDo6aRYwHuUbGQkF/JMBk/nQySXsXP30NAIjPv9+Hq7DTvVNuDgyYNBwc+mYQS/olDXr0sbkm9PQZhumE6vwnhecDPvJCf0sUsqc1BQpddPYuUcsfPsHTA+M3cGxgcG8Hdybm7OOiefsIQoggVLiCJYsIQoggVLiCJYsIQooqvs1xBUyJbgNi71c1ardoqtMbIKGxTcq6tVPJGgWrPzupHU5GAQK6iuMAEhI/TIIpIr2LnbJ6jBkkrcI3jxeF4Xw68LCGr/0PA6GA+Hw1b5jDFmauNGGF8WHO9XV7FTeiadscon7SUSxvG+PtyHHV+Df9CGKewf1BQmkSaSeI/plJ3LvDF8whKiChYsIYpgwRKiCBYsIYpgwRKiCHrrEKIIPmEJUQQLlhBFsGAJUUTXTqev/+mV8B+4hRruDjo1j/1oUkXcdfTMq9Oelql37twBc0b743CNSgWfZ62U8eS6w9MzbTl3n78d5guH8aTBbA7nS6fxecyZpZRnj1smx/Eee3D3TTSKO5X6BwZgfP+BZ9tyXn/tXphv85bN8P2bNm2Ccel89HU33ODZ41/efjvM+eKvXoJrLC1iD5y6i79rr5081ZbzA++9XLiPuGNtsBff3y0bp2D8jr96wLPHb37lNsHPB3dzTZ84BePHjx+H8aOnl+mtQ4hmWLCEKIIFS4giWLCEKKKr6CQdl8tlM/j1gihRrdk3Z9QE8+JmBQ/jcoL4b45wOs7D9u2bYbxfOH514sRpnM82oTFGcGww6Sw+olepY1uJYMQu58zpGRhvCX+us3ksoKVS+HjYdTfc4Im9fPgV+Nq5eTxwLCMMYTtr62YY7+Rtb3srjDshvEnHj7+TccEIHL5WEELdGv4Ox2NYjHL8WABD8AlLiCJYsIQoggVLiCJYsIQoggVLiCK6qsQFYXhaQVCPi8JAtELVXiUuCwbN9UoZxgfjWNWThmx1MhSPw/jkxBiMF4v484Uj9oO0zt25E8aXV7BSOr+4BOMrK1h17OTMaaxsJ5cTwjvwkL1aDd9fxPT06zDuuniw39jYKIzvOhdfq062bdsC4z7Jc6WOv9sBO/9oY4wxjmCU3S8YYkejERj3CdYoCD5hCVEEC5YQRbBgCVEEC5YQRbBgCVFEV5X45CxWFytVrBInc/jQeKFlbwRcEhRo6SBz0IdVwHgsapUvFMCfLdaD1ecN69fj+BQ+9I04/63nwfiSYOXgCBYZs7OzVvkKOWw2vSLcR8kuJSyooghXuI/DI9i0+uLdF8H4W3bZqcQDksWGZDfTwHupFPF3GCFdJ0lNT6XwrwDFfN46J5+whCiCBUuIIliwhCiCBUuIIliwhCiCVh2EKIJPWEIUwYIlRBEsWEIUwYIlRBFdewbPjQizIP34lG9Z0K/KPjzXc6lU8yw0PtAHV2kILWY9Edxitm0L9kh55pevteW8/c+vgfl27NgG3x8K44PxNRe3qX3is3d69njPF7EnS0U43F1z8djMI0f+G8afePqZtpy/d+HbYT6phS4YxPertxe3az7984OePV753stgzomJCbjGznPOhvEJ4WD7h/Zc35bzice+h799TXztfAbH8zncJnjNDbd69vi9e++GOWdmcMvovn3PwPjsqTdgfL7WpLcOIZphwRKiCBYsIYpgwRKiiK6i09mT2DA4EMJvywqeIm8m7M/7tYRzlC0//tvSauCcks9PJ8GAILBE8RTESA+OB0L4zCpichxPZGwJpjstH9778NCgVb53v+tiGJfONdddPBlSEp0Quy/8XRgPCOeP48J51lgPnjTYSVMQJf0Gi4FNQYxaC5LB9apw7jWTweeSWw1OTSTktxIWLCGKYMESoggWLCGKYMESooiuKvF4D277C4QEZTWEHcFrfVh1REQEL5SSt0vrf2kIKqDQ5tdJQFCfpXhMUIkdQVVGDMX7YbwptHBWBL+hwPCQVb6pKdwOWBCm9dXr+NoNDMSt8hljzOjwOhgvlbFHktDtalqWam7TxW2WLUElrgmvr1bwdE5EpSxMDxU8jyqCP1RgDY9NPmEJUQQLlhBFsGAJUQQLlhBFsGAJUURXldgR+jODfqy8+YU+0U1D9grqSAwr02dKUq8o7sO0dbVuCH2zvrUOk1zD9Mm+PqwSu9IafqxcSmpuJzGhB9gnOK27wjWJROz6eo2RfWekLUqvl5TYTty6cFBd+NWhUsXXbmU1bZXPGGNWV1ZgPJ/CHkmtKr6PfWH86wCCT1hCFMGCJUQRLFhCFMGCJUQRLFhCFEFvHUIUwScsIYpgwRKiCBYsIYpgwRKiiK6tiZ8+ewgqUtEQFqqCYdxW2HJwS9s9z896euM+uGMELn5EGJUqtedNTeFRov91fL4t560f+wDMd/4F58P3x4dHYNyJ4va/K/5kj2ePB556HOZ0hfa8qnAYv1TEYzY/+JFr23L+5NGHYb6yMFK2IsQlz509n7zFs8fvfOtvYM6mIHJK7ZOOMIb26ms/2Zbzse/cj/2KSvjQeKZQgPGTrx+H8W//0488e/zjd54Pc+aSS3CN9CpuZexz8B5/kazSW4cQzbBgCVEEC5YQRbBgCVFEV9HJL0wO9As+z04Qvz4kmC4jJobiMJ6u4Y+6vIrPHlYEQaaT1TQ+/5grFmE8GBGmJgqGznDtHPZYaQoeOiEH+/Y4jt11lV+Hz8O2mvj+NtbgRyOdnZXWqAvCWr2Kpxt2Uq3g1yUWEzA+M4dNl2enT1jlM8aY+ZPYiHlAON862huF8RCnJhLy2wkLlhBFsGAJUQQLlhBFsGAJUURXldjnwypiUDAD8QsGKf41mIf0h7EiunUYx1tV3Hq2lMZKbCdvzMzA+OD4KIxns7ilLdpj704+f/oMjPf242mK/XHsoSPY0XhoCf5Dknt9QLjvdWEyIUTI6Qqqr9QO6VpOTVxexO2Ap2exGnzs5RdhvJrOWOUzxhinLjjVR3BZBYWpoo2m3fRLY/iEJUQVLFhCFMGCJUQRLFhCFMGCJUQRXVXihuSP4hPeJvTCSoeWEX6hjzXuYHf38QGsrObKdr3EycQyjL/80iswPjQwCONOCKvYiFdfOQzjU5s2wfhIGSurQcHxvpOM0C8tHf6XJmmWLRVbY4zJZDIwXirh+yIdNLftCX/92DEYPzM7B+O5JL7vI1H7vveRGO4NDgj90j7huraEOkPwCUuIIliwhCiCBUuIIliwhCiCBUuIIuitQ4gi+IQlRBEsWEIUwYIlRBEsWEIUwYIlRBEsWEIU8T8fH0jhw9z8uwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHDdb2NJOVYU"
      },
      "source": [
        "def data_generator(split: str, batch_size: int, shuffle_buffer: int = 10000):\n",
        "  \"\"\"Creates a tf.data.Dataset instance.\n",
        "\n",
        "  Args:\n",
        "      split: The type of data to generate, ['train', 'val', 'test'].\n",
        "      batch_size: Batch size.\n",
        "      shuffle_buffer: Number of elements used for shuffling.\n",
        "\n",
        "  Returns:\n",
        "      A tf.data.Dataset instance.\n",
        "\n",
        "  Raises:\n",
        "      ValueError: If `split` is not ['train', 'val', 'test'].\n",
        "\n",
        "  \"\"\"\n",
        "  if split == 'train':\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x_trn, y_trn))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "  elif split == 'val':\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "  elif split == 'test':\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x_tst, y_tst))\n",
        "  else:\n",
        "    raise ValueError(f\"Unknown data split : {split}\")\n",
        "  return ds.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqkixwrjM7wE"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn9ZY763Jg7x"
      },
      "source": [
        "##CNN\n",
        "\n",
        "A simple CNN example from the keras [CNN benchmark](https://github.com/keras-team/keras/blob/master/keras/benchmarks/keras_examples_benchmarks/cifar10_cnn_benchmark_test.py) .  \n",
        "It is based on this [CNN example](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py) but the example is no longer available.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McvCeeVJJf8j"
      },
      "source": [
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (\n",
        "  Conv2D, \n",
        "  MaxPool2D,\n",
        "  Dropout,\n",
        "  Flatten,\n",
        "  Dense\n",
        ")\n",
        "from tensorflow.keras.layers.experimental.preprocessing import (\n",
        "  RandomFlip,\n",
        "  RandomTranslation,\n",
        "  RandomRotation,\n",
        "  RandomZoom,\n",
        "  Normalization,\n",
        "  Rescaling,\n",
        "  Resizing\n",
        ")\n",
        "\n",
        "\n",
        "class SmallCNN(Model):\n",
        "  \"\"\"SmallCNN implementation.\n",
        "  \n",
        "  Changes compared to the original model:\n",
        "  - maxpool2d uses a kernel_size=3, stride=2.\n",
        "  \n",
        "  \"\"\"\n",
        "  def __init__(self, \n",
        "               num_classes: int, \n",
        "               image_height: int, \n",
        "               image_width: int,\n",
        "               preprocess: bool = False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_classes = num_classes\n",
        "    self.image_height = image_height\n",
        "    self.image_width = image_width\n",
        "    self.preprocess = preprocess\n",
        "\n",
        "    self.rescale = Rescaling(1./255., name='rescale')\n",
        "    self.resize = Resizing(self.image_height, self.image_width, name='resize')\n",
        "\n",
        "    # Preprocessing\n",
        "    self.augs = [\n",
        "      RandomFlip(\"horizontal\", name='data_aug_flip'),\n",
        "      # RandomTranslation((-0.1, 0.1), (-0.1, 0.1), name='data_aug2'),\n",
        "      RandomRotation(0.02, name='data_aug_rot'),\n",
        "      RandomZoom(0.2, 0.2, name='data_aug_zoom'),\n",
        "    ]\n",
        "\n",
        "    # Model\n",
        "    self.conv1 = self.conv2d(32, 3, name='conv1', padding='same')\n",
        "    self.conv2 = self.conv2d(32, 3, name='conv2')\n",
        "    self.pool2 = self.maxpool2d(name='pool1')\n",
        "    self.drop2 = Dropout(0.5, name='drop2')\n",
        "    self.conv3 = self.conv2d(64, 3, name='conv3', padding='same')\n",
        "    self.conv4 = self.conv2d(64, 3, name='conv4')\n",
        "    self.pool4 = self.maxpool2d(name='pool4')\n",
        "    self.drop4 = Dropout(0.25, name='drop4')\n",
        "    self.flat5 = Flatten(name='flat5')\n",
        "    self.dens5 = Dense(512, activation=tf.keras.activations.relu, name='dens5')\n",
        "    self.drop5 = Dropout(0.5, name='drop5')\n",
        "    self.dens6 = Dense(self.num_classes, name='logits')\n",
        "\n",
        "  def call(self, x, training: bool = None):\n",
        "    x = self.rescale(x)\n",
        "    x = self.resize(x)\n",
        "    if self.preprocess:\n",
        "      for aug_i in self.augs:\n",
        "        x = aug_i(x, training=training)\n",
        "    x = self.pool2(self.conv2(self.conv1(x)))\n",
        "    x = self.drop2(x, training=training)\n",
        "    x = self.pool4(self.conv4(self.conv3(x)))\n",
        "    x = self.drop4(x, training=training)\n",
        "    x = self.dens5(self.flat5(x))\n",
        "    x = self.drop5(x, training=training)\n",
        "    x = self.dens6(x)\n",
        "    return x\n",
        "  \n",
        "  # https://stackoverflow.com/questions/55235212\n",
        "  def model(self, input_shape=None):\n",
        "    if input_shape is None:\n",
        "      x = Input(shape=(self.image_height, self.image_width, 3))\n",
        "    else:\n",
        "      x = Input(shape=input_shape)\n",
        "    return Model(inputs=[x], outputs=self.call(x))\n",
        "  \n",
        "  def get_config(self):\n",
        "    return {\"num_classes\": self.num_classes,\n",
        "            \"image_height\": self.image_height,\n",
        "            \"image_width\": self.image_width,\n",
        "            \"preprocess\": self.preprocess}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n",
        "\n",
        "  @staticmethod\n",
        "  def conv2d(filters, kernel_size, strides=(1, 1), padding='valid', **kwargs):\n",
        "    return Conv2D(filters, \n",
        "                  kernel_size, \n",
        "                  strides, \n",
        "                  padding=padding, \n",
        "                  activation=tf.keras.activations.relu,\n",
        "                  **kwargs)\n",
        "  \n",
        "  @staticmethod\n",
        "  def maxpool2d(**kwargs):\n",
        "    return MaxPool2D([3, 3], 2, padding='valid', **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGHzfefDfO5z"
      },
      "source": [
        "##VIT\n",
        "\n",
        "[Vision Transformer](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy et al.  \n",
        "Code taken from this [example](https://keras.io/examples/vision/image_classification_with_vision_transformer/) from the official keras website.\n",
        "\n",
        "**Good to know points**\n",
        "*  Multiheadattention is used here. \n",
        "*  Self attention is implemented, meaning that the projection_dim is used for the QKV projections and the final projection to get the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfUPza88W8GY"
      },
      "source": [
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (\n",
        "  Layer,\n",
        "  Add,\n",
        "  Conv2D, \n",
        "  Dense,\n",
        "  Dropout,\n",
        "  Embedding,\n",
        "  Flatten,\n",
        "  LayerNormalization,\n",
        "  MaxPool2D,\n",
        "  MultiHeadAttention\n",
        ")\n",
        "from tensorflow.keras.layers.experimental.preprocessing import (\n",
        "  Normalization,\n",
        "  RandomFlip,\n",
        "  RandomRotation,\n",
        "  RandomZoom,\n",
        "  Rescaling,\n",
        "  Resizing,\n",
        ")\n",
        "\n",
        "\n",
        "class MLP(Layer):\n",
        "  \"\"\"Multilayer perceptron.\"\"\"\n",
        "  def __init__(self, hidden_units: list, dropout_rate: float):\n",
        "    super().__init__()\n",
        "    self.hidden_units = hidden_units\n",
        "    self.dropout_rate = dropout_rate\n",
        "    \n",
        "    self.layers = []\n",
        "    self.dropouts = []\n",
        "    for idx, units in enumerate(self.hidden_units):\n",
        "      self.layers.append(Dense(units, \n",
        "                               activation=tf.keras.activations.gelu, \n",
        "                               name=f'dens{idx}'))\n",
        "      self.dropouts.append(Dropout(dropout_rate))\n",
        "\n",
        "  def call(self, x, training: bool = None):\n",
        "    for layer_i, dropout_i in zip(self.layers, self.dropouts):\n",
        "        x = layer_i(x)\n",
        "        x = dropout_i(x, training=training)\n",
        "    return x\n",
        "  \n",
        "  def get_config(self):\n",
        "    return {\"hidden_units\": self.hidden_units,\n",
        "            \"dropout_rate\": self.dropout_rate}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n",
        "\n",
        "\n",
        "def mlp(hidden_units: list, dropout_rate: float, name: str = ''):\n",
        "  \"\"\"Multilayer perceptron. \n",
        "  \n",
        "  Same as MLP(...). But this is not a layer, meaning that the layers listed \n",
        "  here will be shown separately in model.summary() .\n",
        "\n",
        "  \"\"\"\n",
        "  layers = []\n",
        "  for idx, units in enumerate(hidden_units):\n",
        "    layers.append(Dense(units, \n",
        "                        activation=tf.keras.activations.gelu, \n",
        "                        name=f'{name}/dens{idx}'))\n",
        "    layers.append(Dropout(dropout_rate, name=f'{name}/drop{idx}'))\n",
        "  return layers\n",
        "\n",
        "\n",
        "class PatchEncoder(Layer):\n",
        "  \"\"\"Encoder for the image patches.\"\"\"\n",
        "  def __init__(self, num_patches: int, projection_dim: int):\n",
        "    super().__init__()\n",
        "    self.num_patches = num_patches\n",
        "    self.projection_dim = projection_dim\n",
        "    self.projection = Dense(units=self.projection_dim)\n",
        "    self.position_embedding = Embedding(input_dim=num_patches,\n",
        "                                        output_dim=self.projection_dim)\n",
        "\n",
        "  def call(self, x):\n",
        "    positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "    encoded = self.projection(x) + self.position_embedding(positions)\n",
        "    return encoded\n",
        "  \n",
        "  def get_config(self):\n",
        "    return {\"num_patches\": self.num_patches,\n",
        "            \"projection_dim\": self.projection_dim}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n",
        "\n",
        "\n",
        "class VIT(Model):\n",
        "  \"\"\"Vision Transformer implementation.\n",
        "  \n",
        "  Changes compared to the original model:\n",
        "  - maxpool2d uses a kernel_size=3, stride=2.\n",
        "  \n",
        "  \"\"\"\n",
        "  def __init__(self, \n",
        "               num_classes: int, \n",
        "               image_height: int, \n",
        "               image_width: int,\n",
        "               patch_size: int, \n",
        "               projection_dim: int,\n",
        "               num_layers: int,\n",
        "               num_heads: int,\n",
        "               mlp_dims: list,\n",
        "               classifier_mlp_dims: list,\n",
        "               preprocess: bool = False):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      num_classes: Number of classes.\n",
        "      image_height: Height of image.\n",
        "      image_width: Width of image.\n",
        "      patch_size: SIze of the input patch.\n",
        "      projection_dim: Dimension of input projection (to transformer) layer.\n",
        "      num_layers: Number of transformer blocks.\n",
        "      num_heads: Number of heads in the multiheadattention layers.\n",
        "      mlp_dims: List of dimensions of the mlp.\n",
        "      classifier_mlp_dims: List of dimension of the classifier layers.\n",
        "      preprocess: Whether to do preprocessing or not.\n",
        "      \n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    \n",
        "    self.num_classes = num_classes\n",
        "    self.image_height = image_height\n",
        "    self.image_width = image_width\n",
        "    self.patch_size = patch_size\n",
        "    self.projection_dim = projection_dim\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.mlp_dims = mlp_dims\n",
        "    self.classifier_mlp_dims = classifier_mlp_dims\n",
        "    self.preprocess = preprocess\n",
        "\n",
        "    self.rescale = Rescaling(1./255., name='rescale')\n",
        "    self.resize = Resizing(self.image_height, self.image_width, name='resize')\n",
        "\n",
        "    # Preprocessing\n",
        "    self.augs = [\n",
        "      RandomFlip(\"horizontal\", name='data_aug_flip'),\n",
        "      # RandomTranslation((-0.1, 0.1), (-0.1, 0.1), name='data_aug2'),\n",
        "      RandomRotation(0.02, name='data_aug_rot'),\n",
        "      RandomZoom(0.2, 0.2, name='data_aug_zoom'),\n",
        "    ]\n",
        "\n",
        "    # Patches\n",
        "    self.patch = Patches(self.patch_size)\n",
        "    num_patches = (self.image_height // self.patch_size) * \\\n",
        "                  (self.image_width // self.patch_size)\n",
        "    self.patch_enc = PatchEncoder(num_patches, self.projection_dim)\n",
        "\n",
        "    # Model\n",
        "    self.transformer_blocks = []\n",
        "    for idx in range(self.num_layers):\n",
        "      block = []\n",
        "      block += [LayerNormalization(epsilon=1e-6, name=f'ln{idx + 1}_1')]\n",
        "      block += [MultiHeadAttention(self.num_heads, \n",
        "                                   self.projection_dim, \n",
        "                                   dropout=0.1, \n",
        "                                   name=f'mha{idx + 1}')]\n",
        "      block += [Add(name=f'skip{idx + 1}_1')]\n",
        "      block += [LayerNormalization(epsilon=1e-6, name=f'ln{idx + 1}_2')]\n",
        "      block += [mlp(mlp_dims, dropout_rate=0.1, name=f'mlp{idx + 1}')]\n",
        "      block += [Add(name=f'skip{idx + 1}_2')]\n",
        "      self.transformer_blocks.append(block)\n",
        "\n",
        "    self.lnorm = LayerNormalization(epsilon=1e-6, name='classifier_ln')\n",
        "    self.flat = Flatten(name='classifier_flat')\n",
        "    self.drop = Dropout(0.5, name='classifier_drop')\n",
        "    self.mlp = mlp(self.classifier_mlp_dims, dropout_rate=0.5, \n",
        "                   name='classifier_mlp')\n",
        "    self.logits = Dense(self.num_classes, name='logits')\n",
        "\n",
        "  def call(self, x, training: bool = None):\n",
        "    x = self.rescale(x)\n",
        "    x = self.resize(x)\n",
        "\n",
        "    if self.preprocess:\n",
        "      for aug_i in self.augs:\n",
        "        x = aug_i(x, training=training)\n",
        "\n",
        "    x = self.patch(x)\n",
        "    x = self.patch_enc(x)\n",
        "    \n",
        "    for block in self.transformer_blocks:\n",
        "      x_layer_norm_1 = block[0](x)\n",
        "      x_attention    = block[1](x_layer_norm_1, x_layer_norm_1)\n",
        "      x_skip         = block[2]([x_attention, x])\n",
        "      x_layer_norm_2 = block[3](x_skip)\n",
        "      x_mlp          = self._iterate_mlp(block[4], x_layer_norm_2)\n",
        "      x              = block[5]([x_mlp, x_skip])\n",
        "    \n",
        "    x = self.lnorm(x)\n",
        "    x = self.flat(x)\n",
        "    x = self.drop(x, training=training)\n",
        "    x = self._iterate_mlp(self.mlp, x)\n",
        "    x = self.logits(x)\n",
        "    return x\n",
        "  \n",
        "  @staticmethod\n",
        "  def _iterate_mlp(mlp_layers: list, x: tf.Tensor):\n",
        "    _x = x\n",
        "    for mlp_i in mlp_layers:\n",
        "      _x = mlp_i(_x)\n",
        "    return _x\n",
        "\n",
        "  # https://stackoverflow.com/questions/55235212\n",
        "  def model(self, input_shape=None):\n",
        "    if input_shape is None:\n",
        "      x = Input(shape=(self.image_height, self.image_width, 3))\n",
        "    else:\n",
        "      x = Input(shape=input_shape)\n",
        "    return Model(inputs=[x], outputs=self.call(x))\n",
        "  \n",
        "  def get_config(self):\n",
        "      return {\"num_classes\": self.num_classes,\n",
        "              \"image_height\": self.image_height,\n",
        "              \"image_width\": self.image_width,\n",
        "              \"patch_size\": self.patch_size,\n",
        "              \"projection_dim\": self.projection_dim,\n",
        "              \"num_layers\": self.num_layers,\n",
        "              \"num_heads\": self.num_heads,\n",
        "              \"mlp_dims\": self.mlp_dims,\n",
        "              \"classifier_mlp_dims\": self.classifier_mlp_dims,\n",
        "              \"preprocess\": self.preprocess}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "      return cls(**config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWcTLWi9KTFh"
      },
      "source": [
        "##LambdaNetworks\n",
        "\n",
        "[LambdaNetworks](https://arxiv.org/abs/2102.08602) from Irwan Bello.  \n",
        "\n",
        "The LambdaNets is based on [tfkeras.py](https://github.com/lucidrains/lambda-networks/blob/main/lambda_networks/tfkeras.py) from [lucidrains/lambda-networks](https://github.com/lucidrains/lambda-networks) and [lambda2d.py](https://github.com/g0lemXIV/LambdaNetworks/blob/main/lambda_layers/lambda2d.py) from [g0lemXIV/LambdaNetworks](https://github.com/g0lemXIV/LambdaNetworks) .  \n",
        "\n",
        "**Good to know points**\n",
        "*   The paper main introduces lambdalayer and uses it to replace the convs in ResNet to create LambdaResNet.\n",
        "*   LambdaResNet achieves high accuracy but is very slow (~7x slower).\n",
        "*   The main conclusion is that a hybrid model of conv + lambda has the best speed-accuracy tradeoff.\n",
        "*   The best hybrid model replaces only some blocks in C4 and all the blocks in C5.\n",
        "*   Unlike the multihead attention in VIT, the final output projection is not included. This means that the V dimension must be divided by the number of heads.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-_CmWcIKSxv"
      },
      "source": [
        "#! pip install einops\n",
        "from einops.layers.tensorflow import Rearrange\n",
        "\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (\n",
        "  BatchNormalization, \n",
        "  Conv2D,\n",
        "  Conv3D\n",
        ")\n",
        "from tensorflow.keras.layers.experimental.preprocessing import (\n",
        "  Normalization,\n",
        "  RandomFlip,\n",
        "  RandomRotation,\n",
        "  RandomZoom,\n",
        "  Rescaling,\n",
        "  Resizing,\n",
        ")\n",
        "from tensorflow import einsum, meshgrid\n",
        "\n",
        "\n",
        "def calc_rel_pos(n: int):\n",
        "  \"\"\"Generates a relative position meshgrid.\n",
        "  \n",
        "  Args:\n",
        "    n: Size of the original meshgrid. Size = Height = Width.\n",
        "\n",
        "  Returns: \n",
        "    rel_pos: An array of [n*n, n*n, 2] with value range from [-n+1, n-1] to \n",
        "             [0, 2n-2].\n",
        "  \"\"\"\n",
        "  # [2, n, n]\n",
        "  pos = tf.stack(meshgrid(tf.range(n), tf.range(n), indexing = 'ij'))\n",
        "  # [n*n, 2], pos[n] = (i, j)\n",
        "  pos = Rearrange('n i j -> (i j) n')(pos)             \n",
        "  # [n*n, n*n, 2], rel_pos[n, m] = (rel_i, rel_j)\n",
        "  rel_pos = pos[None, :] - pos[:, None]                \n",
        "  # shift value range from [-n+1, n-1] to [0, 2n-2]\n",
        "  rel_pos += n - 1\n",
        "  # [n*n, n*n, 2]                      \n",
        "  return rel_pos\n",
        "\n",
        "\n",
        "class Lambda(Layer):\n",
        "  \"\"\"Lambda Networks implementation.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               output_dim: int,\n",
        "               k_dim: int = 16,\n",
        "               u_dim: int = 1,\n",
        "               num_heads: int = 4,\n",
        "               n_r_size: int = None,\n",
        "               local_contexts: bool = False,\n",
        "               batch_norm: bool = True,\n",
        "               **kwargs):\n",
        "    \"\"\"Constructor\n",
        "\n",
        "    Args:\n",
        "      output_dim: Output dimension of the layer or v_dim * `num_heads`.\n",
        "      k_dim: Dimension of key.\n",
        "      u_dim: Intra depth for multiquery heads.\n",
        "      num_heads: Number of heads for multiquery.\n",
        "      n_r_size: If `local_contexts=True` n = height*width of query,\n",
        "                else r = receptive field.\n",
        "      local_contexts: If True lambdaconv is used, \n",
        "                      else relative position embeddings are used.\n",
        "      batch_norm: Whether to apply batch norm to query and value after\n",
        "                  the linear projection.\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.output_dim = output_dim\n",
        "    self.u_dim = u_dim  # intra-depth dimension\n",
        "    self.num_heads = num_heads\n",
        "    self.n_r_size = n_r_size\n",
        "    self.local_contexts = local_contexts\n",
        "    self.batch_norm = batch_norm\n",
        "\n",
        "    assert (self.output_dim % self.num_heads) == 0, \\\n",
        "      '`output_dim` must be divisible by `num_heads` for multi-head query.'\n",
        "    self.v_dim = self.output_dim // self.num_heads\n",
        "    self.k_dim = k_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.to_q = Conv2D(self.k_dim * self.num_heads, 1, use_bias=self.batch_norm)\n",
        "    self.to_k = Conv2D(self.k_dim * self.u_dim, 1, use_bias=self.batch_norm)\n",
        "    self.to_v = Conv2D(self.v_dim * self.u_dim, 1, use_bias=self.batch_norm)\n",
        "\n",
        "    self.norm_q = BatchNormalization() if self.batch_norm else None\n",
        "    self.norm_v = BatchNormalization() if self.batch_norm else None\n",
        "\n",
        "    if self.local_contexts:\n",
        "      assert (self.n_r_size % 2) == 1, 'Receptive kernel size should be odd'\n",
        "      self.pos_conv = Conv3D(self.k_dim, \n",
        "                             (1, self.n_r_size, self.n_r_size), \n",
        "                             padding='same')\n",
        "    else:\n",
        "      assert n is not None, 'You must specify the window length (n = h = w)'\n",
        "      rel_length = 2 * self.n_r_size - 1\n",
        "      self.rel_pos_emb = self.add_weight(\n",
        "        name='pos_emb',\n",
        "        shape=(rel_length, rel_length, self.k_dim, self.u_dim),\n",
        "        initializer=tf.keras.initializers.RandomNormal,\n",
        "        trainable=True\n",
        "      )\n",
        "      self.rel_pos = calc_rel_pos(self.n_r_size)\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Info on the notations for reference:\n",
        "    x = 2D Image data (Assumption)\n",
        "    q = query\n",
        "    k = key\n",
        "    v = value\n",
        "    h = number of heads for multiquery\n",
        "    u = intradepth\n",
        "    b = batch size\n",
        "    hh = height of input data\n",
        "    ww = width of input data\n",
        "    m = height * width of key / value.\n",
        "    n = height * width of query.\n",
        "    \"\"\"\n",
        "    b, hh, ww, c, u, h = *x.get_shape().as_list(), self.u_dim, self.num_heads\n",
        "\n",
        "    q = self.to_q(x)\n",
        "    k = self.to_k(x)\n",
        "    v = self.to_v(x)\n",
        "\n",
        "    if self.batch_norm:\n",
        "      q = self.norm_q(q)\n",
        "      v = self.norm_v(v)\n",
        "\n",
        "    q = Rearrange('b hh ww (h k) -> b h k (hh ww)', h=h)(q)\n",
        "    k = Rearrange('b hh ww (u k) -> b u k (hh ww)', u=u)(k)\n",
        "    v = Rearrange('b hh ww (u v) -> b u v (hh ww)', u=u)(v)\n",
        "\n",
        "    k = tf.nn.softmax(k)\n",
        "\n",
        "    Lc = einsum('b u k m, b u v m -> b k v', k, v)\n",
        "    Yc = einsum('b h k n, b k v -> b n h v', q, Lc)\n",
        "\n",
        "    if self.local_contexts:\n",
        "      # lambda convs, embedding is represented by the conv kernels.\n",
        "      v = Rearrange('b u v (hh ww) -> b v hh ww u', hh=hh, ww=ww)(v)\n",
        "      Lp = self.pos_conv(v)\n",
        "      Lp = Rearrange('b v h w k -> b v k (h w)')(Lp)\n",
        "      Yp = einsum('b h k n, b v k n -> b n h v', q, Lp)\n",
        "    else:\n",
        "      # relative position embedding.\n",
        "      rel_pos_emb = tf.gather_nd(self.rel_pos_emb, self.rel_pos)\n",
        "      Lp = einsum('n m k u, b u v m -> b n k v', rel_pos_emb, v)\n",
        "      Yp = einsum('b h k n, b n k v -> b n h v', q, Lp)\n",
        "\n",
        "    Y = Yc + Yp\n",
        "    out = Rearrange('b (hh ww) h v -> b hh ww (h v)', hh = hh, ww = ww)(Y)\n",
        "    return out\n",
        "\n",
        "  def get_config(self):\n",
        "    return {\"output_dim\": self.output_dim,\n",
        "            \"k_dim\": self.k_dim,\n",
        "            \"u_dim\": self.u_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"n_r_size\": self.n_r_size,\n",
        "            \"local_contexts\": self.local_contexts,\n",
        "            \"batch_norm\": self.batch_norm}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n",
        "\n",
        "\n",
        "class LambdaNetwork(Model):\n",
        "  \"\"\"LambdaNetworks implementation. \"\"\"\n",
        "  \n",
        "  def __init__(self, \n",
        "               num_classes: int, \n",
        "               image_height: int, \n",
        "               image_width: int,\n",
        "               k_dim: int = 16,\n",
        "               u_dim: int = 1,\n",
        "               num_heads: int = 4,\n",
        "               n_r_size: int = None,\n",
        "               local_contexts: bool = False,\n",
        "               preprocess: bool = False,\n",
        "               **kwargs):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      num_classes: Number of classes.\n",
        "      image_height: Height of image.\n",
        "      image_width: Width of image.\n",
        "      k_dim: Dimension of key.\n",
        "      u_dim: Intra depth for multiquery heads.\n",
        "      num_heads: Number of heads for multiquery.\n",
        "      n_r_size: If `local_contexts=True` n = height*width of query,\n",
        "                else r = receptive field.\n",
        "      local_contexts: If True lambdaconv is used, \n",
        "                      else relative position embeddings are used.\n",
        "      preprocess: Whether to do preprocessing or not.\n",
        "      \n",
        "    \"\"\"\n",
        "    super().__init__(**kwargs)\n",
        "    \n",
        "    self.num_classes = num_classes\n",
        "    self.image_height = image_height\n",
        "    self.image_width = image_width\n",
        "    self.k_dim = k_dim\n",
        "    self.u_dim = u_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.n_r_size = n_r_size\n",
        "    self.local_contexts = local_contexts\n",
        "    self.preprocess = preprocess\n",
        "\n",
        "    self.rescale = Rescaling(1./255., name='rescale')\n",
        "    self.resize = Resizing(self.image_height, self.image_width, name='resize')\n",
        "\n",
        "    # Preprocessing\n",
        "    self.augs = [\n",
        "      RandomFlip(\"horizontal\", name='data_aug_flip'),\n",
        "      # RandomTranslation((-0.1, 0.1), (-0.1, 0.1), name='data_aug2'),\n",
        "      RandomRotation(0.02, name='data_aug_rot'),\n",
        "      RandomZoom(0.2, 0.2, name='data_aug_zoom'),\n",
        "    ]\n",
        "\n",
        "    # Model\n",
        "    self.conv1 = self.conv2d(32, 3, name='conv1', padding='same')\n",
        "    self.conv2 = self.conv2d(32, 3, name='conv2')\n",
        "    self.pool2 = self.maxpool2d(name='pool1')\n",
        "    self.drop2 = Dropout(0.5, name='drop2')\n",
        "\n",
        "    # self.lamb3 = Lambda(64, k_dim=8, u_dim=1, num_heads=2, n_r_size=14, \n",
        "    #                     local_contexts=False, batch_norm=False)\n",
        "    # self.lamb4 = Lambda(64, k_dim=8, u_dim=1, num_heads=2, n_r_size=14, \n",
        "    #                     local_contexts=False, batch_norm=False)\n",
        "    self.lamb3 = Lambda(64, k_dim=self.k_dim, u_dim=self.u_dim, \n",
        "                        num_heads=self.num_heads, n_r_size=self.n_r_size, \n",
        "                        local_contexts=self.local_contexts, batch_norm=False,\n",
        "                        name='lamb3')\n",
        "    self.lamb4 = Lambda(64, k_dim=self.k_dim, u_dim=self.u_dim, \n",
        "                        num_heads=self.num_heads, n_r_size=self.n_r_size, \n",
        "                        local_contexts=self.local_contexts, batch_norm=False,\n",
        "                        name='lamb4')\n",
        "    self.pool4 = self.maxpool2d(name='pool4')\n",
        "    self.drop4 = Dropout(0.25, name='drop4')\n",
        "\n",
        "    self.flat5 = Flatten(name='flat5')\n",
        "    self.dens5 = Dense(512, activation=tf.keras.activations.relu, name='dens5')\n",
        "    self.drop5 = Dropout(0.5, name='drop5')\n",
        "    self.logits = Dense(self.num_classes, name='logits')\n",
        "\n",
        "  def call(self, x, training: bool = None):\n",
        "    x = self.rescale(x)\n",
        "    x = self.resize(x)\n",
        "\n",
        "    if self.preprocess:\n",
        "      for aug_i in self.augs:\n",
        "        x = aug_i(x, training=training)\n",
        "\n",
        "    x = self.pool2(self.conv2(self.conv1(x)))\n",
        "    x = self.drop2(x, training=training)\n",
        "\n",
        "    x = self.pool4(self.lamb4(self.lamb3(x)))\n",
        "    x = self.drop4(x, training=training)\n",
        "\n",
        "    x = self.dens5(self.flat5(x))\n",
        "    x = self.drop5(x, training=training)\n",
        "    x = self.logits(x)\n",
        "\n",
        "    return x\n",
        "  \n",
        "  # https://stackoverflow.com/questions/55235212\n",
        "  def model(self, input_shape=None):\n",
        "    if input_shape is None:\n",
        "      x = Input(shape=(self.image_height, self.image_width, 3))\n",
        "    else:\n",
        "      x = Input(shape=input_shape)\n",
        "    return Model(inputs=[x], outputs=self.call(x))\n",
        "  \n",
        "  def get_config(self):\n",
        "      return {\"num_classes\": self.num_classes,\n",
        "              \"image_height\": self.image_height,\n",
        "              \"image_width\": self.image_width,\n",
        "              \"patch_size\": self.patch_size,\n",
        "              \"projection_dim\": self.projection_dim,\n",
        "              \"num_layers\": self.num_layers,\n",
        "              \"num_heads\": self.num_heads,\n",
        "              \"mlp_dims\": self.mlp_dims,\n",
        "              \"classifier_mlp_dims\": self.classifier_mlp_dims,\n",
        "              \"preprocess\": self.preprocess}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "      return cls(**config)\n",
        "  \n",
        "  @staticmethod\n",
        "  def conv2d(filters, kernel_size, strides=(1, 1), padding='valid', **kwargs):\n",
        "    return Conv2D(filters, \n",
        "                  kernel_size, \n",
        "                  strides, \n",
        "                  padding=padding, \n",
        "                  activation=tf.keras.activations.relu,\n",
        "                  **kwargs)\n",
        "    \n",
        "  @staticmethod\n",
        "  def maxpool2d(**kwargs):\n",
        "    return MaxPool2D([3, 3], 2, padding='valid', **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlrbSJscJwur"
      },
      "source": [
        "##Perceiver\n",
        "\n",
        "[Perceiver](https://arxiv.org/abs/2103.03206) from Jaegle et. al. .\n",
        "\n",
        "The code is based on [perceiver_pytorch.py](https://github.com/lucidrains/perceiver-pytorch/blob/main/perceiver_pytorch/perceiver_pytorch.py) from [lucidrains/perceiver_pytorch](https://github.com/lucidrains/perceiver_pytorch) .  \n",
        "\n",
        "**Good to know points from the paper**\n",
        "*  Linear layers are implemented using Dense. In paper it is implied that 1x1 conv was used.\n",
        "* \n",
        "* \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZVxWUrCWMZI"
      },
      "source": [
        "#! pip install einops\n",
        "from einops.layers.tensorflow import Rearrange\n",
        "from einops import repeat\n",
        "from datetime import datetime\n",
        "\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (\n",
        "  Add,\n",
        "  AveragePooling1D,\n",
        "  BatchNormalization, \n",
        "  Conv2D,\n",
        "  Conv3D,\n",
        "  Dense,\n",
        "  Dropout,\n",
        "  Embedding,\n",
        "  Layer,\n",
        "  LayerNormalization,\n",
        "  MultiHeadAttention\n",
        ")\n",
        "from tensorflow.keras.layers.experimental.preprocessing import (\n",
        "  Normalization,\n",
        "  RandomFlip,\n",
        "  RandomRotation,\n",
        "  RandomZoom,\n",
        "  Rescaling,\n",
        "  Resizing,\n",
        ")\n",
        "from tensorflow import einsum, meshgrid\n",
        "\n",
        "from functools import wraps\n",
        "from math import pi\n",
        "\n",
        "\n",
        "def cache_fn(func):\n",
        "    cache = None\n",
        "    @wraps(func)\n",
        "    def cached_func(*args, _cache=False, **kwargs):\n",
        "        if not _cache:\n",
        "            return func(*args, **kwargs)\n",
        "        nonlocal cache\n",
        "        if cache is not None:\n",
        "            return cache\n",
        "        cache = func(*args, **kwargs)\n",
        "        return cache\n",
        "    return cached_func\n",
        "\n",
        "\n",
        "def fourier_encode(x: tf.Tensor, \n",
        "                   max_freq: float, \n",
        "                   num_bands: int = 4, \n",
        "                   base: float = 2.0) -> tf.Tensor:\n",
        "  \"\"\"Positional fourier encoding.\"\"\"\n",
        "  x = tf.expand_dims(x, -1)\n",
        "  dtype, orig_x = x.dtype, x\n",
        "  scales = tf.experimental.numpy.logspace(\n",
        "    start=0., \n",
        "    stop=tf.math.log(max_freq / 2) / tf.math.log(base), \n",
        "    num=num_bands, \n",
        "    base = base, \n",
        "    dtype = dtype)\n",
        "  scales = Rearrange('s -> () () () s')(scales)\n",
        "  # x = [h,w,2,bands]\n",
        "  x = x * scales * pi\n",
        "  x = tf.concat([tf.math.sin(x), tf.math.cos(x), orig_x], axis=-1)\n",
        "  return x\n",
        "\n",
        "\n",
        "class InputByteArray(Layer):\n",
        "  \"\"\"Prepares input byte array for K, V.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               image_height: int = None, \n",
        "               image_width: int = None,\n",
        "               batch_size: int = None, \n",
        "               max_freq: float = 224,\n",
        "               num_freq_bands: int = 64,\n",
        "               freq_base = 2.0,\n",
        "               **kwargs):\n",
        "    \"\"\"Constructor\"\"\"\n",
        "    super().__init__(**kwargs)\n",
        "    self.max_freq = max_freq\n",
        "    self.num_freq_bands = num_freq_bands\n",
        "    self.freq_base = freq_base\n",
        "    self.batch_size = batch_size\n",
        "    self.image_height = image_height\n",
        "    self.image_width = image_width\n",
        "\n",
        "  @cache_fn\n",
        "  def positional_encoding(self, x):\n",
        "    x_shape = x.get_shape().as_list()\n",
        "    assert len(x_shape) == 4, 'input data must have dim of 4'\n",
        "    # 1. calculate fourier encoded positions in the range of [-1, 1]\n",
        "    axis_pos = list(map(lambda num: tf.linspace(-1., 1., num), x_shape[1:3]))\n",
        "    pos = tf.stack(meshgrid(*axis_pos), axis=-1)\n",
        "    enc_pos = fourier_encode(pos, \n",
        "                             max_freq=self.max_freq, \n",
        "                             num_bands=self.num_freq_bands, \n",
        "                             base=self.freq_base)\n",
        "    # 2. Merge the stacked dimension\n",
        "    enc_pos = Rearrange('... n d -> ... (n d)')(enc_pos)\n",
        "    # 3. Tile to the batch size\n",
        "    # enc_pos = repeat(enc_pos, '... -> b ...', b=x_shape[0])\n",
        "    enc_pos = tf.tile(tf.expand_dims(enc_pos, axis=0), \n",
        "                      [tf.shape(x)[0], 1, 1, 1], \n",
        "                      name='enc_pos')\n",
        "    return enc_pos\n",
        "\n",
        "  def call(self, x):\n",
        "    # x_shape = x.get_shape().as_list()\n",
        "    # assert len(x_shape) == 4, 'input data must have dim of 4'\n",
        "    # # 1. calculate fourier encoded positions in the range of [-1, 1]\n",
        "    # axis_pos = list(map(lambda num: tf.linspace(-1., 1., num), x_shape[1:3]))\n",
        "    # pos = tf.stack(meshgrid(*axis_pos), axis=-1)\n",
        "    # enc_pos = fourier_encode(pos, \n",
        "    #                          max_freq=self.max_freq, \n",
        "    #                          num_bands=self.num_freq_bands, \n",
        "    #                          base=self.freq_base)\n",
        "    # # 2. Merge the stacked dimension\n",
        "    # enc_pos = Rearrange('... n d -> ... (n d)')(enc_pos)\n",
        "    # # 3. Tile to the batch size\n",
        "    # # enc_pos = repeat(enc_pos, '... -> b ...', b=x_shape[0])\n",
        "    # enc_pos = tf.tile(tf.expand_dims(enc_pos, axis=0), \n",
        "    #                   [tf.shape(x)[0], 1, 1, 1], \n",
        "    #                   name='enc_pos')\n",
        "    # 4. concat to channels of data and flatten axis\n",
        "    _cache = self.batch_size and self.image_height and self.image_width\n",
        "    enc_pos = self.positional_encoding(x, _cache=_cache)\n",
        "    x = tf.concat([x, enc_pos], axis=-1, name='data_input')\n",
        "    x = Rearrange('b ... d -> b (...) d')(x)\n",
        "    return x\n",
        "\n",
        "  def get_config(self):\n",
        "    return {'batch_size': self.batch_size,\n",
        "            'image_height': self.image_height,\n",
        "            'image_width': self.image_width,\n",
        "            'max_freq': self.max_freq,\n",
        "            'num_freq_bands': self.num_freq_bands,\n",
        "            'freq_base': self.freq_base}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n",
        "\n",
        "\n",
        "class InputLatentArray(Layer):\n",
        "  \"\"\"Prepares latent byte array for Q.\"\"\"\n",
        "\n",
        "  def __init__(self, num_latents: int = 1024, latent_dim: int = 512, **kwargs):\n",
        "    \"\"\"Constructor\"\"\"\n",
        "    super().__init__(**kwargs)\n",
        "    self.num_latents = num_latents\n",
        "    self.latent_dim = latent_dim\n",
        "    self.latents = self.add_weight('latent', \n",
        "                                   [self.num_latents, self.latent_dim],\n",
        "                                   tf.float32,\n",
        "                                   trainable=True)\n",
        "\n",
        "  def call(self, x):\n",
        "    x_shape = x.get_shape().as_list()\n",
        "    assert len(x_shape) == 4, 'input data must have dim of 4'\n",
        "    # z = repeat(self.latents, 'n d -> b n d', b=x_shape[0])\n",
        "    z = tf.tile(tf.expand_dims(self.latents, axis=0), \n",
        "                [tf.shape(x)[0], 1, 1], \n",
        "                name='latent_input')\n",
        "    return z\n",
        "\n",
        "  def get_config(self):\n",
        "    return {'num_latents': self.num_latents,\n",
        "            'latent_dim': self.latent_dim}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n",
        "\n",
        "\n",
        "class GELU(Layer):\n",
        "  \"\"\"GELU with gated linear unit option.\"\"\"\n",
        "\n",
        "  def __init__(self, glu: bool = True, **kwargs):\n",
        "    \"\"\"Constructor\"\"\"\n",
        "    super().__init__(**kwargs)\n",
        "    self.glu = glu\n",
        "    self.gelu = tf.keras.layers.Activation('gelu')\n",
        "\n",
        "  def call(self, x):\n",
        "    if self.glu:\n",
        "      return self.gelu(x)\n",
        "    else:\n",
        "      x_input, x_gates = tf.split(x, 2, axis=-1)\n",
        "      return x_input * self.gelu(x_gates)\n",
        "\n",
        "  def get_config(self):\n",
        "    return {'glu': self.glu}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n",
        "\n",
        "\n",
        "class MLP(Layer):\n",
        "  \"\"\"Multilayer perceptron.\"\"\"\n",
        "  def __init__(self,\n",
        "               latent_dims: list,\n",
        "               geglu: bool = True, \n",
        "               dropout_rate: float = 0.25, \n",
        "               **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.latent_dims = latent_dims\n",
        "    self.geglu = geglu\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.lrn   = LayerNormalization(epsilon=1e-6, name='ln')\n",
        "    self.dens1 = Dense(latent_dims[0], name='dens1', use_bias=False)\n",
        "    self.gelu  = GELU(geglu, name=f'{\"geglu\" if geglu else \"gelu\"}')\n",
        "    self.dens2 = Dense(latent_dims[1], name='dens2')\n",
        "    self.drop  = Dropout(dropout_rate, name='drop')\n",
        "\n",
        "  def call(self, x, training: bool = None):\n",
        "    x = self.lrn(x)\n",
        "    x = self.dens1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.dens2(x)\n",
        "    x = self.drop(x)\n",
        "    return x\n",
        "  \n",
        "  def get_config(self):\n",
        "    return {\"latent_dims\": self.latent_dims,\n",
        "            \"geglu\": self.geglu,\n",
        "            \"dropout_rate\": self.dropout_rate}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n",
        "\n",
        "\n",
        "def mlp(latent_dims: list,\n",
        "        geglu: bool = True, \n",
        "        dropout_rate: float = 0.25, \n",
        "        name: str = '') -> list:\n",
        "  \"\"\"Multilayer perceptron.\n",
        "  \n",
        "  Same as MLP(...). But this is not a layer, meaning that the layers listed \n",
        "  here will be shown separately in model.summary() .\n",
        "  \"\"\"\n",
        "  assert len(latent_dims) == 2, \"Length of `latent_dims` must be 2.\"\n",
        "  layers = []  \n",
        "  layers.append(LayerNormalization(epsilon=1e-6, name=f'{name}/ln{idx}'))\n",
        "  layers.append(Dense(latent_dims[0], name=f'{name}/dens1'))\n",
        "  layers.append(GELU(geglu, name=f'{name}/{\"geglu\" if geglu else \"gelu\"}'))\n",
        "  layers.append(Dense(latent_dims[1], name=f'{name}/dens2'))\n",
        "  layers.append(Dropout(dropout_rate, name=f'{name}/drop'))\n",
        "  return layers\n",
        "\n",
        "\n",
        "class Transformer(Layer):\n",
        "  \"\"\"Transformer module for cross attention or self attention.\n",
        "  \n",
        "  This is different than the `MultiHeadAttention` (MHA) layer. MHA shares the \n",
        "  same Q and K dimensions whereas this layer shares the K and V dimensions.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, \n",
        "               query_dim: int, \n",
        "               use_context: bool = False, \n",
        "               num_heads: int = 8, \n",
        "               dim_head: int = 64, \n",
        "               dropout: float = 0.25,\n",
        "               **kwargs):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      query_dim: Dimension of Query.\n",
        "      use_context: Whether context (Key, Value) is used.\n",
        "      num_heads: Number of heads.\n",
        "      dim_head: Dimension per head.\n",
        "      dropout: Dropout rate.\n",
        "      \n",
        "    \"\"\"\n",
        "    super().__init__(**kwargs)\n",
        "    self.query_dim = query_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.dim_head = dim_head\n",
        "    self.dropout = dropout\n",
        "    self.use_context = use_context\n",
        "\n",
        "    self.lrn = LayerNormalization(epsilon=1e-6, name='ln')\n",
        "\n",
        "    if self.use_context:\n",
        "      self.lrn_ctxt = LayerNormalization(epsilon=1e-6, name='ln_ctxt')\n",
        "    else:\n",
        "      self.lrn_ctxt = None\n",
        "\n",
        "    # Reference : VIT paper appendix.\n",
        "    self.scale = dim_head ** -0.5\n",
        "\n",
        "    inner_dim = self.dim_head * self.num_heads\n",
        "\n",
        "    self.to_q  = Dense(inner_dim, use_bias=False)\n",
        "    self.to_kv = Dense(inner_dim * 2, use_bias=False)\n",
        "\n",
        "    self.dens  = Dense(self.query_dim, name='dens')\n",
        "    # self.drop1 = Dropout(self.dropout, name='drop1')\n",
        "    self.drop2 = Dropout(self.dropout, name='drop2')\n",
        "\n",
        "  def call(self, x: tf.Tensor, context: tf.Tensor = None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: Latent data.\n",
        "      context: Input data.\n",
        "\n",
        "    \"\"\"\n",
        "    x = self.lrn(x)\n",
        "    q = self.to_q(x)\n",
        "\n",
        "    if context is not None:\n",
        "      assert self.lrn_ctxt, '`self.lrn_ctxt` is False.'\n",
        "      context = self.lrn_ctxt(context)\n",
        "      kv = self.to_kv(context)\n",
        "    else:\n",
        "      kv = self.to_kv(x)\n",
        "\n",
        "    k, v = tf.split(kv, 2, axis=-1)\n",
        "\n",
        "    q = Rearrange('b n (h d) -> (b h) n d', h=self.num_heads)(q)\n",
        "    k = Rearrange('b n (h d) -> (b h) n d', h=self.num_heads)(k)\n",
        "    v = Rearrange('b n (h d) -> (b h) n d', h=self.num_heads)(v)\n",
        "\n",
        "    sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
        "    attn = tf.math.softmax(sim, axis=-1)\n",
        "    # attn = self.drop1(attn)\n",
        "\n",
        "    x = einsum('b i j, b j d -> b i d', attn, v)\n",
        "    x = Rearrange('(b h) n d -> b n (h d)', h=self.num_heads)(x)\n",
        "\n",
        "    x = self.dens(x)\n",
        "    x = self.drop2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def get_config(self):\n",
        "    return {'query_dim'  : self.query_dim  ,\n",
        "            'use_context': self.use_context,\n",
        "            'num_heads'  : self.num_heads  ,\n",
        "            'dim_head'   : self.dim_head   ,\n",
        "            'dropout'    : self.dropout    }\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)\n",
        "\n",
        "\n",
        "class Perceiver(Model):\n",
        "  \"\"\"Perceiver implementation. \"\"\"\n",
        "  \n",
        "  def __init__(self,\n",
        "               num_classes: int, \n",
        "               image_height: int, \n",
        "               image_width: int,\n",
        "               batch_size: int = None,\n",
        "               max_freq: float = 224,\n",
        "               num_freq_bands: int = 64,\n",
        "               freq_base: float = 2.0,\n",
        "               num_latents: int = 1024,\n",
        "               latent_dim: int = 512,\n",
        "               latent_heads: int = 8,\n",
        "               latent_head_dim: int = 512,\n",
        "               cross_heads: int = 1,\n",
        "               cross_head_dim: int = 512,\n",
        "               attn_dropout: float = 0.2,\n",
        "               mlp_dropout: float = 0.2,\n",
        "               cross_depth: int = 8,\n",
        "               latent_depth: int = 6,             \n",
        "               weight_tie_layers: bool = False,\n",
        "               preprocess: bool = False,\n",
        "               **kwargs):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      num_classes: Number of classes.\n",
        "      image_height: Height of image.\n",
        "      image_width: Width of image.\n",
        "      max_freq:locked_drop_o\n",
        "      num_freq_bands:\n",
        "      freq_base:\n",
        "      num_latents:\n",
        "      latent_dim:\n",
        "      latent_heads:\n",
        "      latent_head_dim:\n",
        "      cross_heads:\n",
        "      cross_head_dim:\n",
        "      attn_dropout:\n",
        "      mlp_dropout:\n",
        "      cross_depth:\n",
        "      latent_depth:\n",
        "      weight_tie_layers:\n",
        "      preprocess: Whether to do preprocessing or not.\n",
        "      \n",
        "    \"\"\"\n",
        "    super().__init__(**kwargs)\n",
        "    \n",
        "    self.num_classes = num_classes\n",
        "    self.image_height = image_height\n",
        "    self.image_width = image_width\n",
        "    self.preprocess = preprocess\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    # Position encoding\n",
        "    self.max_freq = max_freq\n",
        "    self.num_freq_bands = num_freq_bands\n",
        "    self.freq_base = freq_base\n",
        "\n",
        "    # Latent transformer\n",
        "    self.num_latents = num_latents\n",
        "    self.latent_dim = latent_dim\n",
        "    self.latent_heads = latent_heads\n",
        "    self.latent_head_dim = latent_head_dim\n",
        "\n",
        "    # Cross attention\n",
        "    self.cross_heads = cross_heads\n",
        "    self.cross_head_dim = cross_head_dim\n",
        "\n",
        "    self.attn_dropout = attn_dropout\n",
        "    self.mlp_dropout = mlp_dropout\n",
        "\n",
        "    # How many latent transformer repeats and how many input data attends.\n",
        "    self.cross_depth = cross_depth\n",
        "    self.latent_depth = latent_depth\n",
        "    self.weight_tie_layers = weight_tie_layers\n",
        "\n",
        "    self.rescale = Rescaling(1./255., name='rescale')\n",
        "    self.resize = Resizing(self.image_height, self.image_width, name='resize')\n",
        "\n",
        "    # Preprocessing ------------------------------------------------------------\n",
        "    self.augs = [\n",
        "      RandomFlip(\"horizontal\", name='data_aug_flip'),\n",
        "      # RandomTranslation((-0.1, 0.1), (-0.1, 0.1), name='data_aug2'),\n",
        "      RandomRotation(0.02, name='data_aug_rot'),\n",
        "      RandomZoom(0.2, 0.2, name='data_aug_zoom'),\n",
        "    ]\n",
        "\n",
        "    # Model --------------------------------------------------------------------\n",
        "    # data array for key and value.\n",
        "    self.input_data = InputByteArray(batch_size=self.batch_size,\n",
        "                                     image_height=self.image_height,\n",
        "                                     image_width=self.image_width,\n",
        "                                     max_freq=self.max_freq, \n",
        "                                     num_freq_bands=self.num_freq_bands, \n",
        "                                     freq_base=self.freq_base,\n",
        "                                     name='input_data')\n",
        "    \n",
        "    # latent array for query.\n",
        "    self.input_latent = InputLatentArray(num_latents=self.num_latents, \n",
        "                                         latent_dim=self.latent_dim, \n",
        "                                         name='input_latent')\n",
        "    \n",
        "    # context refers to the input data array.\n",
        "    input_axis, input_channels = 2, 3\n",
        "    # self.context_dim = input_axis * ((self.num_freq_bands * 2) + 1) + input_channels\n",
        "\n",
        "    # cross attention module.\n",
        "    cross_attn  = self.get_cross_attn()\n",
        "    cross_mlp   = self.get_cross_mlp()\n",
        "\n",
        "    # latent attention module. \n",
        "    # We do not want to share the weights within a module.\n",
        "    latent_attn = [self.get_latent_attn() for _ in range(self.latent_depth)]\n",
        "    latent_mlp  = [self.get_latent_mlp() for _ in range(self.latent_depth)]\n",
        "\n",
        "    # stack modules to form Perceiver.\n",
        "    self.blocks = []\n",
        "    for i in range(self.cross_depth):\n",
        "      cache_args = {'_cache': i > 0 and self.weight_tie_layers}\n",
        "      cross_attn_i = cross_attn(cross_idx=i, **cache_args)\n",
        "      cross_mlp_i  = cross_mlp(cross_idx=i)\n",
        "      self.blocks.append((cross_attn_i, f'cross_attn_{i}'))\n",
        "      self.blocks.append((cross_mlp_i,  f'cross_mlp_{i}' ))\n",
        "      for j in range(self.latent_depth):\n",
        "        latent_attn_j = latent_attn[j](cross_idx=i, latent_idx=j, **cache_args)\n",
        "        latent_mlp_j  = latent_mlp[j](cross_idx=i, latent_idx=j)\n",
        "        self.blocks.append((latent_attn_j, f'latent_attn_{i}_{j}'))\n",
        "        self.blocks.append((latent_mlp_j,  f'latent_mlp_{i}_{j}'))\n",
        "\n",
        "    self.avgpool = AveragePooling1D(pool_size=self.num_latents, name='avgpool')\n",
        "    self.lrn = LayerNormalization(epsilon=1e-6, name='ln')\n",
        "    self.logits = Dense(self.num_classes, name='logits')\n",
        "\n",
        "  def call(self, x, training: bool = None):\n",
        "    x = self.rescale(x)\n",
        "    x = self.resize(x)\n",
        "\n",
        "    if self.preprocess:\n",
        "      for aug_i in self.augs:\n",
        "        x = aug_i(x, training=training)\n",
        "\n",
        "    z = self.input_latent(x)\n",
        "    x = self.input_data(x)\n",
        "\n",
        "    for l in self.blocks:\n",
        "      if 'cross_attn' in l[1]:\n",
        "        z = Add(name='skip_' + l[1])([l[0](z, context=x), z])\n",
        "      else:\n",
        "        z = Add(name='skip_' + l[1])([l[0](z), z])\n",
        "\n",
        "    z = self.avgpool(z)\n",
        "    # z = self.lrn(z)\n",
        "    z = self.logits(z)\n",
        "\n",
        "    return z\n",
        "  \n",
        "  def get_cross_attn(self):\n",
        "    \"\"\"Transformer of the cross attention module.\n",
        "    Returns a function that can be cached, enabling weight sharing.\n",
        "    \"\"\"\n",
        "    @cache_fn\n",
        "    def cross_attn(cross_idx: int = 0):\n",
        "      return Transformer(query_dim=self.latent_dim, \n",
        "                         use_context=True, \n",
        "                         num_heads=self.cross_heads, \n",
        "                         dim_head=self.cross_head_dim, \n",
        "                         dropout=self.attn_dropout, \n",
        "                         name=f'cross_attn_{cross_idx}')\n",
        "    return cross_attn\n",
        "    \n",
        "  def get_cross_mlp(self):\n",
        "    \"\"\"MLP of the cross attention module. \n",
        "    Returns a function that can be cached, enabling weight sharing.    \n",
        "    \"\"\"\n",
        "    @cache_fn\n",
        "    def cross_mlp(cross_idx: int = 0):\n",
        "      return MLP(latent_dims=[self.latent_dim, self.latent_dim],\n",
        "                 geglu=True,\n",
        "                 dropout_rate=self.mlp_dropout,\n",
        "                 name=f'cross_mlp_{cross_idx}')\n",
        "    return cross_mlp\n",
        "\n",
        "  def get_latent_attn(self):\n",
        "    \"\"\"Transformer of the latent attention module.\n",
        "    Returns a function that can be cached, enabling weight sharing.\n",
        "    \"\"\"\n",
        "    @cache_fn\n",
        "    def latent_attn(cross_idx: int = 0, latent_idx: int = 0):\n",
        "      return Transformer(query_dim=self.latent_dim, \n",
        "                         use_context=False, \n",
        "                         num_heads=self.latent_heads, \n",
        "                         dim_head=self.latent_head_dim, \n",
        "                         dropout=self.attn_dropout, \n",
        "                         name=f'latent_attn_{cross_idx}_{latent_idx}')\n",
        "    return latent_attn\n",
        "  \n",
        "  def get_latent_mlp(self):\n",
        "    \"\"\"MLP of the latent attention module. \n",
        "    Returns a function that can be cached, enabling weight sharing.    \n",
        "    \"\"\"\n",
        "    @cache_fn      \n",
        "    def latent_mlp(cross_idx: int = 0, latent_idx: int = 0):\n",
        "      return MLP(latent_dims=[self.latent_dim, self.latent_dim],\n",
        "                 geglu=True,\n",
        "                 dropout_rate=self.mlp_dropout,\n",
        "                 name=f'latent_mlp_{cross_idx}_{latent_idx}')\n",
        "    return latent_mlp\n",
        "\n",
        "  # https://stackoverflow.com/questions/55235212\n",
        "  def model(self, input_shape=None):\n",
        "    if input_shape is None:\n",
        "      x = Input(shape=(self.image_height, self.image_width, 3))\n",
        "    else:\n",
        "      x = Input(shape=input_shape)\n",
        "    return Model(inputs=[x], outputs=self.call(x))\n",
        "  \n",
        "  def get_config(self):\n",
        "      return {\"num_classes\"       : self.num_classes,\n",
        "              \"image_height\"      : self.image_height,\n",
        "              \"image_width\"       : self.image_width,\n",
        "              \"batch_size\"        : self.batch_size,\n",
        "              \"max_freq\"          : self.max_freq,\n",
        "              \"num_freq_bands\"    : self.num_freq_bands,\n",
        "              \"freq_base\"         : self.freq_base,\n",
        "              \"num_latents\"       : self.num_latents,\n",
        "              \"latent_dim\"        : self.latent_dim,\n",
        "              \"latent_heads\"      : self.latent_heads,\n",
        "              \"latent_head_dim\"   : self.latent_head_dim,\n",
        "              \"cross_heads\"       : self.cross_heads,\n",
        "              \"cross_head_dim\"    : self.cross_head_dim,\n",
        "              \"attn_dropout\"      : self.attn_dropout,\n",
        "              \"mlp_dropout\"       : self.mlp_dropout,\n",
        "              \"cross_depth\"       : self.cross_depth,\n",
        "              \"latent_depth\"      : self.latent_depth,\n",
        "              \"weight_tie_layers\" : self.weight_tie_layers,\n",
        "              \"preprocess\"        : self.preprocess}\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "      return cls(**config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsy68zdDF-a7",
        "outputId": "c43feaf0-f14f-4d4f-d8f8-2cbc9ee9dc84"
      },
      "source": [
        "model = Perceiver(num_classes=1000, \n",
        "                  image_height=224, \n",
        "                  image_width=224,\n",
        "                  max_freq=224,\n",
        "                  num_freq_bands=64,\n",
        "                  freq_base=2.0,\n",
        "                  num_latents=1024,\n",
        "                  latent_dim=512,\n",
        "                  latent_heads=8,\n",
        "                  latent_head_dim=512//8,\n",
        "                  cross_heads=1,\n",
        "                  cross_head_dim=512,\n",
        "                  attn_dropout=0.2,\n",
        "                  mlp_dropout=0.2,\n",
        "                  cross_depth=8,\n",
        "                  latent_depth=6,             \n",
        "                  weight_tie_layers=True,\n",
        "                  preprocess=False).model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 224, 224, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 224, 224, 3)  0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 1024, 512)    524288      resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 50176, 261)   0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 1024, 512)    793610      input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 1024, 512)    0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 1024, 512)    525824      skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 1024, 512)    0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 1024, 512)    1050112     skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 1024, 512)    0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 1024, 512)    0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 1024, 512)    1050112     skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 1024, 512)    0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 1024, 512)    0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 1024, 512)    1050112     skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 1024, 512)    0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 1024, 512)    0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 1024, 512)    1050112     skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 1024, 512)    0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 1024, 512)    0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_4 (Transformer)   (None, 1024, 512)    1050112     skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_4 (Add)      (None, 1024, 512)    0           latent_attn_0_4[0][0]            \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_4 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_0_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_4 (Add)       (None, 1024, 512)    0           latent_mlp_0_4[0][0]             \n",
            "                                                                 skip_latent_attn_0_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_5 (Transformer)   (None, 1024, 512)    1050112     skip_latent_mlp_0_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_5 (Add)      (None, 1024, 512)    0           latent_attn_0_5[0][0]            \n",
            "                                                                 skip_latent_mlp_0_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_5 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_0_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_5 (Add)       (None, 1024, 512)    0           latent_mlp_0_5[0][0]             \n",
            "                                                                 skip_latent_attn_0_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 1024, 512)    793610      skip_latent_mlp_0_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_1_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_2_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_3_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_4_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_5_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_6_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 1024, 512)    0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 1024, 512)    525824      skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 1024, 512)    0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 1024, 512)    1050112     skip_cross_mlp_1[0][0]           \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "                                                                 skip_cross_mlp_4[0][0]           \n",
            "                                                                 skip_cross_mlp_5[0][0]           \n",
            "                                                                 skip_cross_mlp_6[0][0]           \n",
            "                                                                 skip_cross_mlp_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 1024, 512)    0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 1024, 512)    0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 1024, 512)    1050112     skip_latent_mlp_1_0[0][0]        \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "                                                                 skip_latent_mlp_4_0[0][0]        \n",
            "                                                                 skip_latent_mlp_5_0[0][0]        \n",
            "                                                                 skip_latent_mlp_6_0[0][0]        \n",
            "                                                                 skip_latent_mlp_7_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 1024, 512)    0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 1024, 512)    0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 1024, 512)    1050112     skip_latent_mlp_1_1[0][0]        \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "                                                                 skip_latent_mlp_4_1[0][0]        \n",
            "                                                                 skip_latent_mlp_5_1[0][0]        \n",
            "                                                                 skip_latent_mlp_6_1[0][0]        \n",
            "                                                                 skip_latent_mlp_7_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 1024, 512)    0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 1024, 512)    0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 1024, 512)    1050112     skip_latent_mlp_1_2[0][0]        \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "                                                                 skip_latent_mlp_3_2[0][0]        \n",
            "                                                                 skip_latent_mlp_4_2[0][0]        \n",
            "                                                                 skip_latent_mlp_5_2[0][0]        \n",
            "                                                                 skip_latent_mlp_6_2[0][0]        \n",
            "                                                                 skip_latent_mlp_7_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 1024, 512)    0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 1024, 512)    0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_4 (Transformer)   (None, 1024, 512)    1050112     skip_latent_mlp_1_3[0][0]        \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "                                                                 skip_latent_mlp_3_3[0][0]        \n",
            "                                                                 skip_latent_mlp_4_3[0][0]        \n",
            "                                                                 skip_latent_mlp_5_3[0][0]        \n",
            "                                                                 skip_latent_mlp_6_3[0][0]        \n",
            "                                                                 skip_latent_mlp_7_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_4 (Add)      (None, 1024, 512)    0           latent_attn_1_4[0][0]            \n",
            "                                                                 skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_4 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_1_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_4 (Add)       (None, 1024, 512)    0           latent_mlp_1_4[0][0]             \n",
            "                                                                 skip_latent_attn_1_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_5 (Transformer)   (None, 1024, 512)    1050112     skip_latent_mlp_1_4[0][0]        \n",
            "                                                                 skip_latent_mlp_2_4[0][0]        \n",
            "                                                                 skip_latent_mlp_3_4[0][0]        \n",
            "                                                                 skip_latent_mlp_4_4[0][0]        \n",
            "                                                                 skip_latent_mlp_5_4[0][0]        \n",
            "                                                                 skip_latent_mlp_6_4[0][0]        \n",
            "                                                                 skip_latent_mlp_7_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_5 (Add)      (None, 1024, 512)    0           latent_attn_1_5[0][0]            \n",
            "                                                                 skip_latent_mlp_1_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_5 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_1_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_5 (Add)       (None, 1024, 512)    0           latent_mlp_1_5[0][0]             \n",
            "                                                                 skip_latent_attn_1_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_2 (Add)         (None, 1024, 512)    0           cross_attn_1[1][0]               \n",
            "                                                                 skip_latent_mlp_1_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_2 (MLP)               (None, 1024, 512)    525824      skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_2 (Add)          (None, 1024, 512)    0           cross_mlp_2[0][0]                \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_0 (Add)      (None, 1024, 512)    0           latent_attn_1_0[1][0]            \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_0 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_0 (Add)       (None, 1024, 512)    0           latent_mlp_2_0[0][0]             \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_1 (Add)      (None, 1024, 512)    0           latent_attn_1_1[1][0]            \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_1 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_1 (Add)       (None, 1024, 512)    0           latent_mlp_2_1[0][0]             \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_2 (Add)      (None, 1024, 512)    0           latent_attn_1_2[1][0]            \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_2 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_2 (Add)       (None, 1024, 512)    0           latent_mlp_2_2[0][0]             \n",
            "                                                                 skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_3 (Add)      (None, 1024, 512)    0           latent_attn_1_3[1][0]            \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_3 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_2_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_3 (Add)       (None, 1024, 512)    0           latent_mlp_2_3[0][0]             \n",
            "                                                                 skip_latent_attn_2_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_4 (Add)      (None, 1024, 512)    0           latent_attn_1_4[1][0]            \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_4 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_2_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_4 (Add)       (None, 1024, 512)    0           latent_mlp_2_4[0][0]             \n",
            "                                                                 skip_latent_attn_2_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_5 (Add)      (None, 1024, 512)    0           latent_attn_1_5[1][0]            \n",
            "                                                                 skip_latent_mlp_2_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_5 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_2_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_5 (Add)       (None, 1024, 512)    0           latent_mlp_2_5[0][0]             \n",
            "                                                                 skip_latent_attn_2_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_3 (Add)         (None, 1024, 512)    0           cross_attn_1[2][0]               \n",
            "                                                                 skip_latent_mlp_2_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_3 (MLP)               (None, 1024, 512)    525824      skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_3 (Add)          (None, 1024, 512)    0           cross_mlp_3[0][0]                \n",
            "                                                                 skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_0 (Add)      (None, 1024, 512)    0           latent_attn_1_0[2][0]            \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_0 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_0 (Add)       (None, 1024, 512)    0           latent_mlp_3_0[0][0]             \n",
            "                                                                 skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_1 (Add)      (None, 1024, 512)    0           latent_attn_1_1[2][0]            \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_1 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_1 (Add)       (None, 1024, 512)    0           latent_mlp_3_1[0][0]             \n",
            "                                                                 skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_2 (Add)      (None, 1024, 512)    0           latent_attn_1_2[2][0]            \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_2 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_3_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_2 (Add)       (None, 1024, 512)    0           latent_mlp_3_2[0][0]             \n",
            "                                                                 skip_latent_attn_3_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_3 (Add)      (None, 1024, 512)    0           latent_attn_1_3[2][0]            \n",
            "                                                                 skip_latent_mlp_3_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_3 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_3_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_3 (Add)       (None, 1024, 512)    0           latent_mlp_3_3[0][0]             \n",
            "                                                                 skip_latent_attn_3_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_4 (Add)      (None, 1024, 512)    0           latent_attn_1_4[2][0]            \n",
            "                                                                 skip_latent_mlp_3_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_4 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_3_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_4 (Add)       (None, 1024, 512)    0           latent_mlp_3_4[0][0]             \n",
            "                                                                 skip_latent_attn_3_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_5 (Add)      (None, 1024, 512)    0           latent_attn_1_5[2][0]            \n",
            "                                                                 skip_latent_mlp_3_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_5 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_3_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_5 (Add)       (None, 1024, 512)    0           latent_mlp_3_5[0][0]             \n",
            "                                                                 skip_latent_attn_3_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_4 (Add)         (None, 1024, 512)    0           cross_attn_1[3][0]               \n",
            "                                                                 skip_latent_mlp_3_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_4 (MLP)               (None, 1024, 512)    525824      skip_cross_attn_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_4 (Add)          (None, 1024, 512)    0           cross_mlp_4[0][0]                \n",
            "                                                                 skip_cross_attn_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_0 (Add)      (None, 1024, 512)    0           latent_attn_1_0[3][0]            \n",
            "                                                                 skip_cross_mlp_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_0 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_4_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_0 (Add)       (None, 1024, 512)    0           latent_mlp_4_0[0][0]             \n",
            "                                                                 skip_latent_attn_4_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_1 (Add)      (None, 1024, 512)    0           latent_attn_1_1[3][0]            \n",
            "                                                                 skip_latent_mlp_4_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_1 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_4_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_1 (Add)       (None, 1024, 512)    0           latent_mlp_4_1[0][0]             \n",
            "                                                                 skip_latent_attn_4_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_2 (Add)      (None, 1024, 512)    0           latent_attn_1_2[3][0]            \n",
            "                                                                 skip_latent_mlp_4_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_2 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_4_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_2 (Add)       (None, 1024, 512)    0           latent_mlp_4_2[0][0]             \n",
            "                                                                 skip_latent_attn_4_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_3 (Add)      (None, 1024, 512)    0           latent_attn_1_3[3][0]            \n",
            "                                                                 skip_latent_mlp_4_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_3 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_4_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_3 (Add)       (None, 1024, 512)    0           latent_mlp_4_3[0][0]             \n",
            "                                                                 skip_latent_attn_4_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_4 (Add)      (None, 1024, 512)    0           latent_attn_1_4[3][0]            \n",
            "                                                                 skip_latent_mlp_4_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_4 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_4_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_4 (Add)       (None, 1024, 512)    0           latent_mlp_4_4[0][0]             \n",
            "                                                                 skip_latent_attn_4_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_5 (Add)      (None, 1024, 512)    0           latent_attn_1_5[3][0]            \n",
            "                                                                 skip_latent_mlp_4_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_5 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_4_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_5 (Add)       (None, 1024, 512)    0           latent_mlp_4_5[0][0]             \n",
            "                                                                 skip_latent_attn_4_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_5 (Add)         (None, 1024, 512)    0           cross_attn_1[4][0]               \n",
            "                                                                 skip_latent_mlp_4_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_5 (MLP)               (None, 1024, 512)    525824      skip_cross_attn_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_5 (Add)          (None, 1024, 512)    0           cross_mlp_5[0][0]                \n",
            "                                                                 skip_cross_attn_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_0 (Add)      (None, 1024, 512)    0           latent_attn_1_0[4][0]            \n",
            "                                                                 skip_cross_mlp_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_0 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_5_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_0 (Add)       (None, 1024, 512)    0           latent_mlp_5_0[0][0]             \n",
            "                                                                 skip_latent_attn_5_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_1 (Add)      (None, 1024, 512)    0           latent_attn_1_1[4][0]            \n",
            "                                                                 skip_latent_mlp_5_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_1 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_5_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_1 (Add)       (None, 1024, 512)    0           latent_mlp_5_1[0][0]             \n",
            "                                                                 skip_latent_attn_5_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_2 (Add)      (None, 1024, 512)    0           latent_attn_1_2[4][0]            \n",
            "                                                                 skip_latent_mlp_5_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_2 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_5_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_2 (Add)       (None, 1024, 512)    0           latent_mlp_5_2[0][0]             \n",
            "                                                                 skip_latent_attn_5_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_3 (Add)      (None, 1024, 512)    0           latent_attn_1_3[4][0]            \n",
            "                                                                 skip_latent_mlp_5_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_3 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_5_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_3 (Add)       (None, 1024, 512)    0           latent_mlp_5_3[0][0]             \n",
            "                                                                 skip_latent_attn_5_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_4 (Add)      (None, 1024, 512)    0           latent_attn_1_4[4][0]            \n",
            "                                                                 skip_latent_mlp_5_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_4 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_5_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_4 (Add)       (None, 1024, 512)    0           latent_mlp_5_4[0][0]             \n",
            "                                                                 skip_latent_attn_5_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_5 (Add)      (None, 1024, 512)    0           latent_attn_1_5[4][0]            \n",
            "                                                                 skip_latent_mlp_5_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_5 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_5_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_5 (Add)       (None, 1024, 512)    0           latent_mlp_5_5[0][0]             \n",
            "                                                                 skip_latent_attn_5_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_6 (Add)         (None, 1024, 512)    0           cross_attn_1[5][0]               \n",
            "                                                                 skip_latent_mlp_5_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_6 (MLP)               (None, 1024, 512)    525824      skip_cross_attn_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_6 (Add)          (None, 1024, 512)    0           cross_mlp_6[0][0]                \n",
            "                                                                 skip_cross_attn_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_0 (Add)      (None, 1024, 512)    0           latent_attn_1_0[5][0]            \n",
            "                                                                 skip_cross_mlp_6[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_0 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_6_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_0 (Add)       (None, 1024, 512)    0           latent_mlp_6_0[0][0]             \n",
            "                                                                 skip_latent_attn_6_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_1 (Add)      (None, 1024, 512)    0           latent_attn_1_1[5][0]            \n",
            "                                                                 skip_latent_mlp_6_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_1 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_6_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_1 (Add)       (None, 1024, 512)    0           latent_mlp_6_1[0][0]             \n",
            "                                                                 skip_latent_attn_6_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_2 (Add)      (None, 1024, 512)    0           latent_attn_1_2[5][0]            \n",
            "                                                                 skip_latent_mlp_6_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_2 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_6_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_2 (Add)       (None, 1024, 512)    0           latent_mlp_6_2[0][0]             \n",
            "                                                                 skip_latent_attn_6_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_3 (Add)      (None, 1024, 512)    0           latent_attn_1_3[5][0]            \n",
            "                                                                 skip_latent_mlp_6_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_3 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_6_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_3 (Add)       (None, 1024, 512)    0           latent_mlp_6_3[0][0]             \n",
            "                                                                 skip_latent_attn_6_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_4 (Add)      (None, 1024, 512)    0           latent_attn_1_4[5][0]            \n",
            "                                                                 skip_latent_mlp_6_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_4 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_6_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_4 (Add)       (None, 1024, 512)    0           latent_mlp_6_4[0][0]             \n",
            "                                                                 skip_latent_attn_6_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_5 (Add)      (None, 1024, 512)    0           latent_attn_1_5[5][0]            \n",
            "                                                                 skip_latent_mlp_6_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_5 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_6_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_5 (Add)       (None, 1024, 512)    0           latent_mlp_6_5[0][0]             \n",
            "                                                                 skip_latent_attn_6_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_7 (Add)         (None, 1024, 512)    0           cross_attn_1[6][0]               \n",
            "                                                                 skip_latent_mlp_6_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_7 (MLP)               (None, 1024, 512)    525824      skip_cross_attn_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_7 (Add)          (None, 1024, 512)    0           cross_mlp_7[0][0]                \n",
            "                                                                 skip_cross_attn_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_0 (Add)      (None, 1024, 512)    0           latent_attn_1_0[6][0]            \n",
            "                                                                 skip_cross_mlp_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_0 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_7_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_0 (Add)       (None, 1024, 512)    0           latent_mlp_7_0[0][0]             \n",
            "                                                                 skip_latent_attn_7_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_1 (Add)      (None, 1024, 512)    0           latent_attn_1_1[6][0]            \n",
            "                                                                 skip_latent_mlp_7_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_1 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_7_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_1 (Add)       (None, 1024, 512)    0           latent_mlp_7_1[0][0]             \n",
            "                                                                 skip_latent_attn_7_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_2 (Add)      (None, 1024, 512)    0           latent_attn_1_2[6][0]            \n",
            "                                                                 skip_latent_mlp_7_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_2 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_7_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_2 (Add)       (None, 1024, 512)    0           latent_mlp_7_2[0][0]             \n",
            "                                                                 skip_latent_attn_7_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_3 (Add)      (None, 1024, 512)    0           latent_attn_1_3[6][0]            \n",
            "                                                                 skip_latent_mlp_7_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_3 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_7_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_3 (Add)       (None, 1024, 512)    0           latent_mlp_7_3[0][0]             \n",
            "                                                                 skip_latent_attn_7_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_4 (Add)      (None, 1024, 512)    0           latent_attn_1_4[6][0]            \n",
            "                                                                 skip_latent_mlp_7_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_4 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_7_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_4 (Add)       (None, 1024, 512)    0           latent_mlp_7_4[0][0]             \n",
            "                                                                 skip_latent_attn_7_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_5 (Add)      (None, 1024, 512)    0           latent_attn_1_5[6][0]            \n",
            "                                                                 skip_latent_mlp_7_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_5 (MLP)            (None, 1024, 512)    525824      skip_latent_attn_7_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_5 (Add)       (None, 1024, 512)    0           latent_mlp_7_5[0][0]             \n",
            "                                                                 skip_latent_attn_7_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 512)       0           skip_latent_mlp_7_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 1000)      513000      avgpool[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 44,671,996\n",
            "Trainable params: 44,671,996\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "470J_-chCJN-"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A3t5koq9X5x"
      },
      "source": [
        "Training configs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0rfLzW6CKvI"
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "acc_metric_fn = tf.keras.metrics.SparseCategoricalAccuracy\n",
        "batch_size = 256\n",
        "shuffle_buffer = 50000\n",
        "epochs = 50\n",
        "learning_rate = 1e-3\n",
        "\n",
        "num_classes = 10 \n",
        "image_height = 32\n",
        "image_width = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK_xHPHD81nx"
      },
      "source": [
        "Keras model.compile(...) and model.fit(...) .\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x83Opo2GCQWY"
      },
      "source": [
        "def train_and_eval(_model, checkpoint_dir=None, history_dir=None, verbose=1):\n",
        "  \"\"\"Wrapper code for training and evaluating.\n",
        "\n",
        "  Args:\n",
        "      _model: A keras Model.\n",
        "      checkpoint_dir: Path to save a checkpoint.\n",
        "      history_dir: Path to save a history.\n",
        "      verbose: Option for logging output during train and eval.\n",
        "\n",
        "  Returns:\n",
        "      A history instance that contains logged values per epoch.\n",
        "\n",
        "  \"\"\"\n",
        "  steps_per_epoch = len(data_generator('train', batch_size, shuffle_buffer))\n",
        "\n",
        "  _model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "    loss=loss_fn,\n",
        "    metrics=[acc_metric_fn(name='sparse_categorical_accuracy')]\n",
        "  )\n",
        "\n",
        "  if checkpoint_dir is not None:\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "      checkpoint_dir,\n",
        "      monitor=\"val_sparse_categorical_accuracy\",\n",
        "      save_best_only=True,\n",
        "    )\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=os.path.join(checkpoint_dir, 'logs'), \n",
        "      histogram_freq=epochs//10,\n",
        "      update_freq=(steps_per_epoch // batch_size +\n",
        "                   (steps_per_epoch % batch_size > 0)) * 5\n",
        "    )\n",
        "    callbacks=[tensorboard_callback, checkpoint_callback]\n",
        "  else:\n",
        "    callbacks = None\n",
        "\n",
        "  start = time()\n",
        "  history = _model.fit(\n",
        "      data_generator('train', batch_size, shuffle_buffer), \n",
        "      epochs=epochs, \n",
        "      steps_per_epoch=steps_per_epoch,\n",
        "      validation_data=data_generator('val', batch_size),\n",
        "      callbacks=callbacks,\n",
        "      verbose=verbose\n",
        "  )\n",
        "  end = time()\n",
        "  print(f'Total training time {end - start} seconds')\n",
        "  \n",
        "  if history_dir is not None:\n",
        "    # Save history output, should be the same as the tensorboard logs.\n",
        "    np.save(os.path.join(history_dir, 'history.npy'), history.history)\n",
        "  \n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n8bpH3YC3jZ"
      },
      "source": [
        "# Loss and accuracy plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYmEyrccDQBr"
      },
      "source": [
        "def plot(losses: list, \n",
        "         accuracies: list, \n",
        "         legend_labels: list, \n",
        "         subplot_title: list):\n",
        "  \n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,5))\n",
        "  \n",
        "  for x, ll in zip(losses, legend_labels):\n",
        "    c = ax1.plot(x[0], label='Trn: ' + ll, linestyle='--')[0].get_c()\n",
        "    _ = ax1.plot(x[1], label='Val: ' + ll, linestyle='-', color=c)\n",
        "  ax1.set_xlabel('Epochs')\n",
        "  ax1.set_ylabel('Loss')\n",
        "  ax1.set_title(subplot_title[0])\n",
        "  ax1.legend()   \n",
        "\n",
        "  for x, ll in zip(accuracies, legend_labels):\n",
        "    c = ax2.plot(x[0], label='Trn: ' + ll, linestyle='--')[0].get_c()\n",
        "    _ = ax2.plot(x[1], label='Val: ' + ll, linestyle='-', color=c)\n",
        "  ax2.set_xlabel('Epochs')\n",
        "  ax2.set_ylabel('Accuracy')\n",
        "  ax2.set_title(subplot_title[1])\n",
        "  ax2.legend()   \n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F1flqO1FDl1"
      },
      "source": [
        "#Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiQV1JV7Oqj2"
      },
      "source": [
        "history = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBFk329fU0D1"
      },
      "source": [
        "##Exp 1\n",
        "Model : CNN  \n",
        "Preprocessing : False  \n",
        "Batch size : 256\n",
        "\n",
        "---\n",
        "Trainable params: 890,410  \n",
        "Total training time 251.87630558013916 seconds\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujjSGxZifTLv"
      },
      "source": [
        "# model = SmallCNN(num_classes=num_classes, \n",
        "#                  image_height=image_height, \n",
        "#                  image_width=image_width,\n",
        "#                  preprocess=False).model()\n",
        "# model.summary()\n",
        "# history['CNN'] = train_and_eval(model, SAVE_PATH + '_CNN', verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XKFICEKCZxE"
      },
      "source": [
        "##Exp 2\n",
        "Model : VIT  \n",
        "Preprocessing : False  \n",
        "Batch size : 256  \n",
        "Patch : 4x4  \n",
        "Heads : 2\n",
        "\n",
        "---\n",
        "Trainable params: 1,108,842  \n",
        "Total training time 574.3961868286133 seconds\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lyrU-_sXgkU"
      },
      "source": [
        "# # Saves a full copy of the model.\n",
        "# model = VIT(num_classes=num_classes, \n",
        "#             image_height=image_height, \n",
        "#             image_width=image_width,\n",
        "#             patch_size=4, \n",
        "#             projection_dim=32,\n",
        "#             num_layers=4,\n",
        "#             num_heads=2,\n",
        "#             mlp_dims=[64, 32],\n",
        "#             classifier_mlp_dims=[512],\n",
        "#             preprocess=False).model()\n",
        "# model.summary()\n",
        "# history['VIT'] = train_and_eval(model, SAVE_PATH + '_VIT_4x4_2hds', verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzOlQHR-GYWP"
      },
      "source": [
        "##Exp 3\n",
        "Model : LAMBDA  \n",
        "Preprocessing : False  \n",
        "Batch size : 256  \n",
        "Key dim : 2  \n",
        "u dim : 8  \n",
        "Heads : 2  \n",
        "Pos. emb. size : 14  \n",
        "LambdaConv : False  \n",
        "\n",
        "---\n",
        "<pre>\n",
        "Q = k * hd       =   4\n",
        "K = k * u        =  16\n",
        "V = 64 // hd * u = 256\n",
        "</pre>\n",
        "\n",
        "---\n",
        "Trainable params: 1,245,258  \n",
        "Total training time 729.7639191150665 seconds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8yHokOJbjoc"
      },
      "source": [
        "# for hds in [2]:\n",
        "#   for u_dim in [8]:\n",
        "#     for k_dim in [2]:\n",
        "#       model = LambdaNetwork(num_classes=num_classes, \n",
        "#                             image_height=image_height, \n",
        "#                             image_width=image_width,\n",
        "#                             k_dim=k_dim,\n",
        "#                             u_dim=u_dim,\n",
        "#                             num_heads=hds,\n",
        "#                             n_r_size=14,\n",
        "#                             local_contexts=False,\n",
        "#                             preprocess=False).model()\n",
        "#       model.summary()\n",
        "#       if 'LAMBDA' not in history:\n",
        "#         history['LAMBDA'] = dict()\n",
        "#       if hds not in history['LAMBDA']:\n",
        "#         history['LAMBDA'][hds] = dict()\n",
        "#       if u_dim not in history['LAMBDA'][hds]:\n",
        "#         history['LAMBDA'][hds][u_dim] = dict()\n",
        "#       history['LAMBDA'][hds][u_dim][k_dim]= train_and_eval(model, SAVE_PATH + f'_LAMBDA_{k_dim}kdim_{u_dim}udim_{hds}hds', verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMF-f36r-T60"
      },
      "source": [
        "##Exp 1a-Freq\n",
        "Model : PERCEIVER  \n",
        "Preprocessing   : False  \n",
        "Batch size      : 256  \n",
        "max_freq        : 4  \n",
        "num_freq_bands  : 4  \n",
        "freq_base       : 2.0  \n",
        "num_latents     : 4  \n",
        "latent_dim      : 16  \n",
        "latent_heads    : 1  \n",
        "latent_head_dim : 16  \n",
        "cross_heads     : 1  \n",
        "cross_head_dim  : 16  \n",
        "cross_depth     : 2  \n",
        "latent_depth    : 4     \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzuAdRjSePn8",
        "outputId": "df3db6b8-830a-49c8-9b0e-36c2b428f370"
      },
      "source": [
        "model = Perceiver(num_classes=num_classes, \n",
        "                  image_height=image_height, \n",
        "                  image_width=image_width,\n",
        "                  max_freq=4,\n",
        "                  num_freq_bands=4,\n",
        "                  freq_base=2.0,\n",
        "                  num_latents=4,\n",
        "                  latent_dim=16,\n",
        "                  latent_heads=1,\n",
        "                  latent_head_dim=16,\n",
        "                  cross_heads=1,\n",
        "                  cross_head_dim=16,\n",
        "                  attn_dropout=0.2,\n",
        "                  mlp_dropout=0.2,\n",
        "                  cross_depth=2,\n",
        "                  latent_depth=4,             \n",
        "                  weight_tie_layers=True,\n",
        "                  preprocess=False).model()\n",
        "model.summary()\n",
        "train_and_eval(model, SAVE_PATH + '_PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld', verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 4, 16)        64          resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 21)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 4, 16)        1274        input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 4, 16)        0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 4, 16)        576         skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 4, 16)        0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 4, 16)        1072        skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 4, 16)        0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 4, 16)        576         skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 4, 16)        0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 4, 16)        0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 4, 16)        576         skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 4, 16)        0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 4, 16)        0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 4, 16)        576         skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 4, 16)        0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 4, 16)        0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 4, 16)        576         skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 4, 16)        0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 4, 16)        1274        skip_latent_mlp_0_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 4, 16)        0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 4, 16)        576         skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 4, 16)        0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 4, 16)        1072        skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 4, 16)        0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 4, 16)        576         skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 4, 16)        0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 4, 16)        0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 4, 16)        576         skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 4, 16)        0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 4, 16)        0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 4, 16)        576         skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 4, 16)        0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 4, 16)        0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 4, 16)        576         skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 4, 16)        0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 16)        0           skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 16)        32          avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        170         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 17,150\n",
            "Trainable params: 17,150\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 55s 66ms/step - loss: 2.3314 - sparse_categorical_accuracy: 0.1223 - val_loss: 2.0668 - val_sparse_categorical_accuracy: 0.2022\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 2.0719 - sparse_categorical_accuracy: 0.2113 - val_loss: 1.9792 - val_sparse_categorical_accuracy: 0.2507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.9894 - sparse_categorical_accuracy: 0.2519 - val_loss: 1.9186 - val_sparse_categorical_accuracy: 0.2871\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.9480 - sparse_categorical_accuracy: 0.2723 - val_loss: 1.8822 - val_sparse_categorical_accuracy: 0.3069\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.9191 - sparse_categorical_accuracy: 0.2928 - val_loss: 1.8574 - val_sparse_categorical_accuracy: 0.3215\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.8840 - sparse_categorical_accuracy: 0.3060 - val_loss: 1.8331 - val_sparse_categorical_accuracy: 0.3246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.8715 - sparse_categorical_accuracy: 0.3101 - val_loss: 1.7940 - val_sparse_categorical_accuracy: 0.3417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.8463 - sparse_categorical_accuracy: 0.3249 - val_loss: 1.7801 - val_sparse_categorical_accuracy: 0.3539\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.8347 - sparse_categorical_accuracy: 0.3326 - val_loss: 1.7632 - val_sparse_categorical_accuracy: 0.3599\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.8078 - sparse_categorical_accuracy: 0.3391 - val_loss: 1.7512 - val_sparse_categorical_accuracy: 0.3611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.8038 - sparse_categorical_accuracy: 0.3395 - val_loss: 1.7437 - val_sparse_categorical_accuracy: 0.3686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.7996 - sparse_categorical_accuracy: 0.3451 - val_loss: 1.7384 - val_sparse_categorical_accuracy: 0.3690\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.7808 - sparse_categorical_accuracy: 0.3514 - val_loss: 1.7293 - val_sparse_categorical_accuracy: 0.3748\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.7885 - sparse_categorical_accuracy: 0.3423 - val_loss: 1.7196 - val_sparse_categorical_accuracy: 0.3725\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.7749 - sparse_categorical_accuracy: 0.3559 - val_loss: 1.7176 - val_sparse_categorical_accuracy: 0.3757\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.7721 - sparse_categorical_accuracy: 0.3501 - val_loss: 1.7098 - val_sparse_categorical_accuracy: 0.3777\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.7619 - sparse_categorical_accuracy: 0.3554 - val_loss: 1.7154 - val_sparse_categorical_accuracy: 0.3809\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.7698 - sparse_categorical_accuracy: 0.3557 - val_loss: 1.6954 - val_sparse_categorical_accuracy: 0.3846\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.7570 - sparse_categorical_accuracy: 0.3593 - val_loss: 1.6884 - val_sparse_categorical_accuracy: 0.3847\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.7449 - sparse_categorical_accuracy: 0.3591 - val_loss: 1.6946 - val_sparse_categorical_accuracy: 0.3831\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.7414 - sparse_categorical_accuracy: 0.3618 - val_loss: 1.6766 - val_sparse_categorical_accuracy: 0.3919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.7422 - sparse_categorical_accuracy: 0.3630 - val_loss: 1.6780 - val_sparse_categorical_accuracy: 0.3926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.7384 - sparse_categorical_accuracy: 0.3648 - val_loss: 1.6946 - val_sparse_categorical_accuracy: 0.3837\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.7297 - sparse_categorical_accuracy: 0.3679 - val_loss: 1.6615 - val_sparse_categorical_accuracy: 0.3982\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.7206 - sparse_categorical_accuracy: 0.3727 - val_loss: 1.6728 - val_sparse_categorical_accuracy: 0.3946\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.7238 - sparse_categorical_accuracy: 0.3680 - val_loss: 1.6585 - val_sparse_categorical_accuracy: 0.3949\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.7123 - sparse_categorical_accuracy: 0.3742 - val_loss: 1.6486 - val_sparse_categorical_accuracy: 0.4012\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.7090 - sparse_categorical_accuracy: 0.3790 - val_loss: 1.6362 - val_sparse_categorical_accuracy: 0.4079\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.7014 - sparse_categorical_accuracy: 0.3783 - val_loss: 1.6258 - val_sparse_categorical_accuracy: 0.4081\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6886 - sparse_categorical_accuracy: 0.3820 - val_loss: 1.6310 - val_sparse_categorical_accuracy: 0.4102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6915 - sparse_categorical_accuracy: 0.3834 - val_loss: 1.6167 - val_sparse_categorical_accuracy: 0.4118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6911 - sparse_categorical_accuracy: 0.3859 - val_loss: 1.6120 - val_sparse_categorical_accuracy: 0.4205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.6810 - sparse_categorical_accuracy: 0.3872 - val_loss: 1.6053 - val_sparse_categorical_accuracy: 0.4173\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6721 - sparse_categorical_accuracy: 0.3921 - val_loss: 1.6125 - val_sparse_categorical_accuracy: 0.4146\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6649 - sparse_categorical_accuracy: 0.3923 - val_loss: 1.6038 - val_sparse_categorical_accuracy: 0.4171\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.6718 - sparse_categorical_accuracy: 0.3887 - val_loss: 1.5886 - val_sparse_categorical_accuracy: 0.4274\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6623 - sparse_categorical_accuracy: 0.3959 - val_loss: 1.5856 - val_sparse_categorical_accuracy: 0.4290\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6473 - sparse_categorical_accuracy: 0.3989 - val_loss: 1.5875 - val_sparse_categorical_accuracy: 0.4250\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6548 - sparse_categorical_accuracy: 0.3986 - val_loss: 1.5798 - val_sparse_categorical_accuracy: 0.4252\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6530 - sparse_categorical_accuracy: 0.3962 - val_loss: 1.5783 - val_sparse_categorical_accuracy: 0.4261\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6462 - sparse_categorical_accuracy: 0.4036 - val_loss: 1.5804 - val_sparse_categorical_accuracy: 0.4240\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6427 - sparse_categorical_accuracy: 0.4037 - val_loss: 1.5736 - val_sparse_categorical_accuracy: 0.4292\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6407 - sparse_categorical_accuracy: 0.4029 - val_loss: 1.5868 - val_sparse_categorical_accuracy: 0.4252\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6426 - sparse_categorical_accuracy: 0.4061 - val_loss: 1.5773 - val_sparse_categorical_accuracy: 0.4280\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6314 - sparse_categorical_accuracy: 0.4090 - val_loss: 1.5769 - val_sparse_categorical_accuracy: 0.4285\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6333 - sparse_categorical_accuracy: 0.4057 - val_loss: 1.5612 - val_sparse_categorical_accuracy: 0.4346\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6393 - sparse_categorical_accuracy: 0.4056 - val_loss: 1.5573 - val_sparse_categorical_accuracy: 0.4359\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 48/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6314 - sparse_categorical_accuracy: 0.4071 - val_loss: 1.5661 - val_sparse_categorical_accuracy: 0.4330\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6291 - sparse_categorical_accuracy: 0.4083 - val_loss: 1.5508 - val_sparse_categorical_accuracy: 0.4398\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6150 - sparse_categorical_accuracy: 0.4115 - val_loss: 1.5510 - val_sparse_categorical_accuracy: 0.4434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total training time 1270.7488918304443 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f98cfc30450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzT41loV-4d6"
      },
      "source": [
        "##Exp 1b-Freq\n",
        "Model : PERCEIVER  \n",
        "Preprocessing   : False  \n",
        "Batch size      : 256  \n",
        "max_freq        : 8  \n",
        "num_freq_bands  : 8  \n",
        "freq_base       : 2.0  \n",
        "num_latents     : 4  \n",
        "latent_dim      : 16  \n",
        "latent_heads    : 1  \n",
        "latent_head_dim : 16  \n",
        "cross_heads     : 1  \n",
        "cross_head_dim  : 16  \n",
        "cross_depth     : 2  \n",
        "latent_depth    : 4     \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMM0Bx_x1C_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d87874-5765-4bbf-f74e-3d42aa94e156"
      },
      "source": [
        "model = Perceiver(num_classes=num_classes, \n",
        "                  image_height=image_height, \n",
        "                  image_width=image_width,\n",
        "                  max_freq=8,\n",
        "                  num_freq_bands=8,\n",
        "                  freq_base=2.0,\n",
        "                  num_latents=4,\n",
        "                  latent_dim=16,\n",
        "                  latent_heads=1,\n",
        "                  latent_head_dim=16,\n",
        "                  cross_heads=1,\n",
        "                  cross_head_dim=16,\n",
        "                  attn_dropout=0.2,\n",
        "                  mlp_dropout=0.2,\n",
        "                  cross_depth=2,\n",
        "                  latent_depth=4,             \n",
        "                  weight_tie_layers=True,\n",
        "                  preprocess=False).model()\n",
        "model.summary()\n",
        "train_and_eval(model, SAVE_PATH + '_PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld', verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 4, 16)        64          resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 37)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 4, 16)        1818        input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 4, 16)        0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 4, 16)        576         skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 4, 16)        0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 4, 16)        1072        skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 4, 16)        0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 4, 16)        576         skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 4, 16)        0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 4, 16)        0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 4, 16)        576         skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 4, 16)        0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 4, 16)        0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 4, 16)        576         skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 4, 16)        0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 4, 16)        0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 4, 16)        576         skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 4, 16)        0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 4, 16)        1818        skip_latent_mlp_0_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 4, 16)        0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 4, 16)        576         skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 4, 16)        0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 4, 16)        1072        skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 4, 16)        0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 4, 16)        576         skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 4, 16)        0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 4, 16)        0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 4, 16)        576         skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 4, 16)        0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 4, 16)        0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 4, 16)        576         skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 4, 16)        0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 4, 16)        0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 4, 16)        576         skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 4, 16)        0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 16)        0           skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 16)        32          avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        170         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 18,238\n",
            "Trainable params: 18,238\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 26s 74ms/step - loss: 2.4159 - sparse_categorical_accuracy: 0.1078 - val_loss: 2.2969 - val_sparse_categorical_accuracy: 0.1094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 2.2534 - sparse_categorical_accuracy: 0.1425 - val_loss: 2.0181 - val_sparse_categorical_accuracy: 0.2602\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 2.0281 - sparse_categorical_accuracy: 0.2438 - val_loss: 1.9423 - val_sparse_categorical_accuracy: 0.2909\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.9544 - sparse_categorical_accuracy: 0.2779 - val_loss: 1.8909 - val_sparse_categorical_accuracy: 0.3063\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.9214 - sparse_categorical_accuracy: 0.2898 - val_loss: 1.8659 - val_sparse_categorical_accuracy: 0.3170\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.8926 - sparse_categorical_accuracy: 0.3047 - val_loss: 1.8394 - val_sparse_categorical_accuracy: 0.3273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.8838 - sparse_categorical_accuracy: 0.3137 - val_loss: 1.8157 - val_sparse_categorical_accuracy: 0.3428\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.8579 - sparse_categorical_accuracy: 0.3228 - val_loss: 1.7940 - val_sparse_categorical_accuracy: 0.3515\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.8424 - sparse_categorical_accuracy: 0.3316 - val_loss: 1.7725 - val_sparse_categorical_accuracy: 0.3591\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 [==============================] - 10s 52ms/step - loss: 1.8231 - sparse_categorical_accuracy: 0.3394 - val_loss: 1.7595 - val_sparse_categorical_accuracy: 0.3611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7975 - sparse_categorical_accuracy: 0.3477 - val_loss: 1.7391 - val_sparse_categorical_accuracy: 0.3736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7869 - sparse_categorical_accuracy: 0.3521 - val_loss: 1.7007 - val_sparse_categorical_accuracy: 0.3858\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7623 - sparse_categorical_accuracy: 0.3648 - val_loss: 1.7082 - val_sparse_categorical_accuracy: 0.3855\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7504 - sparse_categorical_accuracy: 0.3686 - val_loss: 1.6926 - val_sparse_categorical_accuracy: 0.3896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7429 - sparse_categorical_accuracy: 0.3682 - val_loss: 1.6608 - val_sparse_categorical_accuracy: 0.4018\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 [==============================] - 10s 52ms/step - loss: 1.7193 - sparse_categorical_accuracy: 0.3775 - val_loss: 1.6599 - val_sparse_categorical_accuracy: 0.4038\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 [==============================] - 10s 52ms/step - loss: 1.7105 - sparse_categorical_accuracy: 0.3798 - val_loss: 1.6529 - val_sparse_categorical_accuracy: 0.4039\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7087 - sparse_categorical_accuracy: 0.3794 - val_loss: 1.6309 - val_sparse_categorical_accuracy: 0.4106\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6910 - sparse_categorical_accuracy: 0.3885 - val_loss: 1.6241 - val_sparse_categorical_accuracy: 0.4127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 [==============================] - 10s 52ms/step - loss: 1.6852 - sparse_categorical_accuracy: 0.3872 - val_loss: 1.6245 - val_sparse_categorical_accuracy: 0.4102\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6817 - sparse_categorical_accuracy: 0.3930 - val_loss: 1.6055 - val_sparse_categorical_accuracy: 0.4177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6748 - sparse_categorical_accuracy: 0.3938 - val_loss: 1.6096 - val_sparse_categorical_accuracy: 0.4176\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6690 - sparse_categorical_accuracy: 0.3984 - val_loss: 1.5968 - val_sparse_categorical_accuracy: 0.4235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6610 - sparse_categorical_accuracy: 0.3993 - val_loss: 1.5859 - val_sparse_categorical_accuracy: 0.4256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 [==============================] - 10s 52ms/step - loss: 1.6492 - sparse_categorical_accuracy: 0.4046 - val_loss: 1.5761 - val_sparse_categorical_accuracy: 0.4261\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26/50\n",
            "196/196 [==============================] - 10s 53ms/step - loss: 1.6506 - sparse_categorical_accuracy: 0.4053 - val_loss: 1.5669 - val_sparse_categorical_accuracy: 0.4312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6363 - sparse_categorical_accuracy: 0.4093 - val_loss: 1.5812 - val_sparse_categorical_accuracy: 0.4227\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 10s 52ms/step - loss: 1.6324 - sparse_categorical_accuracy: 0.4118 - val_loss: 1.5579 - val_sparse_categorical_accuracy: 0.4362\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6251 - sparse_categorical_accuracy: 0.4120 - val_loss: 1.5680 - val_sparse_categorical_accuracy: 0.4348\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6175 - sparse_categorical_accuracy: 0.4120 - val_loss: 1.5600 - val_sparse_categorical_accuracy: 0.4373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6179 - sparse_categorical_accuracy: 0.4134 - val_loss: 1.5611 - val_sparse_categorical_accuracy: 0.4373\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6089 - sparse_categorical_accuracy: 0.4212 - val_loss: 1.5513 - val_sparse_categorical_accuracy: 0.4342\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 10s 52ms/step - loss: 1.6071 - sparse_categorical_accuracy: 0.4190 - val_loss: 1.5479 - val_sparse_categorical_accuracy: 0.4349\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.5990 - sparse_categorical_accuracy: 0.4199 - val_loss: 1.5482 - val_sparse_categorical_accuracy: 0.4363\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6084 - sparse_categorical_accuracy: 0.4195 - val_loss: 1.5203 - val_sparse_categorical_accuracy: 0.4516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.5896 - sparse_categorical_accuracy: 0.4248 - val_loss: 1.5257 - val_sparse_categorical_accuracy: 0.4550\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.5879 - sparse_categorical_accuracy: 0.4274 - val_loss: 1.5188 - val_sparse_categorical_accuracy: 0.4513\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.5882 - sparse_categorical_accuracy: 0.4256 - val_loss: 1.5111 - val_sparse_categorical_accuracy: 0.4515\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.5819 - sparse_categorical_accuracy: 0.4248 - val_loss: 1.5147 - val_sparse_categorical_accuracy: 0.4493\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.5731 - sparse_categorical_accuracy: 0.4315 - val_loss: 1.5242 - val_sparse_categorical_accuracy: 0.4482\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.5643 - sparse_categorical_accuracy: 0.4348 - val_loss: 1.5169 - val_sparse_categorical_accuracy: 0.4541\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.5720 - sparse_categorical_accuracy: 0.4293 - val_loss: 1.5090 - val_sparse_categorical_accuracy: 0.4547\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.5658 - sparse_categorical_accuracy: 0.4345 - val_loss: 1.5138 - val_sparse_categorical_accuracy: 0.4476\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.5691 - sparse_categorical_accuracy: 0.4302 - val_loss: 1.5162 - val_sparse_categorical_accuracy: 0.4514\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.5566 - sparse_categorical_accuracy: 0.4375 - val_loss: 1.5209 - val_sparse_categorical_accuracy: 0.4478\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.5580 - sparse_categorical_accuracy: 0.4379 - val_loss: 1.4884 - val_sparse_categorical_accuracy: 0.4615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.5468 - sparse_categorical_accuracy: 0.4438 - val_loss: 1.4936 - val_sparse_categorical_accuracy: 0.4638\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 48/50\n",
            "196/196 [==============================] - 10s 52ms/step - loss: 1.5570 - sparse_categorical_accuracy: 0.4387 - val_loss: 1.4939 - val_sparse_categorical_accuracy: 0.4583\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.5500 - sparse_categorical_accuracy: 0.4419 - val_loss: 1.4821 - val_sparse_categorical_accuracy: 0.4673\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.5402 - sparse_categorical_accuracy: 0.4453 - val_loss: 1.4930 - val_sparse_categorical_accuracy: 0.4589\n",
            "Total training time 1210.9064228534698 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f97970c1cd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mkOhwyU-_k-"
      },
      "source": [
        "##Exp 1c-Freq\n",
        "Model : PERCEIVER  \n",
        "Preprocessing   : False  \n",
        "Batch size      : 256  \n",
        "max_freq        : 4  \n",
        "num_freq_bands  : 8  \n",
        "freq_base       : 2.0  \n",
        "num_latents     : 4  \n",
        "latent_dim      : 16  \n",
        "latent_heads    : 1  \n",
        "latent_head_dim : 16  \n",
        "cross_heads     : 1  \n",
        "cross_head_dim  : 16  \n",
        "cross_depth     : 2  \n",
        "latent_depth    : 4     \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJs8x_WF1Hb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d50047f-6dd0-4280-c69d-5fa25c8a6ec7"
      },
      "source": [
        "model = Perceiver(num_classes=num_classes, \n",
        "                  image_height=image_height, \n",
        "                  image_width=image_width,\n",
        "                  max_freq=4,\n",
        "                  num_freq_bands=8,\n",
        "                  freq_base=2.0,\n",
        "                  num_latents=4,\n",
        "                  latent_dim=16,\n",
        "                  latent_heads=1,\n",
        "                  latent_head_dim=16,\n",
        "                  cross_heads=1,\n",
        "                  cross_head_dim=16,\n",
        "                  attn_dropout=0.2,\n",
        "                  mlp_dropout=0.2,\n",
        "                  cross_depth=2,\n",
        "                  latent_depth=4,             \n",
        "                  weight_tie_layers=True,\n",
        "                  preprocess=False).model()\n",
        "model.summary()\n",
        "train_and_eval(model, SAVE_PATH + '_PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld', verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 4, 16)        64          resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 37)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 4, 16)        1818        input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 4, 16)        0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 4, 16)        576         skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 4, 16)        0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 4, 16)        1072        skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 4, 16)        0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 4, 16)        576         skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 4, 16)        0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 4, 16)        0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 4, 16)        576         skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 4, 16)        0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 4, 16)        0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 4, 16)        576         skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 4, 16)        0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 4, 16)        0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 4, 16)        576         skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 4, 16)        0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 4, 16)        1818        skip_latent_mlp_0_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 4, 16)        0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 4, 16)        576         skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 4, 16)        0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 4, 16)        1072        skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 4, 16)        0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 4, 16)        576         skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 4, 16)        0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 4, 16)        0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 4, 16)        576         skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 4, 16)        0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 4, 16)        0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 4, 16)        576         skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 4, 16)        0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 4, 16)        1072        skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 4, 16)        0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 4, 16)        576         skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 4, 16)        0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 16)        0           skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 16)        32          avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        170         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 18,238\n",
            "Trainable params: 18,238\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 26s 74ms/step - loss: 2.3502 - sparse_categorical_accuracy: 0.1094 - val_loss: 2.1217 - val_sparse_categorical_accuracy: 0.2054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 [==============================] - 10s 50ms/step - loss: 2.1240 - sparse_categorical_accuracy: 0.1953 - val_loss: 2.0579 - val_sparse_categorical_accuracy: 0.2227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 [==============================] - 10s 50ms/step - loss: 2.0829 - sparse_categorical_accuracy: 0.2090 - val_loss: 2.0211 - val_sparse_categorical_accuracy: 0.2336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 2.0357 - sparse_categorical_accuracy: 0.2337 - val_loss: 1.9377 - val_sparse_categorical_accuracy: 0.2797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.9768 - sparse_categorical_accuracy: 0.2674 - val_loss: 1.8952 - val_sparse_categorical_accuracy: 0.2994\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.9377 - sparse_categorical_accuracy: 0.2825 - val_loss: 1.8722 - val_sparse_categorical_accuracy: 0.3069\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 [==============================] - 10s 50ms/step - loss: 1.9154 - sparse_categorical_accuracy: 0.2943 - val_loss: 1.8421 - val_sparse_categorical_accuracy: 0.3274\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.8954 - sparse_categorical_accuracy: 0.3023 - val_loss: 1.8652 - val_sparse_categorical_accuracy: 0.3196\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 10s 50ms/step - loss: 1.8772 - sparse_categorical_accuracy: 0.3095 - val_loss: 1.8015 - val_sparse_categorical_accuracy: 0.3406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.8536 - sparse_categorical_accuracy: 0.3236 - val_loss: 1.7829 - val_sparse_categorical_accuracy: 0.3480\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.8448 - sparse_categorical_accuracy: 0.3271 - val_loss: 1.7780 - val_sparse_categorical_accuracy: 0.3524\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.8210 - sparse_categorical_accuracy: 0.3366 - val_loss: 1.7501 - val_sparse_categorical_accuracy: 0.3613\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.8124 - sparse_categorical_accuracy: 0.3410 - val_loss: 1.7518 - val_sparse_categorical_accuracy: 0.3632\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7996 - sparse_categorical_accuracy: 0.3479 - val_loss: 1.7435 - val_sparse_categorical_accuracy: 0.3625\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.8045 - sparse_categorical_accuracy: 0.3466 - val_loss: 1.7373 - val_sparse_categorical_accuracy: 0.3676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7836 - sparse_categorical_accuracy: 0.3504 - val_loss: 1.7155 - val_sparse_categorical_accuracy: 0.3757\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7801 - sparse_categorical_accuracy: 0.3505 - val_loss: 1.7092 - val_sparse_categorical_accuracy: 0.3801\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7736 - sparse_categorical_accuracy: 0.3560 - val_loss: 1.7158 - val_sparse_categorical_accuracy: 0.3808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7681 - sparse_categorical_accuracy: 0.3581 - val_loss: 1.6949 - val_sparse_categorical_accuracy: 0.3859\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7642 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.6990 - val_sparse_categorical_accuracy: 0.3805\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7609 - sparse_categorical_accuracy: 0.3582 - val_loss: 1.6840 - val_sparse_categorical_accuracy: 0.3900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7556 - sparse_categorical_accuracy: 0.3616 - val_loss: 1.6782 - val_sparse_categorical_accuracy: 0.3923\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7427 - sparse_categorical_accuracy: 0.3725 - val_loss: 1.6675 - val_sparse_categorical_accuracy: 0.3944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7351 - sparse_categorical_accuracy: 0.3720 - val_loss: 1.6636 - val_sparse_categorical_accuracy: 0.3943\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7398 - sparse_categorical_accuracy: 0.3689 - val_loss: 1.6621 - val_sparse_categorical_accuracy: 0.3956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7297 - sparse_categorical_accuracy: 0.3721 - val_loss: 1.6563 - val_sparse_categorical_accuracy: 0.4005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7253 - sparse_categorical_accuracy: 0.3725 - val_loss: 1.6521 - val_sparse_categorical_accuracy: 0.4005\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7197 - sparse_categorical_accuracy: 0.3741 - val_loss: 1.6524 - val_sparse_categorical_accuracy: 0.3992\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 10s 50ms/step - loss: 1.7095 - sparse_categorical_accuracy: 0.3804 - val_loss: 1.6794 - val_sparse_categorical_accuracy: 0.3887\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 10s 50ms/step - loss: 1.7154 - sparse_categorical_accuracy: 0.3795 - val_loss: 1.6532 - val_sparse_categorical_accuracy: 0.3983\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.7063 - sparse_categorical_accuracy: 0.3849 - val_loss: 1.6432 - val_sparse_categorical_accuracy: 0.4054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32/50\n",
            "196/196 [==============================] - 10s 52ms/step - loss: 1.7077 - sparse_categorical_accuracy: 0.3806 - val_loss: 1.6609 - val_sparse_categorical_accuracy: 0.3974\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6991 - sparse_categorical_accuracy: 0.3837 - val_loss: 1.6262 - val_sparse_categorical_accuracy: 0.4116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6933 - sparse_categorical_accuracy: 0.3882 - val_loss: 1.6249 - val_sparse_categorical_accuracy: 0.4156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6927 - sparse_categorical_accuracy: 0.3885 - val_loss: 1.6182 - val_sparse_categorical_accuracy: 0.4152\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6938 - sparse_categorical_accuracy: 0.3841 - val_loss: 1.6266 - val_sparse_categorical_accuracy: 0.4102\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 10s 52ms/step - loss: 1.6858 - sparse_categorical_accuracy: 0.3881 - val_loss: 1.6121 - val_sparse_categorical_accuracy: 0.4159\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6817 - sparse_categorical_accuracy: 0.3891 - val_loss: 1.6220 - val_sparse_categorical_accuracy: 0.4112\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 10s 50ms/step - loss: 1.6744 - sparse_categorical_accuracy: 0.3918 - val_loss: 1.6010 - val_sparse_categorical_accuracy: 0.4194\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6727 - sparse_categorical_accuracy: 0.3933 - val_loss: 1.6076 - val_sparse_categorical_accuracy: 0.4184\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6637 - sparse_categorical_accuracy: 0.3979 - val_loss: 1.6033 - val_sparse_categorical_accuracy: 0.4183\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6694 - sparse_categorical_accuracy: 0.3954 - val_loss: 1.6020 - val_sparse_categorical_accuracy: 0.4202\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6693 - sparse_categorical_accuracy: 0.3942 - val_loss: 1.5982 - val_sparse_categorical_accuracy: 0.4192\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6587 - sparse_categorical_accuracy: 0.3969 - val_loss: 1.5968 - val_sparse_categorical_accuracy: 0.4204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 45/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6652 - sparse_categorical_accuracy: 0.3942 - val_loss: 1.5911 - val_sparse_categorical_accuracy: 0.4207\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 46/50\n",
            "196/196 [==============================] - 10s 51ms/step - loss: 1.6533 - sparse_categorical_accuracy: 0.3983 - val_loss: 1.5882 - val_sparse_categorical_accuracy: 0.4280\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47/50\n",
            "196/196 [==============================] - 10s 52ms/step - loss: 1.6597 - sparse_categorical_accuracy: 0.4022 - val_loss: 1.5864 - val_sparse_categorical_accuracy: 0.4271\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 10s 52ms/step - loss: 1.6490 - sparse_categorical_accuracy: 0.4015 - val_loss: 1.5920 - val_sparse_categorical_accuracy: 0.4208\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 10s 52ms/step - loss: 1.6466 - sparse_categorical_accuracy: 0.4034 - val_loss: 1.5868 - val_sparse_categorical_accuracy: 0.4231\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 10s 52ms/step - loss: 1.6481 - sparse_categorical_accuracy: 0.4019 - val_loss: 1.5827 - val_sparse_categorical_accuracy: 0.4291\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_8fqb_4l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total training time 1261.4271109104156 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f983a585310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9a4Hw1e_TP6"
      },
      "source": [
        "##Exp 2-latent\n",
        "Model : PERCEIVER  \n",
        "Preprocessing   : False  \n",
        "Batch size      : 256  \n",
        "max_freq        : 4  \n",
        "num_freq_bands  : 4  \n",
        "freq_base       : 2.0  \n",
        "num_latents     : 8,16  \n",
        "latent_dim      : 16  \n",
        "latent_heads    : 1  \n",
        "latent_head_dim : 16  \n",
        "cross_heads     : 1  \n",
        "cross_head_dim  : 16  \n",
        "cross_depth     : 2  \n",
        "latent_depth    : 4     \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSa_sucq_jmM",
        "outputId": "0e49e2f8-3cef-4f79-d210-1113a72e02f1"
      },
      "source": [
        "for num_latents in [8, 16]:\n",
        "  model = Perceiver(num_classes=num_classes, \n",
        "                    image_height=image_height, \n",
        "                    image_width=image_width,\n",
        "                    max_freq=4,\n",
        "                    num_freq_bands=4,\n",
        "                    freq_base=2.0,\n",
        "                    num_latents=num_latents,\n",
        "                    latent_dim=16,\n",
        "                    latent_heads=1,\n",
        "                    latent_head_dim=16,\n",
        "                    cross_heads=1,\n",
        "                    cross_head_dim=16,\n",
        "                    attn_dropout=0.2,\n",
        "                    mlp_dropout=0.2,\n",
        "                    cross_depth=2,\n",
        "                    latent_depth=4,             \n",
        "                    weight_tie_layers=True,\n",
        "                    preprocess=False).model()\n",
        "  model.summary()\n",
        "  train_and_eval(model, SAVE_PATH + f'_PERCEIVER_4fq_4fqb_{num_latents}l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld', verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 8, 16)        128         resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 21)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 8, 16)        1274        input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 8, 16)        0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 8, 16)        576         skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 8, 16)        0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 8, 16)        1072        skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 8, 16)        0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 8, 16)        576         skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 8, 16)        0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 8, 16)        1072        skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 8, 16)        0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 8, 16)        576         skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 8, 16)        0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 8, 16)        1072        skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 8, 16)        0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 8, 16)        576         skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 8, 16)        0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 8, 16)        1072        skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 8, 16)        0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 8, 16)        576         skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 8, 16)        0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 8, 16)        1274        skip_latent_mlp_0_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 8, 16)        0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 8, 16)        576         skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 8, 16)        0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 8, 16)        1072        skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 8, 16)        0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 8, 16)        576         skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 8, 16)        0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 8, 16)        1072        skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 8, 16)        0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 8, 16)        576         skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 8, 16)        0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 8, 16)        1072        skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 8, 16)        0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 8, 16)        576         skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 8, 16)        0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 8, 16)        1072        skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 8, 16)        0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 8, 16)        576         skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 8, 16)        0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 16)        0           skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 16)        32          avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        170         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 17,214\n",
            "Trainable params: 17,214\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 27s 71ms/step - loss: 2.3120 - sparse_categorical_accuracy: 0.1406 - val_loss: 2.0706 - val_sparse_categorical_accuracy: 0.2045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 [==============================] - 9s 48ms/step - loss: 2.0825 - sparse_categorical_accuracy: 0.2030 - val_loss: 1.9958 - val_sparse_categorical_accuracy: 0.2576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 [==============================] - 9s 48ms/step - loss: 2.0139 - sparse_categorical_accuracy: 0.2447 - val_loss: 1.9257 - val_sparse_categorical_accuracy: 0.2838\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.9619 - sparse_categorical_accuracy: 0.2726 - val_loss: 1.9317 - val_sparse_categorical_accuracy: 0.2905\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.9408 - sparse_categorical_accuracy: 0.2822 - val_loss: 1.8987 - val_sparse_categorical_accuracy: 0.2991\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.9376 - sparse_categorical_accuracy: 0.2844 - val_loss: 1.8833 - val_sparse_categorical_accuracy: 0.3059\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.9188 - sparse_categorical_accuracy: 0.2940 - val_loss: 1.8739 - val_sparse_categorical_accuracy: 0.3201\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.9055 - sparse_categorical_accuracy: 0.3011 - val_loss: 1.8494 - val_sparse_categorical_accuracy: 0.3177\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.8990 - sparse_categorical_accuracy: 0.3045 - val_loss: 1.8375 - val_sparse_categorical_accuracy: 0.3294\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.8754 - sparse_categorical_accuracy: 0.3193 - val_loss: 1.8134 - val_sparse_categorical_accuracy: 0.3411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.8619 - sparse_categorical_accuracy: 0.3252 - val_loss: 1.8258 - val_sparse_categorical_accuracy: 0.3358\n",
            "Epoch 12/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.8477 - sparse_categorical_accuracy: 0.3268 - val_loss: 1.8022 - val_sparse_categorical_accuracy: 0.3445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 [==============================] - 9s 48ms/step - loss: 1.8305 - sparse_categorical_accuracy: 0.3352 - val_loss: 1.7813 - val_sparse_categorical_accuracy: 0.3511\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.8302 - sparse_categorical_accuracy: 0.3367 - val_loss: 1.7735 - val_sparse_categorical_accuracy: 0.3552\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.8058 - sparse_categorical_accuracy: 0.3486 - val_loss: 1.7653 - val_sparse_categorical_accuracy: 0.3614\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.8013 - sparse_categorical_accuracy: 0.3460 - val_loss: 1.7782 - val_sparse_categorical_accuracy: 0.3509\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.7856 - sparse_categorical_accuracy: 0.3521 - val_loss: 1.7593 - val_sparse_categorical_accuracy: 0.3599\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.7787 - sparse_categorical_accuracy: 0.3553 - val_loss: 1.7259 - val_sparse_categorical_accuracy: 0.3747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.7642 - sparse_categorical_accuracy: 0.3609 - val_loss: 1.7144 - val_sparse_categorical_accuracy: 0.3758\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.7606 - sparse_categorical_accuracy: 0.3590 - val_loss: 1.7196 - val_sparse_categorical_accuracy: 0.3793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.7475 - sparse_categorical_accuracy: 0.3659 - val_loss: 1.6924 - val_sparse_categorical_accuracy: 0.3867\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.7439 - sparse_categorical_accuracy: 0.3697 - val_loss: 1.6835 - val_sparse_categorical_accuracy: 0.3910\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.7350 - sparse_categorical_accuracy: 0.3739 - val_loss: 1.6833 - val_sparse_categorical_accuracy: 0.3922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.7206 - sparse_categorical_accuracy: 0.3770 - val_loss: 1.6794 - val_sparse_categorical_accuracy: 0.3944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.7147 - sparse_categorical_accuracy: 0.3788 - val_loss: 1.6610 - val_sparse_categorical_accuracy: 0.3957\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.7163 - sparse_categorical_accuracy: 0.3772 - val_loss: 1.6630 - val_sparse_categorical_accuracy: 0.4011\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.7091 - sparse_categorical_accuracy: 0.3823 - val_loss: 1.6557 - val_sparse_categorical_accuracy: 0.4050\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6909 - sparse_categorical_accuracy: 0.3878 - val_loss: 1.6474 - val_sparse_categorical_accuracy: 0.4034\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6911 - sparse_categorical_accuracy: 0.3870 - val_loss: 1.6436 - val_sparse_categorical_accuracy: 0.4058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n",
            "196/196 [==============================] - 9s 48ms/step - loss: 1.6827 - sparse_categorical_accuracy: 0.3916 - val_loss: 1.6488 - val_sparse_categorical_accuracy: 0.4043\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6792 - sparse_categorical_accuracy: 0.3918 - val_loss: 1.6411 - val_sparse_categorical_accuracy: 0.4058\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6792 - sparse_categorical_accuracy: 0.3919 - val_loss: 1.6432 - val_sparse_categorical_accuracy: 0.4047\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6738 - sparse_categorical_accuracy: 0.3930 - val_loss: 1.6399 - val_sparse_categorical_accuracy: 0.4052\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 9s 48ms/step - loss: 1.6722 - sparse_categorical_accuracy: 0.3958 - val_loss: 1.6243 - val_sparse_categorical_accuracy: 0.4149\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6606 - sparse_categorical_accuracy: 0.4001 - val_loss: 1.6166 - val_sparse_categorical_accuracy: 0.4123\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6610 - sparse_categorical_accuracy: 0.3995 - val_loss: 1.6321 - val_sparse_categorical_accuracy: 0.4077\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6555 - sparse_categorical_accuracy: 0.4015 - val_loss: 1.6191 - val_sparse_categorical_accuracy: 0.4163\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 [==============================] - 9s 48ms/step - loss: 1.6474 - sparse_categorical_accuracy: 0.4036 - val_loss: 1.6111 - val_sparse_categorical_accuracy: 0.4199\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6388 - sparse_categorical_accuracy: 0.4111 - val_loss: 1.5997 - val_sparse_categorical_accuracy: 0.4228\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6408 - sparse_categorical_accuracy: 0.4104 - val_loss: 1.5944 - val_sparse_categorical_accuracy: 0.4270\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 41/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6413 - sparse_categorical_accuracy: 0.4073 - val_loss: 1.5899 - val_sparse_categorical_accuracy: 0.4293\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 42/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6257 - sparse_categorical_accuracy: 0.4112 - val_loss: 1.5929 - val_sparse_categorical_accuracy: 0.4250\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6269 - sparse_categorical_accuracy: 0.4133 - val_loss: 1.5857 - val_sparse_categorical_accuracy: 0.4288\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6318 - sparse_categorical_accuracy: 0.4099 - val_loss: 1.5810 - val_sparse_categorical_accuracy: 0.4303\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 45/50\n",
            "196/196 [==============================] - 10s 48ms/step - loss: 1.6157 - sparse_categorical_accuracy: 0.4197 - val_loss: 1.5809 - val_sparse_categorical_accuracy: 0.4293\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 10s 48ms/step - loss: 1.6307 - sparse_categorical_accuracy: 0.4109 - val_loss: 1.5843 - val_sparse_categorical_accuracy: 0.4309\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47/50\n",
            "196/196 [==============================] - 9s 48ms/step - loss: 1.6220 - sparse_categorical_accuracy: 0.4151 - val_loss: 1.5733 - val_sparse_categorical_accuracy: 0.4351\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 48/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6117 - sparse_categorical_accuracy: 0.4145 - val_loss: 1.5762 - val_sparse_categorical_accuracy: 0.4306\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 9s 47ms/step - loss: 1.6082 - sparse_categorical_accuracy: 0.4204 - val_loss: 1.5703 - val_sparse_categorical_accuracy: 0.4321\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 9s 48ms/step - loss: 1.6078 - sparse_categorical_accuracy: 0.4218 - val_loss: 1.5598 - val_sparse_categorical_accuracy: 0.4383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_8l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total training time 1272.4663836956024 seconds\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 16, 16)       256         resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 21)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 16, 16)       1274        input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 16, 16)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 16, 16)       576         skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 16, 16)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 16, 16)       1072        skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 16, 16)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 16, 16)       576         skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 16, 16)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 16, 16)       1072        skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 16, 16)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 16, 16)       576         skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 16, 16)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 16, 16)       1072        skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 16, 16)       0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 16, 16)       576         skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 16, 16)       0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 16, 16)       1072        skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 16, 16)       0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 16, 16)       576         skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 16, 16)       0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 16, 16)       1274        skip_latent_mlp_0_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 16, 16)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 16, 16)       576         skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 16, 16)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 16, 16)       1072        skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 16, 16)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 16, 16)       576         skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 16, 16)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 16, 16)       1072        skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 16, 16)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 16, 16)       576         skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 16, 16)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 16, 16)       1072        skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 16, 16)       0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 16, 16)       576         skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 16, 16)       0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 16, 16)       1072        skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 16, 16)       0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 16, 16)       576         skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 16, 16)       0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 16)        0           skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 16)        32          avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        170         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 17,342\n",
            "Trainable params: 17,342\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 29s 80ms/step - loss: 2.2757 - sparse_categorical_accuracy: 0.1406 - val_loss: 2.0998 - val_sparse_categorical_accuracy: 0.2083\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 [==============================] - 11s 55ms/step - loss: 2.0798 - sparse_categorical_accuracy: 0.2081 - val_loss: 1.9811 - val_sparse_categorical_accuracy: 0.2650\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 2.0012 - sparse_categorical_accuracy: 0.2511 - val_loss: 1.9410 - val_sparse_categorical_accuracy: 0.2841\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.9372 - sparse_categorical_accuracy: 0.2864 - val_loss: 1.8573 - val_sparse_categorical_accuracy: 0.3271\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.8899 - sparse_categorical_accuracy: 0.3129 - val_loss: 1.8617 - val_sparse_categorical_accuracy: 0.3273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 [==============================] - 11s 57ms/step - loss: 1.8665 - sparse_categorical_accuracy: 0.3183 - val_loss: 1.8201 - val_sparse_categorical_accuracy: 0.3413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.8386 - sparse_categorical_accuracy: 0.3304 - val_loss: 1.7991 - val_sparse_categorical_accuracy: 0.3458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 [==============================] - 11s 55ms/step - loss: 1.8317 - sparse_categorical_accuracy: 0.3327 - val_loss: 1.8116 - val_sparse_categorical_accuracy: 0.3439\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 11s 55ms/step - loss: 1.8110 - sparse_categorical_accuracy: 0.3427 - val_loss: 1.7587 - val_sparse_categorical_accuracy: 0.3676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 [==============================] - 11s 55ms/step - loss: 1.7913 - sparse_categorical_accuracy: 0.3500 - val_loss: 1.7458 - val_sparse_categorical_accuracy: 0.3712\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 [==============================] - 11s 57ms/step - loss: 1.7777 - sparse_categorical_accuracy: 0.3567 - val_loss: 1.7349 - val_sparse_categorical_accuracy: 0.3734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.7581 - sparse_categorical_accuracy: 0.3653 - val_loss: 1.7295 - val_sparse_categorical_accuracy: 0.3817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.7585 - sparse_categorical_accuracy: 0.3608 - val_loss: 1.7156 - val_sparse_categorical_accuracy: 0.3778\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 11s 55ms/step - loss: 1.7439 - sparse_categorical_accuracy: 0.3695 - val_loss: 1.7169 - val_sparse_categorical_accuracy: 0.3788\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 11s 55ms/step - loss: 1.7401 - sparse_categorical_accuracy: 0.3700 - val_loss: 1.7040 - val_sparse_categorical_accuracy: 0.3855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.7175 - sparse_categorical_accuracy: 0.3757 - val_loss: 1.6859 - val_sparse_categorical_accuracy: 0.3877\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.7114 - sparse_categorical_accuracy: 0.3809 - val_loss: 1.6695 - val_sparse_categorical_accuracy: 0.3928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.6930 - sparse_categorical_accuracy: 0.3907 - val_loss: 1.6645 - val_sparse_categorical_accuracy: 0.3933\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.6882 - sparse_categorical_accuracy: 0.3886 - val_loss: 1.6453 - val_sparse_categorical_accuracy: 0.4004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.6738 - sparse_categorical_accuracy: 0.3900 - val_loss: 1.6495 - val_sparse_categorical_accuracy: 0.4043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.6625 - sparse_categorical_accuracy: 0.3965 - val_loss: 1.6231 - val_sparse_categorical_accuracy: 0.4130\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.6602 - sparse_categorical_accuracy: 0.3991 - val_loss: 1.6156 - val_sparse_categorical_accuracy: 0.4099\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 11s 55ms/step - loss: 1.6476 - sparse_categorical_accuracy: 0.3985 - val_loss: 1.6392 - val_sparse_categorical_accuracy: 0.4097\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 11s 55ms/step - loss: 1.6418 - sparse_categorical_accuracy: 0.4051 - val_loss: 1.6018 - val_sparse_categorical_accuracy: 0.4193\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.6263 - sparse_categorical_accuracy: 0.4106 - val_loss: 1.5923 - val_sparse_categorical_accuracy: 0.4221\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.6239 - sparse_categorical_accuracy: 0.4080 - val_loss: 1.5944 - val_sparse_categorical_accuracy: 0.4248\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.6197 - sparse_categorical_accuracy: 0.4168 - val_loss: 1.6182 - val_sparse_categorical_accuracy: 0.4135\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 11s 55ms/step - loss: 1.6135 - sparse_categorical_accuracy: 0.4120 - val_loss: 1.5664 - val_sparse_categorical_accuracy: 0.4311\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.6031 - sparse_categorical_accuracy: 0.4185 - val_loss: 1.5929 - val_sparse_categorical_accuracy: 0.4205\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 11s 55ms/step - loss: 1.6022 - sparse_categorical_accuracy: 0.4208 - val_loss: 1.5714 - val_sparse_categorical_accuracy: 0.4280\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.6012 - sparse_categorical_accuracy: 0.4185 - val_loss: 1.5558 - val_sparse_categorical_accuracy: 0.4376\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.6027 - sparse_categorical_accuracy: 0.4168 - val_loss: 1.5538 - val_sparse_categorical_accuracy: 0.4334\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 11s 55ms/step - loss: 1.5937 - sparse_categorical_accuracy: 0.4205 - val_loss: 1.5533 - val_sparse_categorical_accuracy: 0.4374\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.5905 - sparse_categorical_accuracy: 0.4268 - val_loss: 1.5524 - val_sparse_categorical_accuracy: 0.4398\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.5871 - sparse_categorical_accuracy: 0.4231 - val_loss: 1.5376 - val_sparse_categorical_accuracy: 0.4435\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.5761 - sparse_categorical_accuracy: 0.4300 - val_loss: 1.5299 - val_sparse_categorical_accuracy: 0.4409\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.5584 - sparse_categorical_accuracy: 0.4341 - val_loss: 1.5265 - val_sparse_categorical_accuracy: 0.4458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.5642 - sparse_categorical_accuracy: 0.4313 - val_loss: 1.5322 - val_sparse_categorical_accuracy: 0.4410\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.5603 - sparse_categorical_accuracy: 0.4334 - val_loss: 1.5307 - val_sparse_categorical_accuracy: 0.4436\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 11s 55ms/step - loss: 1.5544 - sparse_categorical_accuracy: 0.4373 - val_loss: 1.5184 - val_sparse_categorical_accuracy: 0.4477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 41/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.5452 - sparse_categorical_accuracy: 0.4401 - val_loss: 1.5186 - val_sparse_categorical_accuracy: 0.4475\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 11s 57ms/step - loss: 1.5572 - sparse_categorical_accuracy: 0.4358 - val_loss: 1.5181 - val_sparse_categorical_accuracy: 0.4517\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43/50\n",
            "196/196 [==============================] - 11s 57ms/step - loss: 1.5429 - sparse_categorical_accuracy: 0.4407 - val_loss: 1.5102 - val_sparse_categorical_accuracy: 0.4499\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 11s 57ms/step - loss: 1.5355 - sparse_categorical_accuracy: 0.4453 - val_loss: 1.5088 - val_sparse_categorical_accuracy: 0.4544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 45/50\n",
            "196/196 [==============================] - 11s 55ms/step - loss: 1.5423 - sparse_categorical_accuracy: 0.4432 - val_loss: 1.5235 - val_sparse_categorical_accuracy: 0.4495\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.5494 - sparse_categorical_accuracy: 0.4365 - val_loss: 1.5010 - val_sparse_categorical_accuracy: 0.4591\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47/50\n",
            "196/196 [==============================] - 11s 57ms/step - loss: 1.5268 - sparse_categorical_accuracy: 0.4489 - val_loss: 1.5118 - val_sparse_categorical_accuracy: 0.4537\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.5301 - sparse_categorical_accuracy: 0.4477 - val_loss: 1.5134 - val_sparse_categorical_accuracy: 0.4541\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.5349 - sparse_categorical_accuracy: 0.4412 - val_loss: 1.5095 - val_sparse_categorical_accuracy: 0.4620\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_16l_16ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50/50\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 1.5258 - sparse_categorical_accuracy: 0.4478 - val_loss: 1.5005 - val_sparse_categorical_accuracy: 0.4583\n",
            "Total training time 1287.535783290863 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUvrKjoA_wnW"
      },
      "source": [
        "##Exp 3-latent dimension\n",
        "Model : PERCEIVER  \n",
        "Preprocessing   : False  \n",
        "Batch size      : 256  \n",
        "max_freq        : 4  \n",
        "num_freq_bands  : 4  \n",
        "freq_base       : 2.0  \n",
        "num_latents     : 4  \n",
        "latent_dim      : 32  \n",
        "latent_heads    : 1  \n",
        "latent_head_dim : 16  \n",
        "cross_heads     : 1  \n",
        "cross_head_dim  : 16  \n",
        "cross_depth     : 2  \n",
        "latent_depth    : 4     \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcQBua2y_w9r",
        "outputId": "2771a40c-8797-4ec8-ace9-d32a60d923f6"
      },
      "source": [
        "for num_latents in [4]:\n",
        "  for latent_dim in [32]:\n",
        "    model = Perceiver(num_classes=num_classes, \n",
        "                      image_height=image_height, \n",
        "                      image_width=image_width,\n",
        "                      max_freq=4,\n",
        "                      num_freq_bands=4,\n",
        "                      freq_base=2.0,\n",
        "                      num_latents=num_latents,\n",
        "                      latent_dim=latent_dim,\n",
        "                      latent_heads=1,\n",
        "                      latent_head_dim=16,\n",
        "                      cross_heads=1,\n",
        "                      cross_head_dim=16,\n",
        "                      attn_dropout=0.2,\n",
        "                      mlp_dropout=0.2,\n",
        "                      cross_depth=2,\n",
        "                      latent_depth=4,             \n",
        "                      weight_tie_layers=True,\n",
        "                      preprocess=False).model()\n",
        "    model.summary()\n",
        "    train_and_eval(model, SAVE_PATH + f'_PERCEIVER_4fq_4fqb_{num_latents}l_{latent_dim}ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld', verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 4, 32)        128         resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 21)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 4, 32)        1834        input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 4, 32)        0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 4, 32)        2176        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 4, 32)        0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 4, 32)        2144        skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 4, 32)        0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 4, 32)        2176        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 4, 32)        0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 4, 32)        2144        skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 4, 32)        0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 4, 32)        2176        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 4, 32)        0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 4, 32)        2144        skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 4, 32)        0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 4, 32)        2176        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 4, 32)        0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 4, 32)        2144        skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 4, 32)        0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 4, 32)        2176        skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 4, 32)        0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 4, 32)        1834        skip_latent_mlp_0_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 4, 32)        0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 4, 32)        2176        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 4, 32)        0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 4, 32)        2144        skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 4, 32)        0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 4, 32)        2176        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 4, 32)        0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 4, 32)        2144        skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 4, 32)        0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 4, 32)        2176        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 4, 32)        0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 4, 32)        2144        skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 4, 32)        0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 4, 32)        2176        skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 4, 32)        0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 4, 32)        2144        skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 4, 32)        0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 4, 32)        2176        skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 4, 32)        0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 32)        0           skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 32)        64          avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        330         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 43,102\n",
            "Trainable params: 43,102\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 24s 68ms/step - loss: 2.3025 - sparse_categorical_accuracy: 0.1344 - val_loss: 1.9973 - val_sparse_categorical_accuracy: 0.2563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 2.0265 - sparse_categorical_accuracy: 0.2367 - val_loss: 1.9316 - val_sparse_categorical_accuracy: 0.2752\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.9552 - sparse_categorical_accuracy: 0.2708 - val_loss: 1.8569 - val_sparse_categorical_accuracy: 0.3231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.9013 - sparse_categorical_accuracy: 0.3032 - val_loss: 1.8154 - val_sparse_categorical_accuracy: 0.3414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.8672 - sparse_categorical_accuracy: 0.3161 - val_loss: 1.8021 - val_sparse_categorical_accuracy: 0.3496\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.8394 - sparse_categorical_accuracy: 0.3310 - val_loss: 1.7832 - val_sparse_categorical_accuracy: 0.3572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.8195 - sparse_categorical_accuracy: 0.3396 - val_loss: 1.7589 - val_sparse_categorical_accuracy: 0.3657\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.8088 - sparse_categorical_accuracy: 0.3419 - val_loss: 1.7408 - val_sparse_categorical_accuracy: 0.3684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.7874 - sparse_categorical_accuracy: 0.3525 - val_loss: 1.7244 - val_sparse_categorical_accuracy: 0.3788\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.7706 - sparse_categorical_accuracy: 0.3619 - val_loss: 1.7211 - val_sparse_categorical_accuracy: 0.3777\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.7551 - sparse_categorical_accuracy: 0.3625 - val_loss: 1.7015 - val_sparse_categorical_accuracy: 0.3885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.7499 - sparse_categorical_accuracy: 0.3699 - val_loss: 1.6940 - val_sparse_categorical_accuracy: 0.3856\n",
            "Epoch 13/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.7248 - sparse_categorical_accuracy: 0.3723 - val_loss: 1.6879 - val_sparse_categorical_accuracy: 0.3924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.7247 - sparse_categorical_accuracy: 0.3765 - val_loss: 1.6635 - val_sparse_categorical_accuracy: 0.3978\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.7045 - sparse_categorical_accuracy: 0.3802 - val_loss: 1.6614 - val_sparse_categorical_accuracy: 0.3981\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.6993 - sparse_categorical_accuracy: 0.3833 - val_loss: 1.6582 - val_sparse_categorical_accuracy: 0.3995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6855 - sparse_categorical_accuracy: 0.3927 - val_loss: 1.6265 - val_sparse_categorical_accuracy: 0.4153\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.6725 - sparse_categorical_accuracy: 0.3953 - val_loss: 1.6211 - val_sparse_categorical_accuracy: 0.4136\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6540 - sparse_categorical_accuracy: 0.4010 - val_loss: 1.6134 - val_sparse_categorical_accuracy: 0.4166\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6522 - sparse_categorical_accuracy: 0.4030 - val_loss: 1.6084 - val_sparse_categorical_accuracy: 0.4211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.6485 - sparse_categorical_accuracy: 0.4020 - val_loss: 1.6098 - val_sparse_categorical_accuracy: 0.4140\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.6354 - sparse_categorical_accuracy: 0.4063 - val_loss: 1.6448 - val_sparse_categorical_accuracy: 0.4092\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.6362 - sparse_categorical_accuracy: 0.4086 - val_loss: 1.5812 - val_sparse_categorical_accuracy: 0.4304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.6213 - sparse_categorical_accuracy: 0.4113 - val_loss: 1.5833 - val_sparse_categorical_accuracy: 0.4299\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.6168 - sparse_categorical_accuracy: 0.4153 - val_loss: 1.5715 - val_sparse_categorical_accuracy: 0.4313\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.6029 - sparse_categorical_accuracy: 0.4220 - val_loss: 1.5569 - val_sparse_categorical_accuracy: 0.4364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.5978 - sparse_categorical_accuracy: 0.4242 - val_loss: 1.5638 - val_sparse_categorical_accuracy: 0.4334\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.5999 - sparse_categorical_accuracy: 0.4177 - val_loss: 1.5412 - val_sparse_categorical_accuracy: 0.4414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.5790 - sparse_categorical_accuracy: 0.4285 - val_loss: 1.5509 - val_sparse_categorical_accuracy: 0.4339\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.5752 - sparse_categorical_accuracy: 0.4321 - val_loss: 1.5383 - val_sparse_categorical_accuracy: 0.4479\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.5738 - sparse_categorical_accuracy: 0.4333 - val_loss: 1.5420 - val_sparse_categorical_accuracy: 0.4480\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.5703 - sparse_categorical_accuracy: 0.4330 - val_loss: 1.5222 - val_sparse_categorical_accuracy: 0.4526\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.5632 - sparse_categorical_accuracy: 0.4324 - val_loss: 1.5140 - val_sparse_categorical_accuracy: 0.4553\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.5535 - sparse_categorical_accuracy: 0.4412 - val_loss: 1.5097 - val_sparse_categorical_accuracy: 0.4566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.5445 - sparse_categorical_accuracy: 0.4437 - val_loss: 1.5128 - val_sparse_categorical_accuracy: 0.4566\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.5495 - sparse_categorical_accuracy: 0.4405 - val_loss: 1.4971 - val_sparse_categorical_accuracy: 0.4579\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.5384 - sparse_categorical_accuracy: 0.4468 - val_loss: 1.5196 - val_sparse_categorical_accuracy: 0.4544\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.5373 - sparse_categorical_accuracy: 0.4456 - val_loss: 1.4868 - val_sparse_categorical_accuracy: 0.4601\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.5264 - sparse_categorical_accuracy: 0.4535 - val_loss: 1.4857 - val_sparse_categorical_accuracy: 0.4607\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.5234 - sparse_categorical_accuracy: 0.4498 - val_loss: 1.4815 - val_sparse_categorical_accuracy: 0.4631\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 41/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.5081 - sparse_categorical_accuracy: 0.4588 - val_loss: 1.4723 - val_sparse_categorical_accuracy: 0.4724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 42/50\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.5255 - sparse_categorical_accuracy: 0.4502 - val_loss: 1.4845 - val_sparse_categorical_accuracy: 0.4649\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.5168 - sparse_categorical_accuracy: 0.4561 - val_loss: 1.4854 - val_sparse_categorical_accuracy: 0.4642\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.5105 - sparse_categorical_accuracy: 0.4571 - val_loss: 1.4658 - val_sparse_categorical_accuracy: 0.4753\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 45/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.4984 - sparse_categorical_accuracy: 0.4600 - val_loss: 1.4731 - val_sparse_categorical_accuracy: 0.4673\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.5033 - sparse_categorical_accuracy: 0.4611 - val_loss: 1.4651 - val_sparse_categorical_accuracy: 0.4746\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.4941 - sparse_categorical_accuracy: 0.4633 - val_loss: 1.4652 - val_sparse_categorical_accuracy: 0.4774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 48/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.4966 - sparse_categorical_accuracy: 0.4642 - val_loss: 1.4612 - val_sparse_categorical_accuracy: 0.4746\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 1.4799 - sparse_categorical_accuracy: 0.4696 - val_loss: 1.4569 - val_sparse_categorical_accuracy: 0.4775\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_100_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_4fq_4fqb_4l_32ldim_1lh_16lhdim_1ch_16chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50/50\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.4887 - sparse_categorical_accuracy: 0.4638 - val_loss: 1.4655 - val_sparse_categorical_accuracy: 0.4723\n",
            "Total training time 1249.5326635837555 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-ct2RSvC0js"
      },
      "source": [
        "Different latent dimensions. Helps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF3KI-uVqM7B",
        "outputId": "2198072c-8040-45c1-8cc4-418801ae8dcb"
      },
      "source": [
        "for num_latents in [16]:\n",
        "  for latent_dim in [64,128]:\n",
        "    for latent_head_dim in [64]:\n",
        "      for cross_head_dim in [64]:\n",
        "        model = Perceiver(num_classes=num_classes, \n",
        "                          image_height=image_height, \n",
        "                          image_width=image_width,\n",
        "                          max_freq=8,\n",
        "                          num_freq_bands=8,\n",
        "                          freq_base=2.0,\n",
        "                          num_latents=num_latents,\n",
        "                          latent_dim=latent_dim,\n",
        "                          latent_heads=1,\n",
        "                          latent_head_dim=latent_head_dim,\n",
        "                          cross_heads=1,\n",
        "                          cross_head_dim=cross_head_dim,\n",
        "                          attn_dropout=0.2,\n",
        "                          mlp_dropout=0.2,\n",
        "                          cross_depth=2,\n",
        "                          latent_depth=4,             \n",
        "                          weight_tie_layers=True,\n",
        "                          preprocess=False).model()\n",
        "        model.summary()\n",
        "        train_and_eval(model, SAVE_PATH + f'_PERCEIVER_8fq_8fqb_{num_latents}l_{latent_dim}ldim_1lh_{latent_head_dim}lhdim_1ch_{cross_head_dim}chdim_2cd_4ld', verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 16, 64)       1024        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 37)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 16, 64)       13194       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 16, 64)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 16, 64)       8448        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 16, 64)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 16, 64)       16576       skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 16, 64)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 16, 64)       8448        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 16, 64)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 16, 64)       16576       skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 16, 64)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 16, 64)       8448        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 16, 64)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 16, 64)       16576       skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 16, 64)       0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 16, 64)       8448        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 16, 64)       0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 16, 64)       16576       skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 16, 64)       0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 16, 64)       8448        skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 16, 64)       0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 16, 64)       13194       skip_latent_mlp_0_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 16, 64)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 16, 64)       8448        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 16, 64)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 16, 64)       16576       skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 16, 64)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 16, 64)       8448        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 16, 64)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 16, 64)       16576       skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 16, 64)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 16, 64)       8448        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 16, 64)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 16, 64)       16576       skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 16, 64)       0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 16, 64)       8448        skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 16, 64)       0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 16, 64)       16576       skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 16, 64)       0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 16, 64)       8448        skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 16, 64)       0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 64)        0           skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 64)        128         avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        650         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 245,278\n",
            "Trainable params: 245,278\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 61s 97ms/step - loss: 2.2537 - sparse_categorical_accuracy: 0.1643 - val_loss: 1.9261 - val_sparse_categorical_accuracy: 0.2959\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 [==============================] - 16s 79ms/step - loss: 1.9178 - sparse_categorical_accuracy: 0.2957 - val_loss: 1.8154 - val_sparse_categorical_accuracy: 0.3388\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 [==============================] - 16s 80ms/step - loss: 1.7898 - sparse_categorical_accuracy: 0.3503 - val_loss: 1.7168 - val_sparse_categorical_accuracy: 0.3769\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 [==============================] - 16s 79ms/step - loss: 1.7251 - sparse_categorical_accuracy: 0.3731 - val_loss: 1.6946 - val_sparse_categorical_accuracy: 0.3914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.6755 - sparse_categorical_accuracy: 0.3974 - val_loss: 1.6150 - val_sparse_categorical_accuracy: 0.4211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 [==============================] - 16s 80ms/step - loss: 1.6132 - sparse_categorical_accuracy: 0.4185 - val_loss: 1.5775 - val_sparse_categorical_accuracy: 0.4330\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 1.5804 - sparse_categorical_accuracy: 0.4316 - val_loss: 1.5527 - val_sparse_categorical_accuracy: 0.4403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 1.5482 - sparse_categorical_accuracy: 0.4451 - val_loss: 1.5218 - val_sparse_categorical_accuracy: 0.4551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 1.5225 - sparse_categorical_accuracy: 0.4539 - val_loss: 1.5159 - val_sparse_categorical_accuracy: 0.4595\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.4998 - sparse_categorical_accuracy: 0.4626 - val_loss: 1.4752 - val_sparse_categorical_accuracy: 0.4702\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 [==============================] - 16s 80ms/step - loss: 1.4776 - sparse_categorical_accuracy: 0.4661 - val_loss: 1.4374 - val_sparse_categorical_accuracy: 0.4833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 [==============================] - 16s 79ms/step - loss: 1.4498 - sparse_categorical_accuracy: 0.4773 - val_loss: 1.4272 - val_sparse_categorical_accuracy: 0.4883\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.4089 - sparse_categorical_accuracy: 0.4933 - val_loss: 1.3859 - val_sparse_categorical_accuracy: 0.5075\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.3917 - sparse_categorical_accuracy: 0.4981 - val_loss: 1.3731 - val_sparse_categorical_accuracy: 0.5064\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.3676 - sparse_categorical_accuracy: 0.5082 - val_loss: 1.3691 - val_sparse_categorical_accuracy: 0.5096\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 1.3445 - sparse_categorical_accuracy: 0.5183 - val_loss: 1.3715 - val_sparse_categorical_accuracy: 0.5059\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 1.3319 - sparse_categorical_accuracy: 0.5213 - val_loss: 1.3243 - val_sparse_categorical_accuracy: 0.5217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.3208 - sparse_categorical_accuracy: 0.5255 - val_loss: 1.3282 - val_sparse_categorical_accuracy: 0.5222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 1.2971 - sparse_categorical_accuracy: 0.5361 - val_loss: 1.3162 - val_sparse_categorical_accuracy: 0.5338\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.2888 - sparse_categorical_accuracy: 0.5393 - val_loss: 1.3014 - val_sparse_categorical_accuracy: 0.5272\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 16s 80ms/step - loss: 1.2694 - sparse_categorical_accuracy: 0.5454 - val_loss: 1.3055 - val_sparse_categorical_accuracy: 0.5278\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 16s 80ms/step - loss: 1.2562 - sparse_categorical_accuracy: 0.5479 - val_loss: 1.2889 - val_sparse_categorical_accuracy: 0.5375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 [==============================] - 16s 79ms/step - loss: 1.2354 - sparse_categorical_accuracy: 0.5558 - val_loss: 1.2811 - val_sparse_categorical_accuracy: 0.5440\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 [==============================] - 16s 79ms/step - loss: 1.2329 - sparse_categorical_accuracy: 0.5570 - val_loss: 1.3201 - val_sparse_categorical_accuracy: 0.5285\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.2239 - sparse_categorical_accuracy: 0.5590 - val_loss: 1.2767 - val_sparse_categorical_accuracy: 0.5439\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 16s 80ms/step - loss: 1.2130 - sparse_categorical_accuracy: 0.5649 - val_loss: 1.2619 - val_sparse_categorical_accuracy: 0.5479\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 1.1987 - sparse_categorical_accuracy: 0.5720 - val_loss: 1.2655 - val_sparse_categorical_accuracy: 0.5496\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.2039 - sparse_categorical_accuracy: 0.5700 - val_loss: 1.2862 - val_sparse_categorical_accuracy: 0.5482\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 1.1732 - sparse_categorical_accuracy: 0.5818 - val_loss: 1.2625 - val_sparse_categorical_accuracy: 0.5546\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 1.1738 - sparse_categorical_accuracy: 0.5803 - val_loss: 1.2710 - val_sparse_categorical_accuracy: 0.5479\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.1692 - sparse_categorical_accuracy: 0.5800 - val_loss: 1.2592 - val_sparse_categorical_accuracy: 0.5506\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.1495 - sparse_categorical_accuracy: 0.5880 - val_loss: 1.2533 - val_sparse_categorical_accuracy: 0.5571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.1463 - sparse_categorical_accuracy: 0.5886 - val_loss: 1.2597 - val_sparse_categorical_accuracy: 0.5591\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.1296 - sparse_categorical_accuracy: 0.5921 - val_loss: 1.2413 - val_sparse_categorical_accuracy: 0.5596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 [==============================] - 16s 80ms/step - loss: 1.1266 - sparse_categorical_accuracy: 0.5991 - val_loss: 1.2618 - val_sparse_categorical_accuracy: 0.5559\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.1252 - sparse_categorical_accuracy: 0.5954 - val_loss: 1.2546 - val_sparse_categorical_accuracy: 0.5569\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 1.0972 - sparse_categorical_accuracy: 0.6074 - val_loss: 1.2543 - val_sparse_categorical_accuracy: 0.5578\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.0914 - sparse_categorical_accuracy: 0.6077 - val_loss: 1.2477 - val_sparse_categorical_accuracy: 0.5595\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 16s 80ms/step - loss: 1.0963 - sparse_categorical_accuracy: 0.6072 - val_loss: 1.2437 - val_sparse_categorical_accuracy: 0.5626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.0862 - sparse_categorical_accuracy: 0.6096 - val_loss: 1.2563 - val_sparse_categorical_accuracy: 0.5538\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 16s 80ms/step - loss: 1.0882 - sparse_categorical_accuracy: 0.6096 - val_loss: 1.2472 - val_sparse_categorical_accuracy: 0.5623\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 1.0701 - sparse_categorical_accuracy: 0.6141 - val_loss: 1.2379 - val_sparse_categorical_accuracy: 0.5609\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 16s 79ms/step - loss: 1.0519 - sparse_categorical_accuracy: 0.6226 - val_loss: 1.2446 - val_sparse_categorical_accuracy: 0.5590\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 16s 80ms/step - loss: 1.0566 - sparse_categorical_accuracy: 0.6195 - val_loss: 1.2374 - val_sparse_categorical_accuracy: 0.5669\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 45/50\n",
            "196/196 [==============================] - 16s 79ms/step - loss: 1.0521 - sparse_categorical_accuracy: 0.6251 - val_loss: 1.2576 - val_sparse_categorical_accuracy: 0.5622\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 1.0395 - sparse_categorical_accuracy: 0.6241 - val_loss: 1.2303 - val_sparse_categorical_accuracy: 0.5700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47/50\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 1.0365 - sparse_categorical_accuracy: 0.6241 - val_loss: 1.2562 - val_sparse_categorical_accuracy: 0.5567\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 1.0284 - sparse_categorical_accuracy: 0.6316 - val_loss: 1.2287 - val_sparse_categorical_accuracy: 0.5673\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.0239 - sparse_categorical_accuracy: 0.6323 - val_loss: 1.2479 - val_sparse_categorical_accuracy: 0.5692\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 16s 79ms/step - loss: 1.0177 - sparse_categorical_accuracy: 0.6367 - val_loss: 1.2273 - val_sparse_categorical_accuracy: 0.5672\n",
            "Total training time 1495.888422012329 seconds\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 16, 128)      2048        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 37)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 16, 128)      21578       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 16, 128)      0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 16, 128)      33280       skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 16, 128)      0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 16, 128)      33152       skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 16, 128)      0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 16, 128)      33280       skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 16, 128)      0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 16, 128)      33152       skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 16, 128)      0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 16, 128)      33280       skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 16, 128)      0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 16, 128)      33152       skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 16, 128)      0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 16, 128)      33280       skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 16, 128)      0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 16, 128)      33152       skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 16, 128)      0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 16, 128)      33280       skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 16, 128)      0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 16, 128)      21578       skip_latent_mlp_0_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 16, 128)      0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 16, 128)      33280       skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 16, 128)      0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 16, 128)      33152       skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 16, 128)      0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 16, 128)      33280       skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 16, 128)      0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 16, 128)      33152       skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 16, 128)      0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 16, 128)      33280       skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 16, 128)      0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 16, 128)      33152       skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 16, 128)      0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 16, 128)      33280       skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 16, 128)      0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 16, 128)      33152       skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 16, 128)      0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 16, 128)      33280       skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 16, 128)      0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 128)       0           skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 128)       256         avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        1290        ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 644,766\n",
            "Trainable params: 644,766\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 35s 112ms/step - loss: 2.4031 - sparse_categorical_accuracy: 0.1521 - val_loss: 1.9478 - val_sparse_categorical_accuracy: 0.2814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.9421 - sparse_categorical_accuracy: 0.2727 - val_loss: 1.8298 - val_sparse_categorical_accuracy: 0.3263\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.8332 - sparse_categorical_accuracy: 0.3285 - val_loss: 1.7460 - val_sparse_categorical_accuracy: 0.3563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.7328 - sparse_categorical_accuracy: 0.3662 - val_loss: 1.6577 - val_sparse_categorical_accuracy: 0.3998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.6736 - sparse_categorical_accuracy: 0.3872 - val_loss: 1.5664 - val_sparse_categorical_accuracy: 0.4351\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.5969 - sparse_categorical_accuracy: 0.4143 - val_loss: 1.5065 - val_sparse_categorical_accuracy: 0.4530\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.5316 - sparse_categorical_accuracy: 0.4421 - val_loss: 1.4798 - val_sparse_categorical_accuracy: 0.4627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.4987 - sparse_categorical_accuracy: 0.4559 - val_loss: 1.4664 - val_sparse_categorical_accuracy: 0.4765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.4565 - sparse_categorical_accuracy: 0.4767 - val_loss: 1.4479 - val_sparse_categorical_accuracy: 0.4740\n",
            "Epoch 10/50\n",
            "196/196 [==============================] - 17s 89ms/step - loss: 1.4277 - sparse_categorical_accuracy: 0.4838 - val_loss: 1.3795 - val_sparse_categorical_accuracy: 0.5016\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.3941 - sparse_categorical_accuracy: 0.4974 - val_loss: 1.3434 - val_sparse_categorical_accuracy: 0.5173\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.3659 - sparse_categorical_accuracy: 0.5077 - val_loss: 1.3354 - val_sparse_categorical_accuracy: 0.5214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.3372 - sparse_categorical_accuracy: 0.5197 - val_loss: 1.3329 - val_sparse_categorical_accuracy: 0.5278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.3108 - sparse_categorical_accuracy: 0.5283 - val_loss: 1.3365 - val_sparse_categorical_accuracy: 0.5157\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.3033 - sparse_categorical_accuracy: 0.5288 - val_loss: 1.2865 - val_sparse_categorical_accuracy: 0.5374\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.2755 - sparse_categorical_accuracy: 0.5376 - val_loss: 1.2905 - val_sparse_categorical_accuracy: 0.5406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.2641 - sparse_categorical_accuracy: 0.5461 - val_loss: 1.2806 - val_sparse_categorical_accuracy: 0.5404\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.2444 - sparse_categorical_accuracy: 0.5533 - val_loss: 1.2574 - val_sparse_categorical_accuracy: 0.5525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.2249 - sparse_categorical_accuracy: 0.5559 - val_loss: 1.2562 - val_sparse_categorical_accuracy: 0.5472\n",
            "Epoch 20/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.2031 - sparse_categorical_accuracy: 0.5666 - val_loss: 1.2396 - val_sparse_categorical_accuracy: 0.5597\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.1968 - sparse_categorical_accuracy: 0.5683 - val_loss: 1.2855 - val_sparse_categorical_accuracy: 0.5498\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 18s 89ms/step - loss: 1.1708 - sparse_categorical_accuracy: 0.5791 - val_loss: 1.2304 - val_sparse_categorical_accuracy: 0.5650\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.1579 - sparse_categorical_accuracy: 0.5852 - val_loss: 1.2227 - val_sparse_categorical_accuracy: 0.5679\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.1385 - sparse_categorical_accuracy: 0.5940 - val_loss: 1.2280 - val_sparse_categorical_accuracy: 0.5597\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.1342 - sparse_categorical_accuracy: 0.5920 - val_loss: 1.2298 - val_sparse_categorical_accuracy: 0.5618\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.1183 - sparse_categorical_accuracy: 0.5987 - val_loss: 1.2222 - val_sparse_categorical_accuracy: 0.5665\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 17s 89ms/step - loss: 1.1093 - sparse_categorical_accuracy: 0.6019 - val_loss: 1.2278 - val_sparse_categorical_accuracy: 0.5635\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 17s 89ms/step - loss: 1.0979 - sparse_categorical_accuracy: 0.6015 - val_loss: 1.1929 - val_sparse_categorical_accuracy: 0.5797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 [==============================] - 17s 89ms/step - loss: 1.0710 - sparse_categorical_accuracy: 0.6143 - val_loss: 1.2138 - val_sparse_categorical_accuracy: 0.5710\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 17s 89ms/step - loss: 1.0644 - sparse_categorical_accuracy: 0.6181 - val_loss: 1.1855 - val_sparse_categorical_accuracy: 0.5851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.0423 - sparse_categorical_accuracy: 0.6274 - val_loss: 1.1804 - val_sparse_categorical_accuracy: 0.5804\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.0359 - sparse_categorical_accuracy: 0.6309 - val_loss: 1.2224 - val_sparse_categorical_accuracy: 0.5782\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.0368 - sparse_categorical_accuracy: 0.6286 - val_loss: 1.2031 - val_sparse_categorical_accuracy: 0.5804\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 18s 89ms/step - loss: 1.0112 - sparse_categorical_accuracy: 0.6336 - val_loss: 1.1862 - val_sparse_categorical_accuracy: 0.5873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 0.9924 - sparse_categorical_accuracy: 0.6431 - val_loss: 1.1894 - val_sparse_categorical_accuracy: 0.5851\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 0.9859 - sparse_categorical_accuracy: 0.6462 - val_loss: 1.1963 - val_sparse_categorical_accuracy: 0.5844\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 0.9792 - sparse_categorical_accuracy: 0.6476 - val_loss: 1.1998 - val_sparse_categorical_accuracy: 0.5800\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 0.9515 - sparse_categorical_accuracy: 0.6564 - val_loss: 1.2023 - val_sparse_categorical_accuracy: 0.5822\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 0.9483 - sparse_categorical_accuracy: 0.6600 - val_loss: 1.2152 - val_sparse_categorical_accuracy: 0.5884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 0.9351 - sparse_categorical_accuracy: 0.6587 - val_loss: 1.2129 - val_sparse_categorical_accuracy: 0.5849\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 0.9332 - sparse_categorical_accuracy: 0.6655 - val_loss: 1.1882 - val_sparse_categorical_accuracy: 0.5845\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 17s 89ms/step - loss: 0.9006 - sparse_categorical_accuracy: 0.6772 - val_loss: 1.1975 - val_sparse_categorical_accuracy: 0.5897\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43/50\n",
            "196/196 [==============================] - 17s 89ms/step - loss: 0.8958 - sparse_categorical_accuracy: 0.6779 - val_loss: 1.2017 - val_sparse_categorical_accuracy: 0.5923\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_128ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 44/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 0.8777 - sparse_categorical_accuracy: 0.6838 - val_loss: 1.1935 - val_sparse_categorical_accuracy: 0.5910\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 0.8639 - sparse_categorical_accuracy: 0.6855 - val_loss: 1.2050 - val_sparse_categorical_accuracy: 0.5909\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 0.8539 - sparse_categorical_accuracy: 0.6946 - val_loss: 1.2202 - val_sparse_categorical_accuracy: 0.5782\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 0.8480 - sparse_categorical_accuracy: 0.6955 - val_loss: 1.2432 - val_sparse_categorical_accuracy: 0.5859\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 0.8244 - sparse_categorical_accuracy: 0.7043 - val_loss: 1.2487 - val_sparse_categorical_accuracy: 0.5831\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 17s 89ms/step - loss: 0.8118 - sparse_categorical_accuracy: 0.7073 - val_loss: 1.2626 - val_sparse_categorical_accuracy: 0.5801\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 0.8005 - sparse_categorical_accuracy: 0.7111 - val_loss: 1.2726 - val_sparse_categorical_accuracy: 0.5839\n",
            "Total training time 1453.7621674537659 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFsBvL73DCIJ"
      },
      "source": [
        "Different number of latent heads. Help slightly with 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj3Ma8r1sjJj",
        "outputId": "6a7fcf58-a59b-40bf-905e-9f9da1e6d3bc"
      },
      "source": [
        "for num_latents in [16]:\n",
        "  for latent_dim in [64]:\n",
        "    for latent_head_dim in [64]:\n",
        "      for cross_head_dim in [64]:\n",
        "        for latent_heads in [2,4]:\n",
        "          model = Perceiver(num_classes=num_classes, \n",
        "                            image_height=image_height, \n",
        "                            image_width=image_width,\n",
        "                            max_freq=8,\n",
        "                            num_freq_bands=8,\n",
        "                            freq_base=2.0,\n",
        "                            num_latents=num_latents,\n",
        "                            latent_dim=latent_dim,\n",
        "                            latent_heads=latent_heads,\n",
        "                            latent_head_dim=latent_head_dim,\n",
        "                            cross_heads=1,\n",
        "                            cross_head_dim=cross_head_dim,\n",
        "                            attn_dropout=0.2,\n",
        "                            mlp_dropout=0.2,\n",
        "                            cross_depth=2,\n",
        "                            latent_depth=4,             \n",
        "                            weight_tie_layers=True,\n",
        "                            preprocess=False).model()\n",
        "          model.summary()\n",
        "          train_and_eval(model, SAVE_PATH + f'_PERCEIVER_8fq_8fqb_{num_latents}l_{latent_dim}ldim_{latent_heads}lh_{latent_head_dim}lhdim_1ch_{cross_head_dim}chdim_2cd_4ld', verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 16, 64)       1024        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 37)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 16, 64)       13194       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 16, 64)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 16, 64)       8448        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 16, 64)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 16, 64)       32960       skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 16, 64)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 16, 64)       8448        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 16, 64)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 16, 64)       32960       skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 16, 64)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 16, 64)       8448        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 16, 64)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 16, 64)       32960       skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 16, 64)       0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 16, 64)       8448        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 16, 64)       0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 16, 64)       32960       skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 16, 64)       0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 16, 64)       8448        skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 16, 64)       0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 16, 64)       13194       skip_latent_mlp_0_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 16, 64)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 16, 64)       8448        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 16, 64)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 16, 64)       32960       skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 16, 64)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 16, 64)       8448        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 16, 64)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 16, 64)       32960       skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 16, 64)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 16, 64)       8448        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 16, 64)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 16, 64)       32960       skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 16, 64)       0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 16, 64)       8448        skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 16, 64)       0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 16, 64)       32960       skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 16, 64)       0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 16, 64)       8448        skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 16, 64)       0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 64)        0           skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 64)        128         avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        650         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 376,350\n",
            "Trainable params: 376,350\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 33s 107ms/step - loss: 2.2765 - sparse_categorical_accuracy: 0.1502 - val_loss: 1.9139 - val_sparse_categorical_accuracy: 0.2866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.9272 - sparse_categorical_accuracy: 0.2874 - val_loss: 1.7997 - val_sparse_categorical_accuracy: 0.3413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.8216 - sparse_categorical_accuracy: 0.3354 - val_loss: 1.7090 - val_sparse_categorical_accuracy: 0.3800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.7370 - sparse_categorical_accuracy: 0.3655 - val_loss: 1.6572 - val_sparse_categorical_accuracy: 0.3992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.6750 - sparse_categorical_accuracy: 0.3929 - val_loss: 1.6145 - val_sparse_categorical_accuracy: 0.4162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 1.6226 - sparse_categorical_accuracy: 0.4083 - val_loss: 1.5805 - val_sparse_categorical_accuracy: 0.4294\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 [==============================] - 16s 81ms/step - loss: 1.5941 - sparse_categorical_accuracy: 0.4201 - val_loss: 1.4992 - val_sparse_categorical_accuracy: 0.4575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.5377 - sparse_categorical_accuracy: 0.4427 - val_loss: 1.4756 - val_sparse_categorical_accuracy: 0.4715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.4902 - sparse_categorical_accuracy: 0.4634 - val_loss: 1.4408 - val_sparse_categorical_accuracy: 0.4831\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 [==============================] - 17s 84ms/step - loss: 1.4520 - sparse_categorical_accuracy: 0.4747 - val_loss: 1.4277 - val_sparse_categorical_accuracy: 0.4826\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.4346 - sparse_categorical_accuracy: 0.4824 - val_loss: 1.4114 - val_sparse_categorical_accuracy: 0.4917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.4143 - sparse_categorical_accuracy: 0.4931 - val_loss: 1.3704 - val_sparse_categorical_accuracy: 0.5094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.3793 - sparse_categorical_accuracy: 0.5043 - val_loss: 1.3900 - val_sparse_categorical_accuracy: 0.5038\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.3673 - sparse_categorical_accuracy: 0.5091 - val_loss: 1.3555 - val_sparse_categorical_accuracy: 0.5128\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.3468 - sparse_categorical_accuracy: 0.5166 - val_loss: 1.3404 - val_sparse_categorical_accuracy: 0.5168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 [==============================] - 17s 84ms/step - loss: 1.3358 - sparse_categorical_accuracy: 0.5204 - val_loss: 1.3272 - val_sparse_categorical_accuracy: 0.5208\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 [==============================] - 17s 84ms/step - loss: 1.3132 - sparse_categorical_accuracy: 0.5279 - val_loss: 1.3592 - val_sparse_categorical_accuracy: 0.5118\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.2908 - sparse_categorical_accuracy: 0.5391 - val_loss: 1.3171 - val_sparse_categorical_accuracy: 0.5254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.2845 - sparse_categorical_accuracy: 0.5388 - val_loss: 1.3192 - val_sparse_categorical_accuracy: 0.5305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.2789 - sparse_categorical_accuracy: 0.5412 - val_loss: 1.3001 - val_sparse_categorical_accuracy: 0.5325\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 [==============================] - 16s 84ms/step - loss: 1.2543 - sparse_categorical_accuracy: 0.5510 - val_loss: 1.2951 - val_sparse_categorical_accuracy: 0.5344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 [==============================] - 17s 84ms/step - loss: 1.2448 - sparse_categorical_accuracy: 0.5516 - val_loss: 1.2802 - val_sparse_categorical_accuracy: 0.5400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 [==============================] - 17s 84ms/step - loss: 1.2284 - sparse_categorical_accuracy: 0.5600 - val_loss: 1.2828 - val_sparse_categorical_accuracy: 0.5404\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.2223 - sparse_categorical_accuracy: 0.5611 - val_loss: 1.3027 - val_sparse_categorical_accuracy: 0.5359\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.2146 - sparse_categorical_accuracy: 0.5621 - val_loss: 1.2983 - val_sparse_categorical_accuracy: 0.5366\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 16s 84ms/step - loss: 1.1987 - sparse_categorical_accuracy: 0.5721 - val_loss: 1.2708 - val_sparse_categorical_accuracy: 0.5488\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 [==============================] - 16s 84ms/step - loss: 1.1935 - sparse_categorical_accuracy: 0.5683 - val_loss: 1.2856 - val_sparse_categorical_accuracy: 0.5416\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 17s 84ms/step - loss: 1.1884 - sparse_categorical_accuracy: 0.5727 - val_loss: 1.2698 - val_sparse_categorical_accuracy: 0.5424\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.1692 - sparse_categorical_accuracy: 0.5800 - val_loss: 1.2423 - val_sparse_categorical_accuracy: 0.5561\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n",
            "196/196 [==============================] - 17s 84ms/step - loss: 1.1619 - sparse_categorical_accuracy: 0.5841 - val_loss: 1.2515 - val_sparse_categorical_accuracy: 0.5502\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 16s 84ms/step - loss: 1.1404 - sparse_categorical_accuracy: 0.5931 - val_loss: 1.2478 - val_sparse_categorical_accuracy: 0.5555\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.1413 - sparse_categorical_accuracy: 0.5923 - val_loss: 1.2462 - val_sparse_categorical_accuracy: 0.5617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.1319 - sparse_categorical_accuracy: 0.5943 - val_loss: 1.2558 - val_sparse_categorical_accuracy: 0.5538\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.1152 - sparse_categorical_accuracy: 0.6024 - val_loss: 1.2614 - val_sparse_categorical_accuracy: 0.5562\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 17s 84ms/step - loss: 1.1159 - sparse_categorical_accuracy: 0.6004 - val_loss: 1.2263 - val_sparse_categorical_accuracy: 0.5640\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.1045 - sparse_categorical_accuracy: 0.6059 - val_loss: 1.2407 - val_sparse_categorical_accuracy: 0.5632\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 17s 84ms/step - loss: 1.0948 - sparse_categorical_accuracy: 0.6050 - val_loss: 1.2543 - val_sparse_categorical_accuracy: 0.5557\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.0939 - sparse_categorical_accuracy: 0.6063 - val_loss: 1.2359 - val_sparse_categorical_accuracy: 0.5694\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.0677 - sparse_categorical_accuracy: 0.6196 - val_loss: 1.2395 - val_sparse_categorical_accuracy: 0.5628\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.0646 - sparse_categorical_accuracy: 0.6154 - val_loss: 1.2593 - val_sparse_categorical_accuracy: 0.5645\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.0679 - sparse_categorical_accuracy: 0.6148 - val_loss: 1.2599 - val_sparse_categorical_accuracy: 0.5601\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 16s 82ms/step - loss: 1.0506 - sparse_categorical_accuracy: 0.6229 - val_loss: 1.2284 - val_sparse_categorical_accuracy: 0.5664\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 16s 84ms/step - loss: 1.0494 - sparse_categorical_accuracy: 0.6245 - val_loss: 1.2527 - val_sparse_categorical_accuracy: 0.5669\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.0476 - sparse_categorical_accuracy: 0.6270 - val_loss: 1.2669 - val_sparse_categorical_accuracy: 0.5656\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.0247 - sparse_categorical_accuracy: 0.6334 - val_loss: 1.2554 - val_sparse_categorical_accuracy: 0.5594\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.0302 - sparse_categorical_accuracy: 0.6337 - val_loss: 1.2512 - val_sparse_categorical_accuracy: 0.5657\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 17s 84ms/step - loss: 1.0146 - sparse_categorical_accuracy: 0.6369 - val_loss: 1.2707 - val_sparse_categorical_accuracy: 0.5605\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.0112 - sparse_categorical_accuracy: 0.6378 - val_loss: 1.2405 - val_sparse_categorical_accuracy: 0.5696\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 49/50\n",
            "196/196 [==============================] - 17s 84ms/step - loss: 0.9968 - sparse_categorical_accuracy: 0.6428 - val_loss: 1.2402 - val_sparse_categorical_accuracy: 0.5704\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.0020 - sparse_categorical_accuracy: 0.6416 - val_loss: 1.2412 - val_sparse_categorical_accuracy: 0.5735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_2lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total training time 1504.2219491004944 seconds\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 16, 64)       1024        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 37)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 16, 64)       13194       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 16, 64)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 16, 64)       8448        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 16, 64)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 16, 64)       65728       skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 16, 64)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 16, 64)       8448        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 16, 64)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 16, 64)       65728       skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 16, 64)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 16, 64)       8448        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 16, 64)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 16, 64)       65728       skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 16, 64)       0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 16, 64)       8448        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 16, 64)       0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 16, 64)       65728       skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 16, 64)       0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 16, 64)       8448        skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 16, 64)       0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 16, 64)       13194       skip_latent_mlp_0_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 16, 64)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 16, 64)       8448        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 16, 64)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 16, 64)       65728       skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 16, 64)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 16, 64)       8448        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 16, 64)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 16, 64)       65728       skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 16, 64)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 16, 64)       8448        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 16, 64)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 16, 64)       65728       skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 16, 64)       0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 16, 64)       8448        skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 16, 64)       0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 16, 64)       65728       skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 16, 64)       0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 16, 64)       8448        skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 16, 64)       0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 64)        0           skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 64)        128         avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        650         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 638,494\n",
            "Trainable params: 638,494\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 34s 115ms/step - loss: 2.3414 - sparse_categorical_accuracy: 0.1403 - val_loss: 1.9622 - val_sparse_categorical_accuracy: 0.2653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.9425 - sparse_categorical_accuracy: 0.2802 - val_loss: 1.7731 - val_sparse_categorical_accuracy: 0.3496\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.7936 - sparse_categorical_accuracy: 0.3409 - val_loss: 1.7116 - val_sparse_categorical_accuracy: 0.3781\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.7325 - sparse_categorical_accuracy: 0.3697 - val_loss: 1.6548 - val_sparse_categorical_accuracy: 0.4037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.6771 - sparse_categorical_accuracy: 0.3917 - val_loss: 1.6428 - val_sparse_categorical_accuracy: 0.4071\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 1.6385 - sparse_categorical_accuracy: 0.4119 - val_loss: 1.5640 - val_sparse_categorical_accuracy: 0.4380\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.5899 - sparse_categorical_accuracy: 0.4256 - val_loss: 1.5537 - val_sparse_categorical_accuracy: 0.4398\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 1.5596 - sparse_categorical_accuracy: 0.4429 - val_loss: 1.5158 - val_sparse_categorical_accuracy: 0.4587\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.5268 - sparse_categorical_accuracy: 0.4548 - val_loss: 1.5115 - val_sparse_categorical_accuracy: 0.4654\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 1.5097 - sparse_categorical_accuracy: 0.4546 - val_loss: 1.4764 - val_sparse_categorical_accuracy: 0.4743\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.4676 - sparse_categorical_accuracy: 0.4738 - val_loss: 1.4306 - val_sparse_categorical_accuracy: 0.4831\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 1.4405 - sparse_categorical_accuracy: 0.4831 - val_loss: 1.4232 - val_sparse_categorical_accuracy: 0.4843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.4200 - sparse_categorical_accuracy: 0.4878 - val_loss: 1.4102 - val_sparse_categorical_accuracy: 0.4890\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 [==============================] - 18s 89ms/step - loss: 1.4001 - sparse_categorical_accuracy: 0.4945 - val_loss: 1.4021 - val_sparse_categorical_accuracy: 0.5008\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 [==============================] - 18s 89ms/step - loss: 1.3790 - sparse_categorical_accuracy: 0.5031 - val_loss: 1.3616 - val_sparse_categorical_accuracy: 0.5162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 1.3515 - sparse_categorical_accuracy: 0.5134 - val_loss: 1.3391 - val_sparse_categorical_accuracy: 0.5225\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.3385 - sparse_categorical_accuracy: 0.5179 - val_loss: 1.3467 - val_sparse_categorical_accuracy: 0.5179\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 1.3333 - sparse_categorical_accuracy: 0.5196 - val_loss: 1.3297 - val_sparse_categorical_accuracy: 0.5208\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 18s 92ms/step - loss: 1.3075 - sparse_categorical_accuracy: 0.5304 - val_loss: 1.3299 - val_sparse_categorical_accuracy: 0.5201\n",
            "Epoch 20/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.2891 - sparse_categorical_accuracy: 0.5408 - val_loss: 1.3346 - val_sparse_categorical_accuracy: 0.5268\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.2953 - sparse_categorical_accuracy: 0.5346 - val_loss: 1.3008 - val_sparse_categorical_accuracy: 0.5341\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "196/196 [==============================] - 18s 89ms/step - loss: 1.2704 - sparse_categorical_accuracy: 0.5436 - val_loss: 1.3088 - val_sparse_categorical_accuracy: 0.5363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.2499 - sparse_categorical_accuracy: 0.5521 - val_loss: 1.3024 - val_sparse_categorical_accuracy: 0.5375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.2404 - sparse_categorical_accuracy: 0.5554 - val_loss: 1.2969 - val_sparse_categorical_accuracy: 0.5407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 1.2303 - sparse_categorical_accuracy: 0.5555 - val_loss: 1.2962 - val_sparse_categorical_accuracy: 0.5367\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 1.2147 - sparse_categorical_accuracy: 0.5636 - val_loss: 1.2788 - val_sparse_categorical_accuracy: 0.5415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.2138 - sparse_categorical_accuracy: 0.5657 - val_loss: 1.2747 - val_sparse_categorical_accuracy: 0.5481\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 1.2152 - sparse_categorical_accuracy: 0.5608 - val_loss: 1.2692 - val_sparse_categorical_accuracy: 0.5523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 1.1844 - sparse_categorical_accuracy: 0.5730 - val_loss: 1.2509 - val_sparse_categorical_accuracy: 0.5572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 1.1830 - sparse_categorical_accuracy: 0.5756 - val_loss: 1.2593 - val_sparse_categorical_accuracy: 0.5525\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.1692 - sparse_categorical_accuracy: 0.5771 - val_loss: 1.2508 - val_sparse_categorical_accuracy: 0.5594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.1567 - sparse_categorical_accuracy: 0.5824 - val_loss: 1.2492 - val_sparse_categorical_accuracy: 0.5544\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.1464 - sparse_categorical_accuracy: 0.5871 - val_loss: 1.2606 - val_sparse_categorical_accuracy: 0.5522\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 18s 89ms/step - loss: 1.1344 - sparse_categorical_accuracy: 0.5925 - val_loss: 1.2349 - val_sparse_categorical_accuracy: 0.5627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.1264 - sparse_categorical_accuracy: 0.5950 - val_loss: 1.2546 - val_sparse_categorical_accuracy: 0.5582\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 18s 89ms/step - loss: 1.1172 - sparse_categorical_accuracy: 0.5973 - val_loss: 1.2755 - val_sparse_categorical_accuracy: 0.5519\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.1036 - sparse_categorical_accuracy: 0.6026 - val_loss: 1.2640 - val_sparse_categorical_accuracy: 0.5572\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.0941 - sparse_categorical_accuracy: 0.6069 - val_loss: 1.2416 - val_sparse_categorical_accuracy: 0.5595\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 17s 89ms/step - loss: 1.0980 - sparse_categorical_accuracy: 0.6049 - val_loss: 1.2584 - val_sparse_categorical_accuracy: 0.5584\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 18s 89ms/step - loss: 1.0824 - sparse_categorical_accuracy: 0.6111 - val_loss: 1.2486 - val_sparse_categorical_accuracy: 0.5609\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 1.0674 - sparse_categorical_accuracy: 0.6183 - val_loss: 1.2463 - val_sparse_categorical_accuracy: 0.5635\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 42/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.0609 - sparse_categorical_accuracy: 0.6184 - val_loss: 1.2705 - val_sparse_categorical_accuracy: 0.5566\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 18s 89ms/step - loss: 1.0571 - sparse_categorical_accuracy: 0.6224 - val_loss: 1.2441 - val_sparse_categorical_accuracy: 0.5690\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_16l_64ldim_4lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 44/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.0518 - sparse_categorical_accuracy: 0.6205 - val_loss: 1.2567 - val_sparse_categorical_accuracy: 0.5664\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 18s 89ms/step - loss: 1.0382 - sparse_categorical_accuracy: 0.6262 - val_loss: 1.2518 - val_sparse_categorical_accuracy: 0.5653\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.0399 - sparse_categorical_accuracy: 0.6275 - val_loss: 1.2658 - val_sparse_categorical_accuracy: 0.5585\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 18s 89ms/step - loss: 1.0207 - sparse_categorical_accuracy: 0.6344 - val_loss: 1.2553 - val_sparse_categorical_accuracy: 0.5637\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 18s 93ms/step - loss: 1.0102 - sparse_categorical_accuracy: 0.6342 - val_loss: 1.2784 - val_sparse_categorical_accuracy: 0.5664\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 18s 91ms/step - loss: 1.0074 - sparse_categorical_accuracy: 0.6370 - val_loss: 1.2758 - val_sparse_categorical_accuracy: 0.5615\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 18s 90ms/step - loss: 0.9998 - sparse_categorical_accuracy: 0.6378 - val_loss: 1.2577 - val_sparse_categorical_accuracy: 0.5650\n",
            "Total training time 1594.4333958625793 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXno6lazuJXJ"
      },
      "source": [
        "Different number of latents. Help slightly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98OVv2oBq7di",
        "outputId": "566c2d62-6513-42a6-f762-59fd1f19472a"
      },
      "source": [
        "for num_latents in [32,64,128]:\n",
        "  for latent_dim in [64]:\n",
        "    for latent_head_dim in [64]:\n",
        "      for cross_head_dim in [64]:\n",
        "        model = Perceiver(num_classes=num_classes, \n",
        "                          image_height=image_height, \n",
        "                          image_width=image_width,\n",
        "                          max_freq=8,\n",
        "                          num_freq_bands=8,\n",
        "                          freq_base=2.0,\n",
        "                          num_latents=num_latents,\n",
        "                          latent_dim=latent_dim,\n",
        "                          latent_heads=1,\n",
        "                          latent_head_dim=latent_head_dim,\n",
        "                          cross_heads=1,\n",
        "                          cross_head_dim=cross_head_dim,\n",
        "                          attn_dropout=0.2,\n",
        "                          mlp_dropout=0.2,\n",
        "                          cross_depth=2,\n",
        "                          latent_depth=4,             \n",
        "                          weight_tie_layers=True,\n",
        "                          preprocess=False).model()\n",
        "        model.summary()\n",
        "        train_and_eval(model, SAVE_PATH + f'_PERCEIVER_8fq_8fqb_{num_latents}l_{latent_dim}ldim_1lh_{latent_head_dim}lhdim_1ch_{cross_head_dim}chdim_2cd_4ld', verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 32, 64)       2048        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 37)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 32, 64)       13194       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 32, 64)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 32, 64)       8448        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 32, 64)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 32, 64)       16576       skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 32, 64)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 32, 64)       8448        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 32, 64)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 32, 64)       16576       skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 32, 64)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 32, 64)       8448        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 32, 64)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 32, 64)       16576       skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 32, 64)       0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 32, 64)       8448        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 32, 64)       0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 32, 64)       16576       skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 32, 64)       0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 32, 64)       8448        skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 32, 64)       0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 32, 64)       13194       skip_latent_mlp_0_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 32, 64)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 32, 64)       8448        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 32, 64)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 32, 64)       16576       skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 32, 64)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 32, 64)       8448        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 32, 64)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 32, 64)       16576       skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 32, 64)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 32, 64)       8448        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 32, 64)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 32, 64)       16576       skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 32, 64)       0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 32, 64)       8448        skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 32, 64)       0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 32, 64)       16576       skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 32, 64)       0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 32, 64)       8448        skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 32, 64)       0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 64)        0           skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 64)        128         avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        650         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 246,302\n",
            "Trainable params: 246,302\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 63s 107ms/step - loss: 2.2985 - sparse_categorical_accuracy: 0.1408 - val_loss: 1.9587 - val_sparse_categorical_accuracy: 0.2720\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.9526 - sparse_categorical_accuracy: 0.2785 - val_loss: 1.8416 - val_sparse_categorical_accuracy: 0.3281\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.8337 - sparse_categorical_accuracy: 0.3273 - val_loss: 1.7286 - val_sparse_categorical_accuracy: 0.3718\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.7360 - sparse_categorical_accuracy: 0.3689 - val_loss: 1.6638 - val_sparse_categorical_accuracy: 0.3942\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.6685 - sparse_categorical_accuracy: 0.3920 - val_loss: 1.5870 - val_sparse_categorical_accuracy: 0.4232\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.6062 - sparse_categorical_accuracy: 0.4227 - val_loss: 1.5558 - val_sparse_categorical_accuracy: 0.4396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 [==============================] - 17s 89ms/step - loss: 1.5604 - sparse_categorical_accuracy: 0.4326 - val_loss: 1.5015 - val_sparse_categorical_accuracy: 0.4621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.5233 - sparse_categorical_accuracy: 0.4515 - val_loss: 1.4672 - val_sparse_categorical_accuracy: 0.4577\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.4954 - sparse_categorical_accuracy: 0.4607 - val_loss: 1.4509 - val_sparse_categorical_accuracy: 0.4785\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.4568 - sparse_categorical_accuracy: 0.4740 - val_loss: 1.4170 - val_sparse_categorical_accuracy: 0.4843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.4307 - sparse_categorical_accuracy: 0.4842 - val_loss: 1.4085 - val_sparse_categorical_accuracy: 0.4903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.4075 - sparse_categorical_accuracy: 0.4910 - val_loss: 1.3719 - val_sparse_categorical_accuracy: 0.5054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.3825 - sparse_categorical_accuracy: 0.5038 - val_loss: 1.3825 - val_sparse_categorical_accuracy: 0.5082\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.3591 - sparse_categorical_accuracy: 0.5122 - val_loss: 1.3554 - val_sparse_categorical_accuracy: 0.5138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.3339 - sparse_categorical_accuracy: 0.5168 - val_loss: 1.3249 - val_sparse_categorical_accuracy: 0.5270\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.3076 - sparse_categorical_accuracy: 0.5281 - val_loss: 1.3026 - val_sparse_categorical_accuracy: 0.5315\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.2998 - sparse_categorical_accuracy: 0.5311 - val_loss: 1.3589 - val_sparse_categorical_accuracy: 0.5123\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.3006 - sparse_categorical_accuracy: 0.5268 - val_loss: 1.2989 - val_sparse_categorical_accuracy: 0.5351\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.2745 - sparse_categorical_accuracy: 0.5421 - val_loss: 1.2850 - val_sparse_categorical_accuracy: 0.5449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.2483 - sparse_categorical_accuracy: 0.5532 - val_loss: 1.2862 - val_sparse_categorical_accuracy: 0.5420\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 17s 84ms/step - loss: 1.2488 - sparse_categorical_accuracy: 0.5491 - val_loss: 1.2805 - val_sparse_categorical_accuracy: 0.5422\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 17s 84ms/step - loss: 1.2283 - sparse_categorical_accuracy: 0.5570 - val_loss: 1.2999 - val_sparse_categorical_accuracy: 0.5385\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 16s 83ms/step - loss: 1.2147 - sparse_categorical_accuracy: 0.5646 - val_loss: 1.2747 - val_sparse_categorical_accuracy: 0.5462\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.1982 - sparse_categorical_accuracy: 0.5694 - val_loss: 1.2703 - val_sparse_categorical_accuracy: 0.5424\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.2097 - sparse_categorical_accuracy: 0.5627 - val_loss: 1.2614 - val_sparse_categorical_accuracy: 0.5552\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26/50\n",
            "196/196 [==============================] - 17s 84ms/step - loss: 1.1872 - sparse_categorical_accuracy: 0.5785 - val_loss: 1.2656 - val_sparse_categorical_accuracy: 0.5509\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.1752 - sparse_categorical_accuracy: 0.5773 - val_loss: 1.2467 - val_sparse_categorical_accuracy: 0.5573\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.1744 - sparse_categorical_accuracy: 0.5771 - val_loss: 1.2348 - val_sparse_categorical_accuracy: 0.5611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 [==============================] - 17s 88ms/step - loss: 1.1508 - sparse_categorical_accuracy: 0.5863 - val_loss: 1.2423 - val_sparse_categorical_accuracy: 0.5564\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.1408 - sparse_categorical_accuracy: 0.5928 - val_loss: 1.2229 - val_sparse_categorical_accuracy: 0.5683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.1365 - sparse_categorical_accuracy: 0.5932 - val_loss: 1.2345 - val_sparse_categorical_accuracy: 0.5672\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.1226 - sparse_categorical_accuracy: 0.5978 - val_loss: 1.2347 - val_sparse_categorical_accuracy: 0.5634\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.1265 - sparse_categorical_accuracy: 0.5948 - val_loss: 1.2349 - val_sparse_categorical_accuracy: 0.5623\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.1156 - sparse_categorical_accuracy: 0.6017 - val_loss: 1.2352 - val_sparse_categorical_accuracy: 0.5654\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.1047 - sparse_categorical_accuracy: 0.6045 - val_loss: 1.2276 - val_sparse_categorical_accuracy: 0.5645\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.0902 - sparse_categorical_accuracy: 0.6055 - val_loss: 1.2472 - val_sparse_categorical_accuracy: 0.5673\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.0897 - sparse_categorical_accuracy: 0.6114 - val_loss: 1.2333 - val_sparse_categorical_accuracy: 0.5694\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.0843 - sparse_categorical_accuracy: 0.6096 - val_loss: 1.2738 - val_sparse_categorical_accuracy: 0.5639\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.0717 - sparse_categorical_accuracy: 0.6142 - val_loss: 1.2300 - val_sparse_categorical_accuracy: 0.5700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.0630 - sparse_categorical_accuracy: 0.6174 - val_loss: 1.2386 - val_sparse_categorical_accuracy: 0.5672\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.0578 - sparse_categorical_accuracy: 0.6212 - val_loss: 1.2461 - val_sparse_categorical_accuracy: 0.5661\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 17s 87ms/step - loss: 1.0448 - sparse_categorical_accuracy: 0.6285 - val_loss: 1.2197 - val_sparse_categorical_accuracy: 0.5766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.0365 - sparse_categorical_accuracy: 0.6279 - val_loss: 1.2336 - val_sparse_categorical_accuracy: 0.5747\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.0399 - sparse_categorical_accuracy: 0.6273 - val_loss: 1.2171 - val_sparse_categorical_accuracy: 0.5765\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.0208 - sparse_categorical_accuracy: 0.6327 - val_loss: 1.2314 - val_sparse_categorical_accuracy: 0.5724\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.0098 - sparse_categorical_accuracy: 0.6372 - val_loss: 1.2284 - val_sparse_categorical_accuracy: 0.5679\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 1.0089 - sparse_categorical_accuracy: 0.6391 - val_loss: 1.2405 - val_sparse_categorical_accuracy: 0.5771\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 48/50\n",
            "196/196 [==============================] - 17s 86ms/step - loss: 1.0036 - sparse_categorical_accuracy: 0.6392 - val_loss: 1.2312 - val_sparse_categorical_accuracy: 0.5758\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 0.9980 - sparse_categorical_accuracy: 0.6416 - val_loss: 1.2228 - val_sparse_categorical_accuracy: 0.5831\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50/50\n",
            "196/196 [==============================] - 17s 85ms/step - loss: 0.9899 - sparse_categorical_accuracy: 0.6388 - val_loss: 1.2417 - val_sparse_categorical_accuracy: 0.5680\n",
            "Total training time 1516.595031261444 seconds\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 64, 64)       4096        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 37)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 64, 64)       13194       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 64, 64)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 64, 64)       8448        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 64, 64)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 64, 64)       16576       skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 64, 64)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 64, 64)       8448        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 64, 64)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 64, 64)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 64, 64)       8448        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 64, 64)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 64, 64)       0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 64, 64)       8448        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 64, 64)       0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 64, 64)       0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 64, 64)       8448        skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 64, 64)       0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 64, 64)       13194       skip_latent_mlp_0_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 64, 64)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 64, 64)       8448        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 64, 64)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 64, 64)       16576       skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 64, 64)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 64, 64)       8448        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 64, 64)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 64, 64)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 64, 64)       8448        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 64, 64)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 64, 64)       0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 64, 64)       8448        skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 64, 64)       0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 64, 64)       0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 64, 64)       8448        skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 64, 64)       0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 64)        0           skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 64)        128         avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        650         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 248,350\n",
            "Trainable params: 248,350\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 40s 149ms/step - loss: 2.3342 - sparse_categorical_accuracy: 0.1459 - val_loss: 1.9447 - val_sparse_categorical_accuracy: 0.2726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.9243 - sparse_categorical_accuracy: 0.2853 - val_loss: 1.7814 - val_sparse_categorical_accuracy: 0.3484\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.7843 - sparse_categorical_accuracy: 0.3516 - val_loss: 1.7364 - val_sparse_categorical_accuracy: 0.3717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 [==============================] - 24s 124ms/step - loss: 1.7062 - sparse_categorical_accuracy: 0.3817 - val_loss: 1.6882 - val_sparse_categorical_accuracy: 0.3950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 [==============================] - 25s 126ms/step - loss: 1.6541 - sparse_categorical_accuracy: 0.3960 - val_loss: 1.6374 - val_sparse_categorical_accuracy: 0.4187\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 [==============================] - 24s 125ms/step - loss: 1.6046 - sparse_categorical_accuracy: 0.4165 - val_loss: 1.5590 - val_sparse_categorical_accuracy: 0.4430\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.5740 - sparse_categorical_accuracy: 0.4338 - val_loss: 1.5220 - val_sparse_categorical_accuracy: 0.4528\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 [==============================] - 25s 126ms/step - loss: 1.5212 - sparse_categorical_accuracy: 0.4488 - val_loss: 1.4973 - val_sparse_categorical_accuracy: 0.4555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.4877 - sparse_categorical_accuracy: 0.4631 - val_loss: 1.4716 - val_sparse_categorical_accuracy: 0.4700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 [==============================] - 25s 126ms/step - loss: 1.4454 - sparse_categorical_accuracy: 0.4773 - val_loss: 1.4802 - val_sparse_categorical_accuracy: 0.4717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.4125 - sparse_categorical_accuracy: 0.4901 - val_loss: 1.3931 - val_sparse_categorical_accuracy: 0.5014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.3716 - sparse_categorical_accuracy: 0.5101 - val_loss: 1.3981 - val_sparse_categorical_accuracy: 0.5017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.3559 - sparse_categorical_accuracy: 0.5093 - val_loss: 1.3479 - val_sparse_categorical_accuracy: 0.5190\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.3249 - sparse_categorical_accuracy: 0.5212 - val_loss: 1.3327 - val_sparse_categorical_accuracy: 0.5246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.3071 - sparse_categorical_accuracy: 0.5292 - val_loss: 1.3360 - val_sparse_categorical_accuracy: 0.5177\n",
            "Epoch 16/50\n",
            "196/196 [==============================] - 25s 126ms/step - loss: 1.2813 - sparse_categorical_accuracy: 0.5395 - val_loss: 1.3739 - val_sparse_categorical_accuracy: 0.5175\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.2700 - sparse_categorical_accuracy: 0.5439 - val_loss: 1.2751 - val_sparse_categorical_accuracy: 0.5448\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 [==============================] - 24s 124ms/step - loss: 1.2300 - sparse_categorical_accuracy: 0.5563 - val_loss: 1.3072 - val_sparse_categorical_accuracy: 0.5375\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.2324 - sparse_categorical_accuracy: 0.5574 - val_loss: 1.3008 - val_sparse_categorical_accuracy: 0.5360\n",
            "Epoch 20/50\n",
            "196/196 [==============================] - 24s 124ms/step - loss: 1.2106 - sparse_categorical_accuracy: 0.5665 - val_loss: 1.3131 - val_sparse_categorical_accuracy: 0.5356\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 24s 123ms/step - loss: 1.2010 - sparse_categorical_accuracy: 0.5689 - val_loss: 1.2935 - val_sparse_categorical_accuracy: 0.5366\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 24s 124ms/step - loss: 1.1792 - sparse_categorical_accuracy: 0.5763 - val_loss: 1.2614 - val_sparse_categorical_accuracy: 0.5479\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.1661 - sparse_categorical_accuracy: 0.5811 - val_loss: 1.2693 - val_sparse_categorical_accuracy: 0.5443\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 25s 126ms/step - loss: 1.1619 - sparse_categorical_accuracy: 0.5821 - val_loss: 1.2718 - val_sparse_categorical_accuracy: 0.5519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 [==============================] - 24s 123ms/step - loss: 1.1428 - sparse_categorical_accuracy: 0.5889 - val_loss: 1.2797 - val_sparse_categorical_accuracy: 0.5467\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 24s 124ms/step - loss: 1.1135 - sparse_categorical_accuracy: 0.6023 - val_loss: 1.2385 - val_sparse_categorical_accuracy: 0.5631\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.1158 - sparse_categorical_accuracy: 0.5952 - val_loss: 1.2284 - val_sparse_categorical_accuracy: 0.5700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.1003 - sparse_categorical_accuracy: 0.6074 - val_loss: 1.2064 - val_sparse_categorical_accuracy: 0.5685\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.0948 - sparse_categorical_accuracy: 0.6102 - val_loss: 1.2173 - val_sparse_categorical_accuracy: 0.5655\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 24s 124ms/step - loss: 1.0825 - sparse_categorical_accuracy: 0.6141 - val_loss: 1.2313 - val_sparse_categorical_accuracy: 0.5669\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.0645 - sparse_categorical_accuracy: 0.6184 - val_loss: 1.2557 - val_sparse_categorical_accuracy: 0.5627\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.0705 - sparse_categorical_accuracy: 0.6154 - val_loss: 1.2223 - val_sparse_categorical_accuracy: 0.5674\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 24s 125ms/step - loss: 1.0389 - sparse_categorical_accuracy: 0.6269 - val_loss: 1.2205 - val_sparse_categorical_accuracy: 0.5713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "196/196 [==============================] - 24s 124ms/step - loss: 1.0259 - sparse_categorical_accuracy: 0.6365 - val_loss: 1.2258 - val_sparse_categorical_accuracy: 0.5685\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.0247 - sparse_categorical_accuracy: 0.6302 - val_loss: 1.2355 - val_sparse_categorical_accuracy: 0.5682\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 24s 123ms/step - loss: 1.0205 - sparse_categorical_accuracy: 0.6353 - val_loss: 1.2190 - val_sparse_categorical_accuracy: 0.5743\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 1.0045 - sparse_categorical_accuracy: 0.6397 - val_loss: 1.2373 - val_sparse_categorical_accuracy: 0.5664\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 25s 126ms/step - loss: 0.9952 - sparse_categorical_accuracy: 0.6457 - val_loss: 1.2381 - val_sparse_categorical_accuracy: 0.5751\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39/50\n",
            "196/196 [==============================] - 24s 124ms/step - loss: 0.9855 - sparse_categorical_accuracy: 0.6436 - val_loss: 1.2472 - val_sparse_categorical_accuracy: 0.5703\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 0.9717 - sparse_categorical_accuracy: 0.6521 - val_loss: 1.2625 - val_sparse_categorical_accuracy: 0.5659\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 24s 124ms/step - loss: 0.9716 - sparse_categorical_accuracy: 0.6558 - val_loss: 1.2241 - val_sparse_categorical_accuracy: 0.5764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 42/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 0.9601 - sparse_categorical_accuracy: 0.6544 - val_loss: 1.2322 - val_sparse_categorical_accuracy: 0.5743\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 24s 124ms/step - loss: 0.9524 - sparse_categorical_accuracy: 0.6574 - val_loss: 1.2385 - val_sparse_categorical_accuracy: 0.5740\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 24s 124ms/step - loss: 0.9410 - sparse_categorical_accuracy: 0.6592 - val_loss: 1.2241 - val_sparse_categorical_accuracy: 0.5847\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_20_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_64l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 45/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 0.9302 - sparse_categorical_accuracy: 0.6680 - val_loss: 1.2497 - val_sparse_categorical_accuracy: 0.5660\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 24s 124ms/step - loss: 0.9172 - sparse_categorical_accuracy: 0.6712 - val_loss: 1.2380 - val_sparse_categorical_accuracy: 0.5805\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 24s 124ms/step - loss: 0.9241 - sparse_categorical_accuracy: 0.6680 - val_loss: 1.2290 - val_sparse_categorical_accuracy: 0.5736\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 0.9050 - sparse_categorical_accuracy: 0.6772 - val_loss: 1.2633 - val_sparse_categorical_accuracy: 0.5735\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 0.8921 - sparse_categorical_accuracy: 0.6798 - val_loss: 1.2505 - val_sparse_categorical_accuracy: 0.5763\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 25s 125ms/step - loss: 0.8898 - sparse_categorical_accuracy: 0.6785 - val_loss: 1.2364 - val_sparse_categorical_accuracy: 0.5747\n",
            "Total training time 1805.513309955597 seconds\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 128, 64)      8192        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 37)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 128, 64)      13194       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 128, 64)      0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 128, 64)      8448        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 128, 64)      0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 128, 64)      16576       skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 128, 64)      0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 128, 64)      8448        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 128, 64)      0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 128, 64)      0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 128, 64)      8448        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 128, 64)      0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 128, 64)      0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 128, 64)      8448        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 128, 64)      0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 128, 64)      0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 128, 64)      8448        skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 128, 64)      0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 128, 64)      13194       skip_latent_mlp_0_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 128, 64)      0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 128, 64)      8448        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 128, 64)      0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 128, 64)      16576       skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 128, 64)      0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 128, 64)      8448        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 128, 64)      0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 128, 64)      0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 128, 64)      8448        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 128, 64)      0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 128, 64)      0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 128, 64)      8448        skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 128, 64)      0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 128, 64)      0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 128, 64)      8448        skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 128, 64)      0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 64)        0           skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 64)        128         avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        650         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 252,446\n",
            "Trainable params: 252,446\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 58s 229ms/step - loss: 2.2374 - sparse_categorical_accuracy: 0.1677 - val_loss: 1.9043 - val_sparse_categorical_accuracy: 0.2924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 1.8715 - sparse_categorical_accuracy: 0.3129 - val_loss: 1.7920 - val_sparse_categorical_accuracy: 0.3542\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.7649 - sparse_categorical_accuracy: 0.3528 - val_loss: 1.7410 - val_sparse_categorical_accuracy: 0.3690\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.6911 - sparse_categorical_accuracy: 0.3815 - val_loss: 1.6571 - val_sparse_categorical_accuracy: 0.4029\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.6338 - sparse_categorical_accuracy: 0.4106 - val_loss: 1.6133 - val_sparse_categorical_accuracy: 0.4177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.5869 - sparse_categorical_accuracy: 0.4283 - val_loss: 1.5825 - val_sparse_categorical_accuracy: 0.4355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.5420 - sparse_categorical_accuracy: 0.4385 - val_loss: 1.5166 - val_sparse_categorical_accuracy: 0.4545\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.5012 - sparse_categorical_accuracy: 0.4578 - val_loss: 1.4887 - val_sparse_categorical_accuracy: 0.4657\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.4676 - sparse_categorical_accuracy: 0.4735 - val_loss: 1.4778 - val_sparse_categorical_accuracy: 0.4735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.4610 - sparse_categorical_accuracy: 0.4729 - val_loss: 1.4365 - val_sparse_categorical_accuracy: 0.4864\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.4140 - sparse_categorical_accuracy: 0.4883 - val_loss: 1.4463 - val_sparse_categorical_accuracy: 0.4813\n",
            "Epoch 12/50\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.3944 - sparse_categorical_accuracy: 0.4970 - val_loss: 1.4045 - val_sparse_categorical_accuracy: 0.4989\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.3639 - sparse_categorical_accuracy: 0.5082 - val_loss: 1.4051 - val_sparse_categorical_accuracy: 0.4975\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 1.3404 - sparse_categorical_accuracy: 0.5208 - val_loss: 1.3961 - val_sparse_categorical_accuracy: 0.5029\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.3259 - sparse_categorical_accuracy: 0.5224 - val_loss: 1.3907 - val_sparse_categorical_accuracy: 0.5072\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.3030 - sparse_categorical_accuracy: 0.5292 - val_loss: 1.3590 - val_sparse_categorical_accuracy: 0.5150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.2856 - sparse_categorical_accuracy: 0.5368 - val_loss: 1.3210 - val_sparse_categorical_accuracy: 0.5273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.2563 - sparse_categorical_accuracy: 0.5488 - val_loss: 1.3556 - val_sparse_categorical_accuracy: 0.5224\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 1.2444 - sparse_categorical_accuracy: 0.5522 - val_loss: 1.2936 - val_sparse_categorical_accuracy: 0.5328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.2266 - sparse_categorical_accuracy: 0.5617 - val_loss: 1.2920 - val_sparse_categorical_accuracy: 0.5390\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.2052 - sparse_categorical_accuracy: 0.5693 - val_loss: 1.3115 - val_sparse_categorical_accuracy: 0.5349\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.1965 - sparse_categorical_accuracy: 0.5707 - val_loss: 1.3092 - val_sparse_categorical_accuracy: 0.5411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.1679 - sparse_categorical_accuracy: 0.5815 - val_loss: 1.2717 - val_sparse_categorical_accuracy: 0.5463\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "196/196 [==============================] - 40s 206ms/step - loss: 1.1681 - sparse_categorical_accuracy: 0.5820 - val_loss: 1.2772 - val_sparse_categorical_accuracy: 0.5487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.1436 - sparse_categorical_accuracy: 0.5862 - val_loss: 1.2802 - val_sparse_categorical_accuracy: 0.5473\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1345 - sparse_categorical_accuracy: 0.5909 - val_loss: 1.3112 - val_sparse_categorical_accuracy: 0.5422\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 40s 206ms/step - loss: 1.1216 - sparse_categorical_accuracy: 0.5950 - val_loss: 1.2966 - val_sparse_categorical_accuracy: 0.5444\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.1203 - sparse_categorical_accuracy: 0.5969 - val_loss: 1.2862 - val_sparse_categorical_accuracy: 0.5487\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 40s 206ms/step - loss: 1.0945 - sparse_categorical_accuracy: 0.6088 - val_loss: 1.2946 - val_sparse_categorical_accuracy: 0.5468\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.0895 - sparse_categorical_accuracy: 0.6093 - val_loss: 1.2274 - val_sparse_categorical_accuracy: 0.5612\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.0762 - sparse_categorical_accuracy: 0.6137 - val_loss: 1.2897 - val_sparse_categorical_accuracy: 0.5473\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.0565 - sparse_categorical_accuracy: 0.6193 - val_loss: 1.2919 - val_sparse_categorical_accuracy: 0.5543\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.0443 - sparse_categorical_accuracy: 0.6251 - val_loss: 1.2581 - val_sparse_categorical_accuracy: 0.5612\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.0389 - sparse_categorical_accuracy: 0.6245 - val_loss: 1.2683 - val_sparse_categorical_accuracy: 0.5595\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.0321 - sparse_categorical_accuracy: 0.6286 - val_loss: 1.2510 - val_sparse_categorical_accuracy: 0.5637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36/50\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.0121 - sparse_categorical_accuracy: 0.6340 - val_loss: 1.2928 - val_sparse_categorical_accuracy: 0.5515\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 0.9905 - sparse_categorical_accuracy: 0.6441 - val_loss: 1.2648 - val_sparse_categorical_accuracy: 0.5701\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 0.9889 - sparse_categorical_accuracy: 0.6429 - val_loss: 1.2623 - val_sparse_categorical_accuracy: 0.5653\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 0.9700 - sparse_categorical_accuracy: 0.6517 - val_loss: 1.2703 - val_sparse_categorical_accuracy: 0.5629\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 0.9655 - sparse_categorical_accuracy: 0.6548 - val_loss: 1.2510 - val_sparse_categorical_accuracy: 0.5722\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 41/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 0.9545 - sparse_categorical_accuracy: 0.6556 - val_loss: 1.3238 - val_sparse_categorical_accuracy: 0.5546\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 0.9574 - sparse_categorical_accuracy: 0.6571 - val_loss: 1.3040 - val_sparse_categorical_accuracy: 0.5575\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 0.9459 - sparse_categorical_accuracy: 0.6603 - val_loss: 1.2985 - val_sparse_categorical_accuracy: 0.5619\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 0.9245 - sparse_categorical_accuracy: 0.6667 - val_loss: 1.2905 - val_sparse_categorical_accuracy: 0.5677\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 40s 202ms/step - loss: 0.9062 - sparse_categorical_accuracy: 0.6758 - val_loss: 1.3032 - val_sparse_categorical_accuracy: 0.5643\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 0.8938 - sparse_categorical_accuracy: 0.6762 - val_loss: 1.3017 - val_sparse_categorical_accuracy: 0.5592\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 40s 202ms/step - loss: 0.8981 - sparse_categorical_accuracy: 0.6751 - val_loss: 1.3060 - val_sparse_categorical_accuracy: 0.5588\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 0.8909 - sparse_categorical_accuracy: 0.6771 - val_loss: 1.3012 - val_sparse_categorical_accuracy: 0.5596\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 40s 202ms/step - loss: 0.8749 - sparse_categorical_accuracy: 0.6859 - val_loss: 1.3255 - val_sparse_categorical_accuracy: 0.5631\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 40s 202ms/step - loss: 0.8689 - sparse_categorical_accuracy: 0.6909 - val_loss: 1.3535 - val_sparse_categorical_accuracy: 0.5534\n",
            "Total training time 2584.624345064163 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhazXpbguGqN"
      },
      "source": [
        "Different number of attends. Doesnt help."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keF_npgts3aZ",
        "outputId": "4a73fb1f-f12b-4a70-a948-0dae3beaa475"
      },
      "source": [
        "for num_latents in [128]:\n",
        "  for latent_dim in [64]:\n",
        "    for latent_head_dim in [64]:\n",
        "      for cross_head_dim in [64]:\n",
        "        for cross_depth in [4,8]:\n",
        "          model = Perceiver(num_classes=num_classes, \n",
        "                            image_height=image_height, \n",
        "                            image_width=image_width,\n",
        "                            max_freq=8,\n",
        "                            num_freq_bands=8,\n",
        "                            freq_base=2.0,\n",
        "                            num_latents=num_latents,\n",
        "                            latent_dim=latent_dim,\n",
        "                            latent_heads=1,\n",
        "                            latent_head_dim=latent_head_dim,\n",
        "                            cross_heads=1,\n",
        "                            cross_head_dim=cross_head_dim,\n",
        "                            attn_dropout=0.2,\n",
        "                            mlp_dropout=0.2,\n",
        "                            cross_depth=cross_depth,\n",
        "                            latent_depth=4,             \n",
        "                            weight_tie_layers=True,\n",
        "                            preprocess=False).model()\n",
        "          model.summary()\n",
        "          train_and_eval(model, SAVE_PATH + f'_PERCEIVER_8fq_8fqb_{num_latents}l_{latent_dim}ldim_1lh_{latent_head_dim}lhdim_1ch_{cross_head_dim}chdim_{cross_depth}cd_4ld', verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 128, 64)      8192        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 37)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 128, 64)      13194       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 128, 64)      0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 128, 64)      8448        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 128, 64)      0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 128, 64)      16576       skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 128, 64)      0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 128, 64)      8448        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 128, 64)      0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 128, 64)      0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 128, 64)      8448        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 128, 64)      0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 128, 64)      0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 128, 64)      8448        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 128, 64)      0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 128, 64)      0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 128, 64)      8448        skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 128, 64)      0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 128, 64)      13194       skip_latent_mlp_0_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_1_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 128, 64)      0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 128, 64)      8448        skip_cross_attn_1[0][0]          \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "                                                                 skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 128, 64)      0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 128, 64)      16576       skip_cross_mlp_1[0][0]           \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 128, 64)      0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 128, 64)      8448        skip_latent_attn_1_0[0][0]       \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "                                                                 skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 128, 64)      0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_1_0[0][0]        \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 128, 64)      0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 128, 64)      8448        skip_latent_attn_1_1[0][0]       \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "                                                                 skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 128, 64)      0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_1_1[0][0]        \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 128, 64)      0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 128, 64)      8448        skip_latent_attn_1_2[0][0]       \n",
            "                                                                 skip_latent_attn_2_2[0][0]       \n",
            "                                                                 skip_latent_attn_3_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 128, 64)      0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_1_2[0][0]        \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "                                                                 skip_latent_mlp_3_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 128, 64)      0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 128, 64)      8448        skip_latent_attn_1_3[0][0]       \n",
            "                                                                 skip_latent_attn_2_3[0][0]       \n",
            "                                                                 skip_latent_attn_3_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 128, 64)      0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_2 (Add)         (None, 128, 64)      0           cross_attn_1[1][0]               \n",
            "                                                                 skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_2 (Add)          (None, 128, 64)      0           cross_mlp_1[1][0]                \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_0 (Add)      (None, 128, 64)      0           latent_attn_1_0[1][0]            \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_0 (Add)       (None, 128, 64)      0           latent_mlp_1_0[1][0]             \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_1 (Add)      (None, 128, 64)      0           latent_attn_1_1[1][0]            \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_1 (Add)       (None, 128, 64)      0           latent_mlp_1_1[1][0]             \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_2 (Add)      (None, 128, 64)      0           latent_attn_1_2[1][0]            \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_2 (Add)       (None, 128, 64)      0           latent_mlp_1_2[1][0]             \n",
            "                                                                 skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_3 (Add)      (None, 128, 64)      0           latent_attn_1_3[1][0]            \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_3 (Add)       (None, 128, 64)      0           latent_mlp_1_3[1][0]             \n",
            "                                                                 skip_latent_attn_2_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_3 (Add)         (None, 128, 64)      0           cross_attn_1[2][0]               \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_3 (Add)          (None, 128, 64)      0           cross_mlp_1[2][0]                \n",
            "                                                                 skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_0 (Add)      (None, 128, 64)      0           latent_attn_1_0[2][0]            \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_0 (Add)       (None, 128, 64)      0           latent_mlp_1_0[2][0]             \n",
            "                                                                 skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_1 (Add)      (None, 128, 64)      0           latent_attn_1_1[2][0]            \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_1 (Add)       (None, 128, 64)      0           latent_mlp_1_1[2][0]             \n",
            "                                                                 skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_2 (Add)      (None, 128, 64)      0           latent_attn_1_2[2][0]            \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_2 (Add)       (None, 128, 64)      0           latent_mlp_1_2[2][0]             \n",
            "                                                                 skip_latent_attn_3_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_3 (Add)      (None, 128, 64)      0           latent_attn_1_3[2][0]            \n",
            "                                                                 skip_latent_mlp_3_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_3 (Add)       (None, 128, 64)      0           latent_mlp_1_3[2][0]             \n",
            "                                                                 skip_latent_attn_3_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 64)        0           skip_latent_mlp_3_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 64)        128         avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        650         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 252,446\n",
            "Trainable params: 252,446\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 107s 432ms/step - loss: 2.3062 - sparse_categorical_accuracy: 0.1389 - val_loss: 2.0130 - val_sparse_categorical_accuracy: 0.2384\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 [==============================] - 77s 392ms/step - loss: 1.9551 - sparse_categorical_accuracy: 0.2705 - val_loss: 1.8299 - val_sparse_categorical_accuracy: 0.3332\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 [==============================] - 77s 392ms/step - loss: 1.8308 - sparse_categorical_accuracy: 0.3313 - val_loss: 1.7708 - val_sparse_categorical_accuracy: 0.3478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 [==============================] - 77s 392ms/step - loss: 1.7486 - sparse_categorical_accuracy: 0.3593 - val_loss: 1.7053 - val_sparse_categorical_accuracy: 0.3802\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 [==============================] - 77s 393ms/step - loss: 1.6828 - sparse_categorical_accuracy: 0.3834 - val_loss: 1.6112 - val_sparse_categorical_accuracy: 0.4081\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "196/196 [==============================] - 77s 395ms/step - loss: 1.6122 - sparse_categorical_accuracy: 0.4091 - val_loss: 1.5587 - val_sparse_categorical_accuracy: 0.4300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 [==============================] - 77s 395ms/step - loss: 1.5698 - sparse_categorical_accuracy: 0.4293 - val_loss: 1.5516 - val_sparse_categorical_accuracy: 0.4440\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.5251 - sparse_categorical_accuracy: 0.4440 - val_loss: 1.4648 - val_sparse_categorical_accuracy: 0.4717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.4706 - sparse_categorical_accuracy: 0.4689 - val_loss: 1.4376 - val_sparse_categorical_accuracy: 0.4794\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.4346 - sparse_categorical_accuracy: 0.4794 - val_loss: 1.4386 - val_sparse_categorical_accuracy: 0.4788\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.4019 - sparse_categorical_accuracy: 0.4923 - val_loss: 1.3684 - val_sparse_categorical_accuracy: 0.5055\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 [==============================] - 77s 395ms/step - loss: 1.3680 - sparse_categorical_accuracy: 0.5059 - val_loss: 1.3668 - val_sparse_categorical_accuracy: 0.5077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.3326 - sparse_categorical_accuracy: 0.5167 - val_loss: 1.3232 - val_sparse_categorical_accuracy: 0.5243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "196/196 [==============================] - 77s 395ms/step - loss: 1.3166 - sparse_categorical_accuracy: 0.5214 - val_loss: 1.3647 - val_sparse_categorical_accuracy: 0.5184\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 77s 393ms/step - loss: 1.2946 - sparse_categorical_accuracy: 0.5340 - val_loss: 1.3285 - val_sparse_categorical_accuracy: 0.5304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.2757 - sparse_categorical_accuracy: 0.5420 - val_loss: 1.3270 - val_sparse_categorical_accuracy: 0.5271\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 77s 393ms/step - loss: 1.2488 - sparse_categorical_accuracy: 0.5541 - val_loss: 1.3044 - val_sparse_categorical_accuracy: 0.5339\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.2387 - sparse_categorical_accuracy: 0.5552 - val_loss: 1.3371 - val_sparse_categorical_accuracy: 0.5290\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.2213 - sparse_categorical_accuracy: 0.5627 - val_loss: 1.2809 - val_sparse_categorical_accuracy: 0.5420\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.2004 - sparse_categorical_accuracy: 0.5702 - val_loss: 1.2725 - val_sparse_categorical_accuracy: 0.5482\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "196/196 [==============================] - 78s 395ms/step - loss: 1.1948 - sparse_categorical_accuracy: 0.5706 - val_loss: 1.2818 - val_sparse_categorical_accuracy: 0.5469\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.1754 - sparse_categorical_accuracy: 0.5790 - val_loss: 1.2525 - val_sparse_categorical_accuracy: 0.5592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "196/196 [==============================] - 77s 393ms/step - loss: 1.1502 - sparse_categorical_accuracy: 0.5888 - val_loss: 1.2649 - val_sparse_categorical_accuracy: 0.5505\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 77s 393ms/step - loss: 1.1338 - sparse_categorical_accuracy: 0.5953 - val_loss: 1.2517 - val_sparse_categorical_accuracy: 0.5547\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 77s 393ms/step - loss: 1.1182 - sparse_categorical_accuracy: 0.6010 - val_loss: 1.2723 - val_sparse_categorical_accuracy: 0.5536\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 77s 393ms/step - loss: 1.1186 - sparse_categorical_accuracy: 0.5974 - val_loss: 1.2469 - val_sparse_categorical_accuracy: 0.5656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.1039 - sparse_categorical_accuracy: 0.6050 - val_loss: 1.2583 - val_sparse_categorical_accuracy: 0.5613\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.0938 - sparse_categorical_accuracy: 0.6093 - val_loss: 1.2397 - val_sparse_categorical_accuracy: 0.5667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.0678 - sparse_categorical_accuracy: 0.6152 - val_loss: 1.2377 - val_sparse_categorical_accuracy: 0.5684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n",
            "196/196 [==============================] - 78s 396ms/step - loss: 1.0699 - sparse_categorical_accuracy: 0.6147 - val_loss: 1.2359 - val_sparse_categorical_accuracy: 0.5671\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.0524 - sparse_categorical_accuracy: 0.6199 - val_loss: 1.2176 - val_sparse_categorical_accuracy: 0.5707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.0386 - sparse_categorical_accuracy: 0.6289 - val_loss: 1.2409 - val_sparse_categorical_accuracy: 0.5747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33/50\n",
            "196/196 [==============================] - 77s 394ms/step - loss: 1.0252 - sparse_categorical_accuracy: 0.6342 - val_loss: 1.2774 - val_sparse_categorical_accuracy: 0.5581\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 77s 392ms/step - loss: 1.0207 - sparse_categorical_accuracy: 0.6368 - val_loss: 1.2581 - val_sparse_categorical_accuracy: 0.5625\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 77s 392ms/step - loss: 1.0074 - sparse_categorical_accuracy: 0.6376 - val_loss: 1.2363 - val_sparse_categorical_accuracy: 0.5719\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 77s 392ms/step - loss: 0.9929 - sparse_categorical_accuracy: 0.6445 - val_loss: 1.2312 - val_sparse_categorical_accuracy: 0.5777\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_60_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_4cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37/50\n",
            "196/196 [==============================] - 77s 393ms/step - loss: 0.9708 - sparse_categorical_accuracy: 0.6539 - val_loss: 1.2798 - val_sparse_categorical_accuracy: 0.5613\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 77s 392ms/step - loss: 0.9663 - sparse_categorical_accuracy: 0.6588 - val_loss: 1.2732 - val_sparse_categorical_accuracy: 0.5654\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 77s 393ms/step - loss: 0.9579 - sparse_categorical_accuracy: 0.6587 - val_loss: 1.2632 - val_sparse_categorical_accuracy: 0.5657\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 77s 392ms/step - loss: 0.9430 - sparse_categorical_accuracy: 0.6650 - val_loss: 1.2346 - val_sparse_categorical_accuracy: 0.5760\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 77s 393ms/step - loss: 0.9199 - sparse_categorical_accuracy: 0.6737 - val_loss: 1.2596 - val_sparse_categorical_accuracy: 0.5710\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 77s 392ms/step - loss: 0.9263 - sparse_categorical_accuracy: 0.6681 - val_loss: 1.2513 - val_sparse_categorical_accuracy: 0.5724\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 77s 392ms/step - loss: 0.9023 - sparse_categorical_accuracy: 0.6808 - val_loss: 1.2843 - val_sparse_categorical_accuracy: 0.5664\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 77s 392ms/step - loss: 0.9000 - sparse_categorical_accuracy: 0.6795 - val_loss: 1.2955 - val_sparse_categorical_accuracy: 0.5692\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 77s 392ms/step - loss: 0.8912 - sparse_categorical_accuracy: 0.6833 - val_loss: 1.2801 - val_sparse_categorical_accuracy: 0.5694\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 77s 393ms/step - loss: 0.8823 - sparse_categorical_accuracy: 0.6815 - val_loss: 1.3186 - val_sparse_categorical_accuracy: 0.5614\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 77s 391ms/step - loss: 0.8644 - sparse_categorical_accuracy: 0.6913 - val_loss: 1.2911 - val_sparse_categorical_accuracy: 0.5660\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 77s 392ms/step - loss: 0.8532 - sparse_categorical_accuracy: 0.6948 - val_loss: 1.3095 - val_sparse_categorical_accuracy: 0.5650\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 77s 393ms/step - loss: 0.8526 - sparse_categorical_accuracy: 0.6949 - val_loss: 1.3106 - val_sparse_categorical_accuracy: 0.5662\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 77s 393ms/step - loss: 0.8312 - sparse_categorical_accuracy: 0.7037 - val_loss: 1.3351 - val_sparse_categorical_accuracy: 0.5616\n",
            "Total training time 4591.413452863693 seconds\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 128, 64)      8192        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 37)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 128, 64)      13194       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 128, 64)      0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 128, 64)      8448        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 128, 64)      0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 128, 64)      16576       skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 128, 64)      0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 128, 64)      8448        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 128, 64)      0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 128, 64)      0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 128, 64)      8448        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 128, 64)      0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 128, 64)      0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 128, 64)      8448        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 128, 64)      0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 128, 64)      0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 128, 64)      8448        skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 128, 64)      0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 128, 64)      13194       skip_latent_mlp_0_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_1_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_3_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_4_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_5_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_6_3[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 128, 64)      0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 128, 64)      8448        skip_cross_attn_1[0][0]          \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "                                                                 skip_cross_attn_3[0][0]          \n",
            "                                                                 skip_cross_attn_4[0][0]          \n",
            "                                                                 skip_cross_attn_5[0][0]          \n",
            "                                                                 skip_cross_attn_6[0][0]          \n",
            "                                                                 skip_cross_attn_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 128, 64)      0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 128, 64)      16576       skip_cross_mlp_1[0][0]           \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "                                                                 skip_cross_mlp_4[0][0]           \n",
            "                                                                 skip_cross_mlp_5[0][0]           \n",
            "                                                                 skip_cross_mlp_6[0][0]           \n",
            "                                                                 skip_cross_mlp_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 128, 64)      0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 128, 64)      8448        skip_latent_attn_1_0[0][0]       \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "                                                                 skip_latent_attn_3_0[0][0]       \n",
            "                                                                 skip_latent_attn_4_0[0][0]       \n",
            "                                                                 skip_latent_attn_5_0[0][0]       \n",
            "                                                                 skip_latent_attn_6_0[0][0]       \n",
            "                                                                 skip_latent_attn_7_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 128, 64)      0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_1_0[0][0]        \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "                                                                 skip_latent_mlp_4_0[0][0]        \n",
            "                                                                 skip_latent_mlp_5_0[0][0]        \n",
            "                                                                 skip_latent_mlp_6_0[0][0]        \n",
            "                                                                 skip_latent_mlp_7_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 128, 64)      0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 128, 64)      8448        skip_latent_attn_1_1[0][0]       \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "                                                                 skip_latent_attn_3_1[0][0]       \n",
            "                                                                 skip_latent_attn_4_1[0][0]       \n",
            "                                                                 skip_latent_attn_5_1[0][0]       \n",
            "                                                                 skip_latent_attn_6_1[0][0]       \n",
            "                                                                 skip_latent_attn_7_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 128, 64)      0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_1_1[0][0]        \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "                                                                 skip_latent_mlp_4_1[0][0]        \n",
            "                                                                 skip_latent_mlp_5_1[0][0]        \n",
            "                                                                 skip_latent_mlp_6_1[0][0]        \n",
            "                                                                 skip_latent_mlp_7_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 128, 64)      0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 128, 64)      8448        skip_latent_attn_1_2[0][0]       \n",
            "                                                                 skip_latent_attn_2_2[0][0]       \n",
            "                                                                 skip_latent_attn_3_2[0][0]       \n",
            "                                                                 skip_latent_attn_4_2[0][0]       \n",
            "                                                                 skip_latent_attn_5_2[0][0]       \n",
            "                                                                 skip_latent_attn_6_2[0][0]       \n",
            "                                                                 skip_latent_attn_7_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 128, 64)      0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_1_2[0][0]        \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "                                                                 skip_latent_mlp_3_2[0][0]        \n",
            "                                                                 skip_latent_mlp_4_2[0][0]        \n",
            "                                                                 skip_latent_mlp_5_2[0][0]        \n",
            "                                                                 skip_latent_mlp_6_2[0][0]        \n",
            "                                                                 skip_latent_mlp_7_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 128, 64)      0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 128, 64)      8448        skip_latent_attn_1_3[0][0]       \n",
            "                                                                 skip_latent_attn_2_3[0][0]       \n",
            "                                                                 skip_latent_attn_3_3[0][0]       \n",
            "                                                                 skip_latent_attn_4_3[0][0]       \n",
            "                                                                 skip_latent_attn_5_3[0][0]       \n",
            "                                                                 skip_latent_attn_6_3[0][0]       \n",
            "                                                                 skip_latent_attn_7_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 128, 64)      0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_2 (Add)         (None, 128, 64)      0           cross_attn_1[1][0]               \n",
            "                                                                 skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_2 (Add)          (None, 128, 64)      0           cross_mlp_1[1][0]                \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_0 (Add)      (None, 128, 64)      0           latent_attn_1_0[1][0]            \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_0 (Add)       (None, 128, 64)      0           latent_mlp_1_0[1][0]             \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_1 (Add)      (None, 128, 64)      0           latent_attn_1_1[1][0]            \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_1 (Add)       (None, 128, 64)      0           latent_mlp_1_1[1][0]             \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_2 (Add)      (None, 128, 64)      0           latent_attn_1_2[1][0]            \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_2 (Add)       (None, 128, 64)      0           latent_mlp_1_2[1][0]             \n",
            "                                                                 skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_3 (Add)      (None, 128, 64)      0           latent_attn_1_3[1][0]            \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_3 (Add)       (None, 128, 64)      0           latent_mlp_1_3[1][0]             \n",
            "                                                                 skip_latent_attn_2_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_3 (Add)         (None, 128, 64)      0           cross_attn_1[2][0]               \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_3 (Add)          (None, 128, 64)      0           cross_mlp_1[2][0]                \n",
            "                                                                 skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_0 (Add)      (None, 128, 64)      0           latent_attn_1_0[2][0]            \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_0 (Add)       (None, 128, 64)      0           latent_mlp_1_0[2][0]             \n",
            "                                                                 skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_1 (Add)      (None, 128, 64)      0           latent_attn_1_1[2][0]            \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_1 (Add)       (None, 128, 64)      0           latent_mlp_1_1[2][0]             \n",
            "                                                                 skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_2 (Add)      (None, 128, 64)      0           latent_attn_1_2[2][0]            \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_2 (Add)       (None, 128, 64)      0           latent_mlp_1_2[2][0]             \n",
            "                                                                 skip_latent_attn_3_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_3 (Add)      (None, 128, 64)      0           latent_attn_1_3[2][0]            \n",
            "                                                                 skip_latent_mlp_3_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_3 (Add)       (None, 128, 64)      0           latent_mlp_1_3[2][0]             \n",
            "                                                                 skip_latent_attn_3_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_4 (Add)         (None, 128, 64)      0           cross_attn_1[3][0]               \n",
            "                                                                 skip_latent_mlp_3_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_4 (Add)          (None, 128, 64)      0           cross_mlp_1[3][0]                \n",
            "                                                                 skip_cross_attn_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_0 (Add)      (None, 128, 64)      0           latent_attn_1_0[3][0]            \n",
            "                                                                 skip_cross_mlp_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_0 (Add)       (None, 128, 64)      0           latent_mlp_1_0[3][0]             \n",
            "                                                                 skip_latent_attn_4_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_1 (Add)      (None, 128, 64)      0           latent_attn_1_1[3][0]            \n",
            "                                                                 skip_latent_mlp_4_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_1 (Add)       (None, 128, 64)      0           latent_mlp_1_1[3][0]             \n",
            "                                                                 skip_latent_attn_4_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_2 (Add)      (None, 128, 64)      0           latent_attn_1_2[3][0]            \n",
            "                                                                 skip_latent_mlp_4_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_2 (Add)       (None, 128, 64)      0           latent_mlp_1_2[3][0]             \n",
            "                                                                 skip_latent_attn_4_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_3 (Add)      (None, 128, 64)      0           latent_attn_1_3[3][0]            \n",
            "                                                                 skip_latent_mlp_4_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_3 (Add)       (None, 128, 64)      0           latent_mlp_1_3[3][0]             \n",
            "                                                                 skip_latent_attn_4_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_5 (Add)         (None, 128, 64)      0           cross_attn_1[4][0]               \n",
            "                                                                 skip_latent_mlp_4_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_5 (Add)          (None, 128, 64)      0           cross_mlp_1[4][0]                \n",
            "                                                                 skip_cross_attn_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_0 (Add)      (None, 128, 64)      0           latent_attn_1_0[4][0]            \n",
            "                                                                 skip_cross_mlp_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_0 (Add)       (None, 128, 64)      0           latent_mlp_1_0[4][0]             \n",
            "                                                                 skip_latent_attn_5_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_1 (Add)      (None, 128, 64)      0           latent_attn_1_1[4][0]            \n",
            "                                                                 skip_latent_mlp_5_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_1 (Add)       (None, 128, 64)      0           latent_mlp_1_1[4][0]             \n",
            "                                                                 skip_latent_attn_5_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_2 (Add)      (None, 128, 64)      0           latent_attn_1_2[4][0]            \n",
            "                                                                 skip_latent_mlp_5_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_2 (Add)       (None, 128, 64)      0           latent_mlp_1_2[4][0]             \n",
            "                                                                 skip_latent_attn_5_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_3 (Add)      (None, 128, 64)      0           latent_attn_1_3[4][0]            \n",
            "                                                                 skip_latent_mlp_5_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_3 (Add)       (None, 128, 64)      0           latent_mlp_1_3[4][0]             \n",
            "                                                                 skip_latent_attn_5_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_6 (Add)         (None, 128, 64)      0           cross_attn_1[5][0]               \n",
            "                                                                 skip_latent_mlp_5_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_6 (Add)          (None, 128, 64)      0           cross_mlp_1[5][0]                \n",
            "                                                                 skip_cross_attn_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_0 (Add)      (None, 128, 64)      0           latent_attn_1_0[5][0]            \n",
            "                                                                 skip_cross_mlp_6[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_0 (Add)       (None, 128, 64)      0           latent_mlp_1_0[5][0]             \n",
            "                                                                 skip_latent_attn_6_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_1 (Add)      (None, 128, 64)      0           latent_attn_1_1[5][0]            \n",
            "                                                                 skip_latent_mlp_6_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_1 (Add)       (None, 128, 64)      0           latent_mlp_1_1[5][0]             \n",
            "                                                                 skip_latent_attn_6_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_2 (Add)      (None, 128, 64)      0           latent_attn_1_2[5][0]            \n",
            "                                                                 skip_latent_mlp_6_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_2 (Add)       (None, 128, 64)      0           latent_mlp_1_2[5][0]             \n",
            "                                                                 skip_latent_attn_6_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_3 (Add)      (None, 128, 64)      0           latent_attn_1_3[5][0]            \n",
            "                                                                 skip_latent_mlp_6_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_3 (Add)       (None, 128, 64)      0           latent_mlp_1_3[5][0]             \n",
            "                                                                 skip_latent_attn_6_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_7 (Add)         (None, 128, 64)      0           cross_attn_1[6][0]               \n",
            "                                                                 skip_latent_mlp_6_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_7 (Add)          (None, 128, 64)      0           cross_mlp_1[6][0]                \n",
            "                                                                 skip_cross_attn_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_0 (Add)      (None, 128, 64)      0           latent_attn_1_0[6][0]            \n",
            "                                                                 skip_cross_mlp_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_0 (Add)       (None, 128, 64)      0           latent_mlp_1_0[6][0]             \n",
            "                                                                 skip_latent_attn_7_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_1 (Add)      (None, 128, 64)      0           latent_attn_1_1[6][0]            \n",
            "                                                                 skip_latent_mlp_7_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_1 (Add)       (None, 128, 64)      0           latent_mlp_1_1[6][0]             \n",
            "                                                                 skip_latent_attn_7_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_2 (Add)      (None, 128, 64)      0           latent_attn_1_2[6][0]            \n",
            "                                                                 skip_latent_mlp_7_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_2 (Add)       (None, 128, 64)      0           latent_mlp_1_2[6][0]             \n",
            "                                                                 skip_latent_attn_7_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_3 (Add)      (None, 128, 64)      0           latent_attn_1_3[6][0]            \n",
            "                                                                 skip_latent_mlp_7_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_3 (Add)       (None, 128, 64)      0           latent_mlp_1_3[6][0]             \n",
            "                                                                 skip_latent_attn_7_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 64)        0           skip_latent_mlp_7_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 64)        128         avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        650         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 252,446\n",
            "Trainable params: 252,446\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 209s 846ms/step - loss: 2.2680 - sparse_categorical_accuracy: 0.1448 - val_loss: 1.9893 - val_sparse_categorical_accuracy: 0.2676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "196/196 [==============================] - 152s 775ms/step - loss: 1.9468 - sparse_categorical_accuracy: 0.2793 - val_loss: 1.8643 - val_sparse_categorical_accuracy: 0.3266\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "196/196 [==============================] - 152s 777ms/step - loss: 1.8234 - sparse_categorical_accuracy: 0.3303 - val_loss: 1.8016 - val_sparse_categorical_accuracy: 0.3470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "196/196 [==============================] - 152s 776ms/step - loss: 1.7457 - sparse_categorical_accuracy: 0.3613 - val_loss: 1.6860 - val_sparse_categorical_accuracy: 0.3922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "196/196 [==============================] - 152s 775ms/step - loss: 1.6732 - sparse_categorical_accuracy: 0.3867 - val_loss: 1.6653 - val_sparse_categorical_accuracy: 0.3904\n",
            "Epoch 6/50\n",
            "196/196 [==============================] - 151s 772ms/step - loss: 1.6223 - sparse_categorical_accuracy: 0.4067 - val_loss: 1.5689 - val_sparse_categorical_accuracy: 0.4274\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/50\n",
            "196/196 [==============================] - 152s 776ms/step - loss: 1.5662 - sparse_categorical_accuracy: 0.4230 - val_loss: 1.5178 - val_sparse_categorical_accuracy: 0.4497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "196/196 [==============================] - 152s 776ms/step - loss: 1.5169 - sparse_categorical_accuracy: 0.4464 - val_loss: 1.4776 - val_sparse_categorical_accuracy: 0.4625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "196/196 [==============================] - 152s 774ms/step - loss: 1.4712 - sparse_categorical_accuracy: 0.4626 - val_loss: 1.4473 - val_sparse_categorical_accuracy: 0.4703\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "196/196 [==============================] - 152s 775ms/step - loss: 1.4372 - sparse_categorical_accuracy: 0.4789 - val_loss: 1.4279 - val_sparse_categorical_accuracy: 0.4869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "196/196 [==============================] - 152s 776ms/step - loss: 1.4039 - sparse_categorical_accuracy: 0.4941 - val_loss: 1.4138 - val_sparse_categorical_accuracy: 0.4885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "196/196 [==============================] - 152s 776ms/step - loss: 1.3708 - sparse_categorical_accuracy: 0.5026 - val_loss: 1.3640 - val_sparse_categorical_accuracy: 0.5166\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_80_layer_call_and_return_conditional_losses while saving (showing 5 of 560). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_128l_64ldim_1lh_64lhdim_1ch_64chdim_8cd_4ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "196/196 [==============================] - 152s 776ms/step - loss: 1.3537 - sparse_categorical_accuracy: 0.5116 - val_loss: 1.4414 - val_sparse_categorical_accuracy: 0.4884\n",
            "Epoch 14/50\n",
            " 61/196 [========>.....................] - ETA: 1:37 - loss: 1.3380 - sparse_categorical_accuracy: 0.5171"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvWdqKS2t-iw"
      },
      "source": [
        "Different number of latent transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rMhKLpYA01-R",
        "outputId": "9a951937-da0f-4ad3-c625-026908e379c0"
      },
      "source": [
        "epochs = 30\n",
        "for num_latents in [32]:\n",
        "  for latent_dim in [64]:\n",
        "    for latent_head_dim in [64]:\n",
        "      for cross_head_dim in [64]:\n",
        "        for cross_depth in [2]:\n",
        "          for latent_depth in [2,8]:\n",
        "            model = Perceiver(num_classes=num_classes, \n",
        "                              image_height=image_height, \n",
        "                              image_width=image_width,\n",
        "                              max_freq=8,\n",
        "                              num_freq_bands=8,\n",
        "                              freq_base=2.0,\n",
        "                              num_latents=num_latents,\n",
        "                              latent_dim=latent_dim,\n",
        "                              latent_heads=1,\n",
        "                              latent_head_dim=latent_head_dim,\n",
        "                              cross_heads=1,\n",
        "                              cross_head_dim=cross_head_dim,\n",
        "                              attn_dropout=0.2,\n",
        "                              mlp_dropout=0.2,\n",
        "                              cross_depth=cross_depth,\n",
        "                              latent_depth=latent_depth,             \n",
        "                              weight_tie_layers=True,\n",
        "                              preprocess=False).model()\n",
        "            model.summary()\n",
        "            train_and_eval(model, SAVE_PATH + f'_PERCEIVER_8fq_8fqb_{num_latents}l_{latent_dim}ldim_1lh_{latent_head_dim}lhdim_1ch_{cross_head_dim}chdim_{cross_depth}cd_{latent_depth}ld', verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 32, 64)       2048        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 37)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 32, 64)       13194       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 32, 64)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 32, 64)       8448        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 32, 64)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 32, 64)       16576       skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 32, 64)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 32, 64)       8448        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 32, 64)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 32, 64)       16576       skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 32, 64)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 32, 64)       8448        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 32, 64)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 32, 64)       13194       skip_latent_mlp_0_1[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 32, 64)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 32, 64)       8448        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 32, 64)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 32, 64)       16576       skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 32, 64)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 32, 64)       8448        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 32, 64)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 32, 64)       16576       skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 32, 64)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 32, 64)       8448        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 32, 64)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 64)        0           skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 64)        128         avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        650         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 146,206\n",
            "Trainable params: 146,206\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "196/196 [==============================] - 55s 83ms/step - loss: 2.2533 - sparse_categorical_accuracy: 0.1603 - val_loss: 1.9843 - val_sparse_categorical_accuracy: 0.2616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 340). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 340). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/30\n",
            "196/196 [==============================] - 13s 68ms/step - loss: 1.9396 - sparse_categorical_accuracy: 0.2907 - val_loss: 1.8047 - val_sparse_categorical_accuracy: 0.3518\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 340). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 340). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/30\n",
            "196/196 [==============================] - 13s 68ms/step - loss: 1.8103 - sparse_categorical_accuracy: 0.3401 - val_loss: 1.7465 - val_sparse_categorical_accuracy: 0.3693\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 340). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 340). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_8fq_8fqb_32l_64ldim_1lh_64lhdim_1ch_64chdim_2cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/30\n",
            "183/196 [===========================>..] - ETA: 0s - loss: 1.7432 - sparse_categorical_accuracy: 0.3679"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-fa440fc1001b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                               preprocess=False).model()\n\u001b[1;32m     26\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAVE_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'_PERCEIVER_8fq_8fqb_{num_latents}l_{latent_dim}ldim_1lh_{latent_head_dim}lhdim_1ch_{cross_head_dim}chdim_{cross_depth}cd_{latent_depth}ld'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-6e3649293d1b>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(_model, _checkpoint_filepath, verbose)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   )\n\u001b[1;32m     43\u001b[0m   \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-kdMuU-DXtx",
        "outputId": "11125f68-2011-4ccf-9337-d51388986620"
      },
      "source": [
        "learning_rate = 3e-3\n",
        "epochs = 30\n",
        "for max_freq in [image_height]:\n",
        "  for num_freq_bands in [64]:\n",
        "    for num_latents in [16]:\n",
        "      for latent_dim in [32]:\n",
        "        for latent_head_dim in [32]:\n",
        "          for cross_head_dim in [32]:\n",
        "            for cross_depth in [4]:\n",
        "              for latent_depth in [3]:\n",
        "                model = Perceiver(num_classes=num_classes, \n",
        "                                  image_height=image_height, \n",
        "                                  image_width=image_width,\n",
        "                                  max_freq=max_freq,\n",
        "                                  num_freq_bands=num_freq_bands,\n",
        "                                  freq_base=2.0,\n",
        "                                  num_latents=num_latents,\n",
        "                                  latent_dim=latent_dim,\n",
        "                                  latent_heads=1,\n",
        "                                  latent_head_dim=latent_head_dim,\n",
        "                                  cross_heads=1,\n",
        "                                  cross_head_dim=cross_head_dim,\n",
        "                                  attn_dropout=0.2,\n",
        "                                  mlp_dropout=0.2,\n",
        "                                  cross_depth=cross_depth,\n",
        "                                  latent_depth=latent_depth,             \n",
        "                                  weight_tie_layers=True,\n",
        "                                  preprocess=False).model()\n",
        "                model.summary()\n",
        "                train_and_eval(model, \n",
        "                               SAVE_PATH + f'_PERCEIVER' \\\n",
        "                                           f'_{max_freq}fq' \\\n",
        "                                           f'_{num_freq_bands}fqb' \\\n",
        "                                           f'_{num_latents}l' \\\n",
        "                                           f'_{latent_dim}ldim' \\\n",
        "                                           f'_1lh' \\\n",
        "                                           f'_{latent_head_dim}lhdim' \\\n",
        "                                           f'_1ch' \\\n",
        "                                           f'_{cross_head_dim}chdim' \\\n",
        "                                           f'_{cross_depth}cd' \\\n",
        "                                           f'_{latent_depth}ld', \n",
        "                               verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_18 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 16, 32)       512         resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 261)    0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 16, 32)       19338       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 16, 32)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 16, 32)       2144        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 16, 32)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 16, 32)       4160        skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 16, 32)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 16, 32)       2144        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 16, 32)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 16, 32)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 16, 32)       2144        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 16, 32)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 16, 32)       0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 16, 32)       2144        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 16, 32)       0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 16, 32)       19338       skip_latent_mlp_0_2[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 16, 32)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 16, 32)       2144        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 16, 32)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 16, 32)       4160        skip_cross_mlp_1[0][0]           \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 16, 32)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 16, 32)       2144        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 16, 32)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_1_0[0][0]        \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 16, 32)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 16, 32)       2144        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 16, 32)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_1_1[0][0]        \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 16, 32)       0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 16, 32)       2144        skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 16, 32)       0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_2 (Add)         (None, 16, 32)       0           cross_attn_1[1][0]               \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_2 (MLP)               (None, 16, 32)       2144        skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_2 (Add)          (None, 16, 32)       0           cross_mlp_2[0][0]                \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_0 (Add)      (None, 16, 32)       0           latent_attn_1_0[1][0]            \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_0 (MLP)            (None, 16, 32)       2144        skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_0 (Add)       (None, 16, 32)       0           latent_mlp_2_0[0][0]             \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_1 (Add)      (None, 16, 32)       0           latent_attn_1_1[1][0]            \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_1 (MLP)            (None, 16, 32)       2144        skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_1 (Add)       (None, 16, 32)       0           latent_mlp_2_1[0][0]             \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_2 (Add)      (None, 16, 32)       0           latent_attn_1_2[1][0]            \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_2 (MLP)            (None, 16, 32)       2144        skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_2 (Add)       (None, 16, 32)       0           latent_mlp_2_2[0][0]             \n",
            "                                                                 skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_3 (Add)         (None, 16, 32)       0           cross_attn_1[2][0]               \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_3 (MLP)               (None, 16, 32)       2144        skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_3 (Add)          (None, 16, 32)       0           cross_mlp_3[0][0]                \n",
            "                                                                 skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_0 (Add)      (None, 16, 32)       0           latent_attn_1_0[2][0]            \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_0 (MLP)            (None, 16, 32)       2144        skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_0 (Add)       (None, 16, 32)       0           latent_mlp_3_0[0][0]             \n",
            "                                                                 skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_1 (Add)      (None, 16, 32)       0           latent_attn_1_1[2][0]            \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_1 (MLP)            (None, 16, 32)       2144        skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_1 (Add)       (None, 16, 32)       0           latent_mlp_3_1[0][0]             \n",
            "                                                                 skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_2 (Add)      (None, 16, 32)       0           latent_attn_1_2[2][0]            \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_2 (MLP)            (None, 16, 32)       2144        skip_latent_attn_3_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_2 (Add)       (None, 16, 32)       0           latent_mlp_3_2[0][0]             \n",
            "                                                                 skip_latent_attn_3_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 32)        0           skip_latent_mlp_3_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 32)        64          avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        330         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 98,846\n",
            "Trainable params: 98,846\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "196/196 [==============================] - 77s 302ms/step - loss: 2.3036 - sparse_categorical_accuracy: 0.1299 - val_loss: 2.1063 - val_sparse_categorical_accuracy: 0.2434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/30\n",
            "196/196 [==============================] - 51s 260ms/step - loss: 1.9331 - sparse_categorical_accuracy: 0.2924 - val_loss: 1.8348 - val_sparse_categorical_accuracy: 0.3262\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/30\n",
            "196/196 [==============================] - 51s 261ms/step - loss: 1.7998 - sparse_categorical_accuracy: 0.3499 - val_loss: 1.7410 - val_sparse_categorical_accuracy: 0.3660\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/30\n",
            "196/196 [==============================] - 51s 260ms/step - loss: 1.7251 - sparse_categorical_accuracy: 0.3767 - val_loss: 1.6927 - val_sparse_categorical_accuracy: 0.3894\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/30\n",
            "196/196 [==============================] - 51s 262ms/step - loss: 1.6873 - sparse_categorical_accuracy: 0.3909 - val_loss: 1.6715 - val_sparse_categorical_accuracy: 0.4079\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/30\n",
            "196/196 [==============================] - 51s 261ms/step - loss: 1.6376 - sparse_categorical_accuracy: 0.4074 - val_loss: 1.6342 - val_sparse_categorical_accuracy: 0.4143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/30\n",
            "196/196 [==============================] - 51s 261ms/step - loss: 1.5805 - sparse_categorical_accuracy: 0.4285 - val_loss: 1.5246 - val_sparse_categorical_accuracy: 0.4521\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/30\n",
            "196/196 [==============================] - 51s 261ms/step - loss: 1.5294 - sparse_categorical_accuracy: 0.4510 - val_loss: 1.5121 - val_sparse_categorical_accuracy: 0.4429\n",
            "Epoch 9/30\n",
            "196/196 [==============================] - 51s 259ms/step - loss: 1.4926 - sparse_categorical_accuracy: 0.4605 - val_loss: 1.4880 - val_sparse_categorical_accuracy: 0.4602\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/30\n",
            "196/196 [==============================] - 51s 261ms/step - loss: 1.4643 - sparse_categorical_accuracy: 0.4717 - val_loss: 1.4370 - val_sparse_categorical_accuracy: 0.4854\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/30\n",
            "196/196 [==============================] - 51s 261ms/step - loss: 1.4334 - sparse_categorical_accuracy: 0.4864 - val_loss: 1.4100 - val_sparse_categorical_accuracy: 0.4920\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/30\n",
            "196/196 [==============================] - 51s 262ms/step - loss: 1.3964 - sparse_categorical_accuracy: 0.4986 - val_loss: 1.4189 - val_sparse_categorical_accuracy: 0.4902\n",
            "Epoch 13/30\n",
            "196/196 [==============================] - 51s 260ms/step - loss: 1.3834 - sparse_categorical_accuracy: 0.5020 - val_loss: 1.3787 - val_sparse_categorical_accuracy: 0.5049\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/30\n",
            "196/196 [==============================] - 51s 262ms/step - loss: 1.3700 - sparse_categorical_accuracy: 0.5106 - val_loss: 1.3650 - val_sparse_categorical_accuracy: 0.5112\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/30\n",
            "196/196 [==============================] - 51s 261ms/step - loss: 1.3456 - sparse_categorical_accuracy: 0.5152 - val_loss: 1.3832 - val_sparse_categorical_accuracy: 0.5054\n",
            "Epoch 16/30\n",
            "196/196 [==============================] - 51s 259ms/step - loss: 1.3416 - sparse_categorical_accuracy: 0.5234 - val_loss: 1.3388 - val_sparse_categorical_accuracy: 0.5191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/30\n",
            "196/196 [==============================] - 51s 261ms/step - loss: 1.3313 - sparse_categorical_accuracy: 0.5246 - val_loss: 1.3232 - val_sparse_categorical_accuracy: 0.5233\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/30\n",
            "196/196 [==============================] - 51s 262ms/step - loss: 1.3070 - sparse_categorical_accuracy: 0.5327 - val_loss: 1.3240 - val_sparse_categorical_accuracy: 0.5298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/30\n",
            "196/196 [==============================] - 52s 263ms/step - loss: 1.2969 - sparse_categorical_accuracy: 0.5362 - val_loss: 1.3344 - val_sparse_categorical_accuracy: 0.5239\n",
            "Epoch 20/30\n",
            "196/196 [==============================] - 51s 261ms/step - loss: 1.2879 - sparse_categorical_accuracy: 0.5377 - val_loss: 1.3134 - val_sparse_categorical_accuracy: 0.5310\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/30\n",
            "196/196 [==============================] - 52s 263ms/step - loss: 1.2617 - sparse_categorical_accuracy: 0.5484 - val_loss: 1.3011 - val_sparse_categorical_accuracy: 0.5368\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/30\n",
            "196/196 [==============================] - 51s 262ms/step - loss: 1.2600 - sparse_categorical_accuracy: 0.5497 - val_loss: 1.3089 - val_sparse_categorical_accuracy: 0.5369\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/30\n",
            "196/196 [==============================] - 52s 263ms/step - loss: 1.2552 - sparse_categorical_accuracy: 0.5492 - val_loss: 1.3002 - val_sparse_categorical_accuracy: 0.5355\n",
            "Epoch 24/30\n",
            "196/196 [==============================] - 51s 261ms/step - loss: 1.2308 - sparse_categorical_accuracy: 0.5595 - val_loss: 1.2918 - val_sparse_categorical_accuracy: 0.5385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/30\n",
            "196/196 [==============================] - 52s 263ms/step - loss: 1.2291 - sparse_categorical_accuracy: 0.5633 - val_loss: 1.2867 - val_sparse_categorical_accuracy: 0.5465\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26/30\n",
            "196/196 [==============================] - 52s 263ms/step - loss: 1.2098 - sparse_categorical_accuracy: 0.5675 - val_loss: 1.2951 - val_sparse_categorical_accuracy: 0.5329\n",
            "Epoch 27/30\n",
            "196/196 [==============================] - 51s 262ms/step - loss: 1.2202 - sparse_categorical_accuracy: 0.5642 - val_loss: 1.2658 - val_sparse_categorical_accuracy: 0.5508\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/30\n",
            "196/196 [==============================] - 52s 264ms/step - loss: 1.2099 - sparse_categorical_accuracy: 0.5640 - val_loss: 1.2797 - val_sparse_categorical_accuracy: 0.5409\n",
            "Epoch 29/30\n",
            "196/196 [==============================] - 51s 261ms/step - loss: 1.1867 - sparse_categorical_accuracy: 0.5761 - val_loss: 1.2472 - val_sparse_categorical_accuracy: 0.5540\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_440_layer_call_fn while saving (showing 5 of 690). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_4cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/30\n",
            "196/196 [==============================] - 52s 263ms/step - loss: 1.1796 - sparse_categorical_accuracy: 0.5815 - val_loss: 1.2689 - val_sparse_categorical_accuracy: 0.5453\n",
            "Total training time 2218.105672121048 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0VoFpXzgn0T",
        "outputId": "4e01c9e3-9a3f-4c73-d824-0f2bb889dc4c"
      },
      "source": [
        "learning_rate = 3e-3\n",
        "epochs = 20\n",
        "for max_freq in [image_height]:\n",
        "  for num_freq_bands in [64]:\n",
        "    for num_latents in [16]:\n",
        "      for latent_dim in [32]:\n",
        "        for latent_head_dim in [32]:\n",
        "          for cross_head_dim in [32]:\n",
        "            for cross_depth in [3]:\n",
        "              for latent_depth in [3,6]:\n",
        "                model = Perceiver(num_classes=num_classes, \n",
        "                                  image_height=image_height, \n",
        "                                  image_width=image_width,\n",
        "                                  max_freq=max_freq,\n",
        "                                  num_freq_bands=num_freq_bands,\n",
        "                                  freq_base=2.0,\n",
        "                                  num_latents=num_latents,\n",
        "                                  latent_dim=latent_dim,\n",
        "                                  latent_heads=1,\n",
        "                                  latent_head_dim=latent_head_dim,\n",
        "                                  cross_heads=1,\n",
        "                                  cross_head_dim=cross_head_dim,\n",
        "                                  attn_dropout=0.2,\n",
        "                                  mlp_dropout=0.2,\n",
        "                                  cross_depth=cross_depth,\n",
        "                                  latent_depth=latent_depth,             \n",
        "                                  weight_tie_layers=True,\n",
        "                                  preprocess=False).model()\n",
        "                model.summary()\n",
        "                train_and_eval(model, \n",
        "                               SAVE_PATH + f'_PERCEIVER' \\\n",
        "                                           f'_{max_freq}fq' \\\n",
        "                                           f'_{num_freq_bands}fqb' \\\n",
        "                                           f'_{num_latents}l' \\\n",
        "                                           f'_{latent_dim}ldim' \\\n",
        "                                           f'_1lh' \\\n",
        "                                           f'_{latent_head_dim}lhdim' \\\n",
        "                                           f'_1ch' \\\n",
        "                                           f'_{cross_head_dim}chdim' \\\n",
        "                                           f'_{cross_depth}cd' \\\n",
        "                                           f'_{latent_depth}ld', \n",
        "                               verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_24\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_25 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 16, 32)       512         resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 261)    0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 16, 32)       19338       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 16, 32)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 16, 32)       2144        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 16, 32)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 16, 32)       4160        skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 16, 32)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 16, 32)       2144        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 16, 32)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 16, 32)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 16, 32)       2144        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 16, 32)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 16, 32)       0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 16, 32)       2144        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 16, 32)       0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 16, 32)       19338       skip_latent_mlp_0_2[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 16, 32)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 16, 32)       2144        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 16, 32)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 16, 32)       4160        skip_cross_mlp_1[0][0]           \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 16, 32)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 16, 32)       2144        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 16, 32)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_1_0[0][0]        \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 16, 32)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 16, 32)       2144        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 16, 32)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_1_1[0][0]        \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 16, 32)       0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 16, 32)       2144        skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 16, 32)       0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_2 (Add)         (None, 16, 32)       0           cross_attn_1[1][0]               \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_2 (MLP)               (None, 16, 32)       2144        skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_2 (Add)          (None, 16, 32)       0           cross_mlp_2[0][0]                \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_0 (Add)      (None, 16, 32)       0           latent_attn_1_0[1][0]            \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_0 (MLP)            (None, 16, 32)       2144        skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_0 (Add)       (None, 16, 32)       0           latent_mlp_2_0[0][0]             \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_1 (Add)      (None, 16, 32)       0           latent_attn_1_1[1][0]            \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_1 (MLP)            (None, 16, 32)       2144        skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_1 (Add)       (None, 16, 32)       0           latent_mlp_2_1[0][0]             \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_2 (Add)      (None, 16, 32)       0           latent_attn_1_2[1][0]            \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_2 (MLP)            (None, 16, 32)       2144        skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_2 (Add)       (None, 16, 32)       0           latent_mlp_2_2[0][0]             \n",
            "                                                                 skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 32)        0           skip_latent_mlp_2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 32)        64          avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        330         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 90,270\n",
            "Trainable params: 90,270\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "196/196 [==============================] - 60s 232ms/step - loss: 2.3451 - sparse_categorical_accuracy: 0.1093 - val_loss: 2.0884 - val_sparse_categorical_accuracy: 0.2312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/20\n",
            "196/196 [==============================] - 40s 201ms/step - loss: 2.0343 - sparse_categorical_accuracy: 0.2507 - val_loss: 1.8332 - val_sparse_categorical_accuracy: 0.3298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/20\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 1.8229 - sparse_categorical_accuracy: 0.3332 - val_loss: 1.7209 - val_sparse_categorical_accuracy: 0.3757\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/20\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.7318 - sparse_categorical_accuracy: 0.3741 - val_loss: 1.6833 - val_sparse_categorical_accuracy: 0.3889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/20\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.6564 - sparse_categorical_accuracy: 0.4007 - val_loss: 1.6239 - val_sparse_categorical_accuracy: 0.4169\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/20\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.6247 - sparse_categorical_accuracy: 0.4150 - val_loss: 1.5697 - val_sparse_categorical_accuracy: 0.4348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/20\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 1.5811 - sparse_categorical_accuracy: 0.4326 - val_loss: 1.5079 - val_sparse_categorical_accuracy: 0.4611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/20\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 1.5436 - sparse_categorical_accuracy: 0.4451 - val_loss: 1.5115 - val_sparse_categorical_accuracy: 0.4579\n",
            "Epoch 9/20\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 1.5075 - sparse_categorical_accuracy: 0.4590 - val_loss: 1.4678 - val_sparse_categorical_accuracy: 0.4728\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/20\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.4705 - sparse_categorical_accuracy: 0.4717 - val_loss: 1.4959 - val_sparse_categorical_accuracy: 0.4554\n",
            "Epoch 11/20\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 1.4445 - sparse_categorical_accuracy: 0.4807 - val_loss: 1.4152 - val_sparse_categorical_accuracy: 0.4906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/20\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 1.4073 - sparse_categorical_accuracy: 0.4957 - val_loss: 1.4396 - val_sparse_categorical_accuracy: 0.4944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/20\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 1.3864 - sparse_categorical_accuracy: 0.5008 - val_loss: 1.3731 - val_sparse_categorical_accuracy: 0.5071\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/20\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.3768 - sparse_categorical_accuracy: 0.5048 - val_loss: 1.3592 - val_sparse_categorical_accuracy: 0.5128\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/20\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 1.3499 - sparse_categorical_accuracy: 0.5169 - val_loss: 1.3265 - val_sparse_categorical_accuracy: 0.5299\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/20\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.3144 - sparse_categorical_accuracy: 0.5295 - val_loss: 1.3226 - val_sparse_categorical_accuracy: 0.5301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/20\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 1.3116 - sparse_categorical_accuracy: 0.5308 - val_loss: 1.3254 - val_sparse_categorical_accuracy: 0.5265\n",
            "Epoch 18/20\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 1.3103 - sparse_categorical_accuracy: 0.5328 - val_loss: 1.2769 - val_sparse_categorical_accuracy: 0.5460\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_556_layer_call_fn while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/20\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.2907 - sparse_categorical_accuracy: 0.5398 - val_loss: 1.3069 - val_sparse_categorical_accuracy: 0.5373\n",
            "Epoch 20/20\n",
            "196/196 [==============================] - 40s 203ms/step - loss: 1.2766 - sparse_categorical_accuracy: 0.5398 - val_loss: 1.2617 - val_sparse_categorical_accuracy: 0.5453\n",
            "Total training time 1189.623524904251 seconds\n",
            "Model: \"model_25\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_26 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 16, 32)       512         resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 261)    0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 16, 32)       19338       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 16, 32)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 16, 32)       2144        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 16, 32)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 16, 32)       4160        skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 16, 32)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 16, 32)       2144        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 16, 32)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 16, 32)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 16, 32)       2144        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 16, 32)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 16, 32)       0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 16, 32)       2144        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 16, 32)       0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 16, 32)       0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 16, 32)       2144        skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 16, 32)       0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_4 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_4 (Add)      (None, 16, 32)       0           latent_attn_0_4[0][0]            \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_4 (MLP)            (None, 16, 32)       2144        skip_latent_attn_0_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_4 (Add)       (None, 16, 32)       0           latent_mlp_0_4[0][0]             \n",
            "                                                                 skip_latent_attn_0_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_5 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_0_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_5 (Add)      (None, 16, 32)       0           latent_attn_0_5[0][0]            \n",
            "                                                                 skip_latent_mlp_0_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_5 (MLP)            (None, 16, 32)       2144        skip_latent_attn_0_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_5 (Add)       (None, 16, 32)       0           latent_mlp_0_5[0][0]             \n",
            "                                                                 skip_latent_attn_0_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 16, 32)       19338       skip_latent_mlp_0_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_1_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 16, 32)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 16, 32)       2144        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 16, 32)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 16, 32)       4160        skip_cross_mlp_1[0][0]           \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 16, 32)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 16, 32)       2144        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 16, 32)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_1_0[0][0]        \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 16, 32)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 16, 32)       2144        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 16, 32)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_1_1[0][0]        \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 16, 32)       0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 16, 32)       2144        skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 16, 32)       0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_1_2[0][0]        \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 16, 32)       0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 16, 32)       2144        skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 16, 32)       0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_4 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_1_3[0][0]        \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_4 (Add)      (None, 16, 32)       0           latent_attn_1_4[0][0]            \n",
            "                                                                 skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_4 (MLP)            (None, 16, 32)       2144        skip_latent_attn_1_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_4 (Add)       (None, 16, 32)       0           latent_mlp_1_4[0][0]             \n",
            "                                                                 skip_latent_attn_1_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_5 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_1_4[0][0]        \n",
            "                                                                 skip_latent_mlp_2_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_5 (Add)      (None, 16, 32)       0           latent_attn_1_5[0][0]            \n",
            "                                                                 skip_latent_mlp_1_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_5 (MLP)            (None, 16, 32)       2144        skip_latent_attn_1_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_5 (Add)       (None, 16, 32)       0           latent_mlp_1_5[0][0]             \n",
            "                                                                 skip_latent_attn_1_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_2 (Add)         (None, 16, 32)       0           cross_attn_1[1][0]               \n",
            "                                                                 skip_latent_mlp_1_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_2 (MLP)               (None, 16, 32)       2144        skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_2 (Add)          (None, 16, 32)       0           cross_mlp_2[0][0]                \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_0 (Add)      (None, 16, 32)       0           latent_attn_1_0[1][0]            \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_0 (MLP)            (None, 16, 32)       2144        skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_0 (Add)       (None, 16, 32)       0           latent_mlp_2_0[0][0]             \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_1 (Add)      (None, 16, 32)       0           latent_attn_1_1[1][0]            \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_1 (MLP)            (None, 16, 32)       2144        skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_1 (Add)       (None, 16, 32)       0           latent_mlp_2_1[0][0]             \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_2 (Add)      (None, 16, 32)       0           latent_attn_1_2[1][0]            \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_2 (MLP)            (None, 16, 32)       2144        skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_2 (Add)       (None, 16, 32)       0           latent_mlp_2_2[0][0]             \n",
            "                                                                 skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_3 (Add)      (None, 16, 32)       0           latent_attn_1_3[1][0]            \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_3 (MLP)            (None, 16, 32)       2144        skip_latent_attn_2_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_3 (Add)       (None, 16, 32)       0           latent_mlp_2_3[0][0]             \n",
            "                                                                 skip_latent_attn_2_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_4 (Add)      (None, 16, 32)       0           latent_attn_1_4[1][0]            \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_4 (MLP)            (None, 16, 32)       2144        skip_latent_attn_2_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_4 (Add)       (None, 16, 32)       0           latent_mlp_2_4[0][0]             \n",
            "                                                                 skip_latent_attn_2_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_5 (Add)      (None, 16, 32)       0           latent_attn_1_5[1][0]            \n",
            "                                                                 skip_latent_mlp_2_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_5 (MLP)            (None, 16, 32)       2144        skip_latent_attn_2_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_5 (Add)       (None, 16, 32)       0           latent_mlp_2_5[0][0]             \n",
            "                                                                 skip_latent_attn_2_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 32)        0           skip_latent_mlp_2_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "ln (LayerNormalization)         (None, 1, 32)        64          avgpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        330         ln[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 134,526\n",
            "Trainable params: 134,526\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "196/196 [==============================] - 79s 279ms/step - loss: 2.3850 - sparse_categorical_accuracy: 0.1065 - val_loss: 2.2151 - val_sparse_categorical_accuracy: 0.1726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/20\n",
            "196/196 [==============================] - 45s 229ms/step - loss: 2.1657 - sparse_categorical_accuracy: 0.1912 - val_loss: 2.0584 - val_sparse_categorical_accuracy: 0.2403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/20\n",
            "196/196 [==============================] - 45s 229ms/step - loss: 1.9693 - sparse_categorical_accuracy: 0.2733 - val_loss: 1.8616 - val_sparse_categorical_accuracy: 0.3133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/20\n",
            "196/196 [==============================] - 45s 229ms/step - loss: 1.8195 - sparse_categorical_accuracy: 0.3358 - val_loss: 1.7235 - val_sparse_categorical_accuracy: 0.3775\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/20\n",
            "196/196 [==============================] - 45s 229ms/step - loss: 1.7104 - sparse_categorical_accuracy: 0.3804 - val_loss: 1.6679 - val_sparse_categorical_accuracy: 0.3946\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/20\n",
            "196/196 [==============================] - 45s 229ms/step - loss: 1.6786 - sparse_categorical_accuracy: 0.3977 - val_loss: 1.6530 - val_sparse_categorical_accuracy: 0.3973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/20\n",
            "196/196 [==============================] - 45s 230ms/step - loss: 1.6340 - sparse_categorical_accuracy: 0.4103 - val_loss: 1.6198 - val_sparse_categorical_accuracy: 0.4201\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/20\n",
            "196/196 [==============================] - 45s 230ms/step - loss: 1.5951 - sparse_categorical_accuracy: 0.4259 - val_loss: 1.5712 - val_sparse_categorical_accuracy: 0.4389\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/20\n",
            "196/196 [==============================] - 45s 230ms/step - loss: 1.5587 - sparse_categorical_accuracy: 0.4446 - val_loss: 1.5197 - val_sparse_categorical_accuracy: 0.4575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/20\n",
            "196/196 [==============================] - 45s 230ms/step - loss: 1.5243 - sparse_categorical_accuracy: 0.4543 - val_loss: 1.5199 - val_sparse_categorical_accuracy: 0.4570\n",
            "Epoch 11/20\n",
            "196/196 [==============================] - 45s 231ms/step - loss: 1.5044 - sparse_categorical_accuracy: 0.4589 - val_loss: 1.4425 - val_sparse_categorical_accuracy: 0.4764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/20\n",
            "196/196 [==============================] - 45s 231ms/step - loss: 1.4533 - sparse_categorical_accuracy: 0.4782 - val_loss: 1.4321 - val_sparse_categorical_accuracy: 0.4893\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/20\n",
            "196/196 [==============================] - 45s 231ms/step - loss: 1.4381 - sparse_categorical_accuracy: 0.4868 - val_loss: 1.4445 - val_sparse_categorical_accuracy: 0.4816\n",
            "Epoch 14/20\n",
            "196/196 [==============================] - 45s 230ms/step - loss: 1.4085 - sparse_categorical_accuracy: 0.4962 - val_loss: 1.4238 - val_sparse_categorical_accuracy: 0.5007\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/20\n",
            "196/196 [==============================] - 46s 233ms/step - loss: 1.3822 - sparse_categorical_accuracy: 0.5074 - val_loss: 1.3845 - val_sparse_categorical_accuracy: 0.5101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/20\n",
            "196/196 [==============================] - 46s 232ms/step - loss: 1.3616 - sparse_categorical_accuracy: 0.5128 - val_loss: 1.3761 - val_sparse_categorical_accuracy: 0.5131\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/20\n",
            "196/196 [==============================] - 45s 231ms/step - loss: 1.3401 - sparse_categorical_accuracy: 0.5237 - val_loss: 1.3507 - val_sparse_categorical_accuracy: 0.5158\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/20\n",
            "196/196 [==============================] - 45s 230ms/step - loss: 1.3197 - sparse_categorical_accuracy: 0.5299 - val_loss: 1.3093 - val_sparse_categorical_accuracy: 0.5322\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/20\n",
            "196/196 [==============================] - 45s 231ms/step - loss: 1.3058 - sparse_categorical_accuracy: 0.5340 - val_loss: 1.3461 - val_sparse_categorical_accuracy: 0.5272\n",
            "Epoch 20/20\n",
            "196/196 [==============================] - 45s 231ms/step - loss: 1.2985 - sparse_categorical_accuracy: 0.5393 - val_loss: 1.2960 - val_sparse_categorical_accuracy: 0.5406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_572_layer_call_fn while saving (showing 5 of 990). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total training time 1650.4739453792572 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xTeypYe4DEe",
        "outputId": "6e6d2020-8069-4866-ff01-497f88c4306c"
      },
      "source": [
        "learning_rate = 3e-3\n",
        "epochs = 20\n",
        "for max_freq in [image_height]:\n",
        "  for num_freq_bands in [64]:\n",
        "    for num_latents in [16]:\n",
        "      for latent_dim in [256]:\n",
        "        for latent_head_dim in [256]:\n",
        "          for cross_head_dim in [256]:\n",
        "            for cross_depth in [3]:\n",
        "              for latent_depth in [2]:\n",
        "                model = Perceiver(num_classes=num_classes, \n",
        "                                  image_height=image_height, \n",
        "                                  image_width=image_width,\n",
        "                                  max_freq=max_freq,\n",
        "                                  num_freq_bands=num_freq_bands,\n",
        "                                  freq_base=2.0,\n",
        "                                  num_latents=num_latents,\n",
        "                                  latent_dim=latent_dim,\n",
        "                                  latent_heads=1,\n",
        "                                  latent_head_dim=latent_head_dim,\n",
        "                                  cross_heads=1,\n",
        "                                  cross_head_dim=cross_head_dim,\n",
        "                                  attn_dropout=0.2,\n",
        "                                  mlp_dropout=0.2,\n",
        "                                  cross_depth=cross_depth,\n",
        "                                  latent_depth=latent_depth,             \n",
        "                                  weight_tie_layers=True,\n",
        "                                  preprocess=False).model()\n",
        "                model.summary()\n",
        "                train_and_eval(model, \n",
        "                               SAVE_PATH + f'_PERCEIVER' \\\n",
        "                                           f'_{max_freq}fq' \\\n",
        "                                           f'_{num_freq_bands}fqb' \\\n",
        "                                           f'_{num_latents}l' \\\n",
        "                                           f'_{latent_dim}ldim' \\\n",
        "                                           f'_1lh' \\\n",
        "                                           f'_{latent_head_dim}lhdim' \\\n",
        "                                           f'_1ch' \\\n",
        "                                           f'_{cross_head_dim}chdim' \\\n",
        "                                           f'_{cross_depth}cd' \\\n",
        "                                           f'_{latent_depth}ld', \n",
        "                               verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 16, 256)      4096        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 261)    0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 16, 256)      265738      input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 16, 256)      0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 16, 256)      131840      skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 16, 256)      0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 16, 256)      262656      skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 16, 256)      0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 16, 256)      131840      skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 16, 256)      0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 16, 256)      262656      skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 16, 256)      0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 16, 256)      131840      skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 16, 256)      0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 16, 256)      265738      skip_latent_mlp_0_1[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 16, 256)      0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 16, 256)      131840      skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 16, 256)      0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 16, 256)      262656      skip_cross_mlp_1[0][0]           \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 16, 256)      0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 16, 256)      131840      skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 16, 256)      0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 16, 256)      262656      skip_latent_mlp_1_0[0][0]        \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 16, 256)      0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 16, 256)      131840      skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 16, 256)      0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_2 (Add)         (None, 16, 256)      0           cross_attn_1[1][0]               \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_2 (MLP)               (None, 16, 256)      131840      skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_2 (Add)          (None, 16, 256)      0           cross_mlp_2[0][0]                \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_0 (Add)      (None, 16, 256)      0           latent_attn_1_0[1][0]            \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_0 (MLP)            (None, 16, 256)      131840      skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_0 (Add)       (None, 16, 256)      0           latent_mlp_2_0[0][0]             \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_1 (Add)      (None, 16, 256)      0           latent_attn_1_1[1][0]            \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_1 (MLP)            (None, 16, 256)      131840      skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_1 (Add)       (None, 16, 256)      0           latent_mlp_2_1[0][0]             \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 256)       0           skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        2570        avgpool[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,775,326\n",
            "Trainable params: 2,775,326\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "196/196 [==============================] - 109s 503ms/step - loss: 11.0749 - sparse_categorical_accuracy: 0.1030 - val_loss: 2.4941 - val_sparse_categorical_accuracy: 0.1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/20\n",
            "196/196 [==============================] - 95s 483ms/step - loss: 2.3227 - sparse_categorical_accuracy: 0.1403 - val_loss: 2.2080 - val_sparse_categorical_accuracy: 0.1992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/20\n",
            "196/196 [==============================] - 95s 484ms/step - loss: 2.1748 - sparse_categorical_accuracy: 0.1832 - val_loss: 2.1296 - val_sparse_categorical_accuracy: 0.2192\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/20\n",
            "196/196 [==============================] - 95s 484ms/step - loss: 2.1097 - sparse_categorical_accuracy: 0.2156 - val_loss: 2.1203 - val_sparse_categorical_accuracy: 0.2292\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/20\n",
            "196/196 [==============================] - 95s 484ms/step - loss: 2.0566 - sparse_categorical_accuracy: 0.2442 - val_loss: 1.9851 - val_sparse_categorical_accuracy: 0.2875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/20\n",
            "196/196 [==============================] - 95s 484ms/step - loss: 1.9531 - sparse_categorical_accuracy: 0.2884 - val_loss: 1.8616 - val_sparse_categorical_accuracy: 0.3243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/20\n",
            "196/196 [==============================] - 95s 484ms/step - loss: 1.8678 - sparse_categorical_accuracy: 0.3189 - val_loss: 1.8412 - val_sparse_categorical_accuracy: 0.3376\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/20\n",
            "196/196 [==============================] - 95s 485ms/step - loss: 1.8094 - sparse_categorical_accuracy: 0.3439 - val_loss: 1.7607 - val_sparse_categorical_accuracy: 0.3763\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/20\n",
            "196/196 [==============================] - 95s 484ms/step - loss: 1.7533 - sparse_categorical_accuracy: 0.3655 - val_loss: 1.6982 - val_sparse_categorical_accuracy: 0.3871\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/20\n",
            "196/196 [==============================] - 95s 485ms/step - loss: 1.7078 - sparse_categorical_accuracy: 0.3819 - val_loss: 1.6433 - val_sparse_categorical_accuracy: 0.4180\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/20\n",
            "196/196 [==============================] - 95s 485ms/step - loss: 1.6377 - sparse_categorical_accuracy: 0.4072 - val_loss: 1.6704 - val_sparse_categorical_accuracy: 0.4050\n",
            "Epoch 12/20\n",
            "196/196 [==============================] - 95s 483ms/step - loss: 1.5690 - sparse_categorical_accuracy: 0.4331 - val_loss: 1.5090 - val_sparse_categorical_accuracy: 0.4541\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/20\n",
            "196/196 [==============================] - 95s 485ms/step - loss: 1.5262 - sparse_categorical_accuracy: 0.4481 - val_loss: 1.4804 - val_sparse_categorical_accuracy: 0.4642\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/20\n",
            "196/196 [==============================] - 95s 485ms/step - loss: 1.4791 - sparse_categorical_accuracy: 0.4671 - val_loss: 1.4555 - val_sparse_categorical_accuracy: 0.4798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/20\n",
            "196/196 [==============================] - 95s 484ms/step - loss: 1.4377 - sparse_categorical_accuracy: 0.4812 - val_loss: 1.4928 - val_sparse_categorical_accuracy: 0.4632\n",
            "Epoch 16/20\n",
            "196/196 [==============================] - 95s 483ms/step - loss: 1.4121 - sparse_categorical_accuracy: 0.4905 - val_loss: 1.3813 - val_sparse_categorical_accuracy: 0.5099\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/20\n",
            "196/196 [==============================] - 95s 484ms/step - loss: 1.3682 - sparse_categorical_accuracy: 0.5065 - val_loss: 1.3992 - val_sparse_categorical_accuracy: 0.5029\n",
            "Epoch 18/20\n",
            "196/196 [==============================] - 95s 483ms/step - loss: 1.3253 - sparse_categorical_accuracy: 0.5262 - val_loss: 1.2870 - val_sparse_categorical_accuracy: 0.5386\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/20\n",
            "196/196 [==============================] - 95s 484ms/step - loss: 1.3002 - sparse_categorical_accuracy: 0.5345 - val_loss: 1.2644 - val_sparse_categorical_accuracy: 0.5457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/20\n",
            "196/196 [==============================] - 95s 484ms/step - loss: 1.2471 - sparse_categorical_accuracy: 0.5533 - val_loss: 1.2227 - val_sparse_categorical_accuracy: 0.5568\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_40_layer_call_and_return_conditional_losses while saving (showing 5 of 430). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_256ldim_1lh_256lhdim_1ch_256chdim_3cd_2ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total training time 2217.087265729904 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YkTX_PGD7WUS",
        "outputId": "eab9b893-a784-43e4-b1c8-b3627ad8bbef"
      },
      "source": [
        "learning_rate = 1e-2\n",
        "epochs = 50\n",
        "batch_size=1024\n",
        "for max_freq in [image_height]:\n",
        "  for num_freq_bands in [64]:\n",
        "    for num_latents in [16]:\n",
        "      for latent_dim in [32]:\n",
        "        for latent_head_dim in [32]:\n",
        "          for cross_head_dim in [32]:\n",
        "            for cross_depth in [3]:\n",
        "              for latent_depth in [3]:\n",
        "                model = Perceiver(num_classes=num_classes, \n",
        "                                  image_height=image_height, \n",
        "                                  image_width=image_width,\n",
        "                                  max_freq=max_freq,\n",
        "                                  num_freq_bands=num_freq_bands,\n",
        "                                  freq_base=2.0,\n",
        "                                  num_latents=num_latents,\n",
        "                                  latent_dim=latent_dim,\n",
        "                                  latent_heads=1,\n",
        "                                  latent_head_dim=latent_head_dim,\n",
        "                                  cross_heads=1,\n",
        "                                  cross_head_dim=cross_head_dim,\n",
        "                                  attn_dropout=0.2,\n",
        "                                  mlp_dropout=0.2,\n",
        "                                  cross_depth=cross_depth,\n",
        "                                  latent_depth=latent_depth,             \n",
        "                                  weight_tie_layers=True,\n",
        "                                  preprocess=False).model()\n",
        "                model.summary()\n",
        "                train_and_eval(model, \n",
        "                               SAVE_PATH + f'_PERCEIVER' \\\n",
        "                                           f'_{max_freq}fq' \\\n",
        "                                           f'_{num_freq_bands}fqb' \\\n",
        "                                           f'_{num_latents}l' \\\n",
        "                                           f'_{latent_dim}ldim' \\\n",
        "                                           f'_1lh' \\\n",
        "                                           f'_{latent_head_dim}lhdim' \\\n",
        "                                           f'_1ch' \\\n",
        "                                           f'_{cross_head_dim}chdim' \\\n",
        "                                           f'_{cross_depth}cd' \\\n",
        "                                           f'_{latent_depth}ld', \n",
        "                               verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 16, 32)       512         resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 261)    0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 16, 32)       19338       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 16, 32)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 16, 32)       2144        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 16, 32)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 16, 32)       4160        skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 16, 32)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 16, 32)       2144        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 16, 32)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 16, 32)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 16, 32)       2144        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 16, 32)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 16, 32)       0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 16, 32)       2144        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 16, 32)       0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 16, 32)       19338       skip_latent_mlp_0_2[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 16, 32)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 16, 32)       2144        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 16, 32)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 16, 32)       4160        skip_cross_mlp_1[0][0]           \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 16, 32)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 16, 32)       2144        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 16, 32)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_1_0[0][0]        \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 16, 32)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 16, 32)       2144        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 16, 32)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 16, 32)       4160        skip_latent_mlp_1_1[0][0]        \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 16, 32)       0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 16, 32)       2144        skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 16, 32)       0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_2 (Add)         (None, 16, 32)       0           cross_attn_1[1][0]               \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_2 (MLP)               (None, 16, 32)       2144        skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_2 (Add)          (None, 16, 32)       0           cross_mlp_2[0][0]                \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_0 (Add)      (None, 16, 32)       0           latent_attn_1_0[1][0]            \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_0 (MLP)            (None, 16, 32)       2144        skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_0 (Add)       (None, 16, 32)       0           latent_mlp_2_0[0][0]             \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_1 (Add)      (None, 16, 32)       0           latent_attn_1_1[1][0]            \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_1 (MLP)            (None, 16, 32)       2144        skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_1 (Add)       (None, 16, 32)       0           latent_mlp_2_1[0][0]             \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_2 (Add)      (None, 16, 32)       0           latent_attn_1_2[1][0]            \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_2 (MLP)            (None, 16, 32)       2144        skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_2 (Add)       (None, 16, 32)       0           latent_mlp_2_2[0][0]             \n",
            "                                                                 skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 32)        0           skip_latent_mlp_2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        330         avgpool[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 90,206\n",
            "Trainable params: 90,206\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "49/49 [==============================] - 54s 843ms/step - loss: 6.4219 - sparse_categorical_accuracy: 0.0986 - val_loss: 2.3509 - val_sparse_categorical_accuracy: 0.1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "49/49 [==============================] - 36s 739ms/step - loss: 2.3546 - sparse_categorical_accuracy: 0.1188 - val_loss: 2.1544 - val_sparse_categorical_accuracy: 0.1948\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "49/49 [==============================] - 36s 740ms/step - loss: 2.1668 - sparse_categorical_accuracy: 0.1909 - val_loss: 2.0200 - val_sparse_categorical_accuracy: 0.2695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/50\n",
            "49/49 [==============================] - 36s 740ms/step - loss: 2.0306 - sparse_categorical_accuracy: 0.2571 - val_loss: 1.9355 - val_sparse_categorical_accuracy: 0.3053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "49/49 [==============================] - 36s 740ms/step - loss: 1.9301 - sparse_categorical_accuracy: 0.3029 - val_loss: 1.8439 - val_sparse_categorical_accuracy: 0.3371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "49/49 [==============================] - 36s 742ms/step - loss: 1.8660 - sparse_categorical_accuracy: 0.3302 - val_loss: 1.8317 - val_sparse_categorical_accuracy: 0.3348\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 36s 735ms/step - loss: 1.8109 - sparse_categorical_accuracy: 0.3449 - val_loss: 1.7753 - val_sparse_categorical_accuracy: 0.3720\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/50\n",
            "49/49 [==============================] - 36s 738ms/step - loss: 1.7497 - sparse_categorical_accuracy: 0.3714 - val_loss: 1.6919 - val_sparse_categorical_accuracy: 0.3922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "49/49 [==============================] - 36s 740ms/step - loss: 1.7079 - sparse_categorical_accuracy: 0.3824 - val_loss: 1.6550 - val_sparse_categorical_accuracy: 0.4071\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "49/49 [==============================] - 36s 741ms/step - loss: 1.6702 - sparse_categorical_accuracy: 0.4002 - val_loss: 1.6248 - val_sparse_categorical_accuracy: 0.4208\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "49/49 [==============================] - 36s 739ms/step - loss: 1.6348 - sparse_categorical_accuracy: 0.4118 - val_loss: 1.5952 - val_sparse_categorical_accuracy: 0.4307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "49/49 [==============================] - 36s 739ms/step - loss: 1.5997 - sparse_categorical_accuracy: 0.4252 - val_loss: 1.5827 - val_sparse_categorical_accuracy: 0.4401\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "49/49 [==============================] - 36s 742ms/step - loss: 1.5669 - sparse_categorical_accuracy: 0.4325 - val_loss: 1.5324 - val_sparse_categorical_accuracy: 0.4537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/50\n",
            "49/49 [==============================] - 36s 741ms/step - loss: 1.5226 - sparse_categorical_accuracy: 0.4536 - val_loss: 1.5348 - val_sparse_categorical_accuracy: 0.4533\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 36s 734ms/step - loss: 1.5126 - sparse_categorical_accuracy: 0.4588 - val_loss: 1.4880 - val_sparse_categorical_accuracy: 0.4614\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "49/49 [==============================] - 36s 741ms/step - loss: 1.4811 - sparse_categorical_accuracy: 0.4672 - val_loss: 1.4547 - val_sparse_categorical_accuracy: 0.4787\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "49/49 [==============================] - 36s 741ms/step - loss: 1.4484 - sparse_categorical_accuracy: 0.4814 - val_loss: 1.4345 - val_sparse_categorical_accuracy: 0.4835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/50\n",
            "49/49 [==============================] - 36s 741ms/step - loss: 1.4218 - sparse_categorical_accuracy: 0.4904 - val_loss: 1.4088 - val_sparse_categorical_accuracy: 0.4921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "49/49 [==============================] - 36s 741ms/step - loss: 1.3990 - sparse_categorical_accuracy: 0.5001 - val_loss: 1.3709 - val_sparse_categorical_accuracy: 0.5054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "49/49 [==============================] - 36s 742ms/step - loss: 1.3849 - sparse_categorical_accuracy: 0.5044 - val_loss: 1.3616 - val_sparse_categorical_accuracy: 0.5063\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "49/49 [==============================] - 36s 742ms/step - loss: 1.3588 - sparse_categorical_accuracy: 0.5162 - val_loss: 1.3650 - val_sparse_categorical_accuracy: 0.5137\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "49/49 [==============================] - 36s 741ms/step - loss: 1.3637 - sparse_categorical_accuracy: 0.5097 - val_loss: 1.3573 - val_sparse_categorical_accuracy: 0.5175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "49/49 [==============================] - 36s 742ms/step - loss: 1.3317 - sparse_categorical_accuracy: 0.5216 - val_loss: 1.3032 - val_sparse_categorical_accuracy: 0.5287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/50\n",
            "49/49 [==============================] - 36s 740ms/step - loss: 1.3040 - sparse_categorical_accuracy: 0.5320 - val_loss: 1.3098 - val_sparse_categorical_accuracy: 0.5293\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "49/49 [==============================] - 36s 744ms/step - loss: 1.3010 - sparse_categorical_accuracy: 0.5348 - val_loss: 1.3269 - val_sparse_categorical_accuracy: 0.5275\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 36s 735ms/step - loss: 1.3030 - sparse_categorical_accuracy: 0.5328 - val_loss: 1.2989 - val_sparse_categorical_accuracy: 0.5326\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "49/49 [==============================] - 36s 741ms/step - loss: 1.2935 - sparse_categorical_accuracy: 0.5375 - val_loss: 1.3190 - val_sparse_categorical_accuracy: 0.5257\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 36s 736ms/step - loss: 1.2713 - sparse_categorical_accuracy: 0.5454 - val_loss: 1.2981 - val_sparse_categorical_accuracy: 0.5352\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/50\n",
            "49/49 [==============================] - 36s 740ms/step - loss: 1.2733 - sparse_categorical_accuracy: 0.5437 - val_loss: 1.2918 - val_sparse_categorical_accuracy: 0.5382\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n",
            "49/49 [==============================] - 36s 742ms/step - loss: 1.2665 - sparse_categorical_accuracy: 0.5461 - val_loss: 1.2722 - val_sparse_categorical_accuracy: 0.5452\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "49/49 [==============================] - 36s 740ms/step - loss: 1.2412 - sparse_categorical_accuracy: 0.5578 - val_loss: 1.2569 - val_sparse_categorical_accuracy: 0.5491\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32/50\n",
            "49/49 [==============================] - 36s 724ms/step - loss: 1.2366 - sparse_categorical_accuracy: 0.5605 - val_loss: 1.2306 - val_sparse_categorical_accuracy: 0.5564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33/50\n",
            "49/49 [==============================] - 35s 724ms/step - loss: 1.2293 - sparse_categorical_accuracy: 0.5595 - val_loss: 1.2436 - val_sparse_categorical_accuracy: 0.5639\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_44_layer_call_and_return_conditional_losses while saving (showing 5 of 570). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_16l_32ldim_1lh_32lhdim_1ch_32chdim_3cd_3ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "49/49 [==============================] - 35s 718ms/step - loss: 1.2219 - sparse_categorical_accuracy: 0.5628 - val_loss: 1.2439 - val_sparse_categorical_accuracy: 0.5536\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 35s 724ms/step - loss: 1.2210 - sparse_categorical_accuracy: 0.5625 - val_loss: 1.2514 - val_sparse_categorical_accuracy: 0.5548\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 36s 726ms/step - loss: 1.2077 - sparse_categorical_accuracy: 0.5688 - val_loss: 1.2351 - val_sparse_categorical_accuracy: 0.5617\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 36s 727ms/step - loss: 1.1876 - sparse_categorical_accuracy: 0.5740 - val_loss: 1.2480 - val_sparse_categorical_accuracy: 0.5536\n",
            "Epoch 38/50\n",
            "10/49 [=====>........................] - ETA: 25s - loss: 1.1739 - sparse_categorical_accuracy: 0.5755"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-790c91f63ae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m                                            \u001b[0;34mf'_{cross_depth}cd'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                                            \u001b[0;34mf'_{latent_depth}ld'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                                verbose=1)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-6e3649293d1b>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(_model, _checkpoint_filepath, verbose)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   )\n\u001b[1;32m     43\u001b[0m   \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBjI56VEEMwK",
        "outputId": "aab7a519-61a8-4b87-e147-de288fbb9c9d"
      },
      "source": [
        "learning_rate = 3e-3\n",
        "epochs = 500\n",
        "batch_size=256\n",
        "for max_freq in [image_height]:\n",
        "  for num_freq_bands in [64]:\n",
        "    for num_latents in [64]:\n",
        "      for latent_dim in [32]:\n",
        "        for latent_head_dim in [32]:\n",
        "          for cross_head_dim in [32]:\n",
        "            for cross_depth in [8]:\n",
        "              for latent_depth in [6]:\n",
        "                model = Perceiver(num_classes=num_classes, \n",
        "                                  image_height=image_height, \n",
        "                                  image_width=image_width,\n",
        "                                  max_freq=max_freq,\n",
        "                                  num_freq_bands=num_freq_bands,\n",
        "                                  freq_base=2.0,\n",
        "                                  num_latents=num_latents,\n",
        "                                  latent_dim=latent_dim,\n",
        "                                  latent_heads=1,\n",
        "                                  latent_head_dim=latent_head_dim,\n",
        "                                  cross_heads=1,\n",
        "                                  cross_head_dim=cross_head_dim,\n",
        "                                  attn_dropout=0.2,\n",
        "                                  mlp_dropout=0.2,\n",
        "                                  cross_depth=cross_depth,\n",
        "                                  latent_depth=latent_depth,             \n",
        "                                  weight_tie_layers=True,\n",
        "                                  preprocess=False).model()\n",
        "                model.summary()\n",
        "                train_and_eval(model, \n",
        "                               SAVE_PATH + f'_PERCEIVER' \\\n",
        "                                           f'_{max_freq}fq' \\\n",
        "                                           f'_{num_freq_bands}fqb' \\\n",
        "                                           f'_{num_latents}l' \\\n",
        "                                           f'_{latent_dim}ldim' \\\n",
        "                                           f'_1lh' \\\n",
        "                                           f'_{latent_head_dim}lhdim' \\\n",
        "                                           f'_1ch' \\\n",
        "                                           f'_{cross_head_dim}chdim' \\\n",
        "                                           f'_{cross_depth}cd' \\\n",
        "                                           f'_{latent_depth}ld', \n",
        "                               verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 64, 32)       2048        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 261)    0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 64, 32)       19338       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 64, 32)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 64, 32)       2144        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 64, 32)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 64, 32)       4160        skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 64, 32)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 64, 32)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 64, 32)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 64, 32)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 64, 32)       0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 64, 32)       0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 64, 32)       0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 64, 32)       0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_4 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_4 (Add)      (None, 64, 32)       0           latent_attn_0_4[0][0]            \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_0_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_4 (Add)       (None, 64, 32)       0           latent_mlp_0_4[0][0]             \n",
            "                                                                 skip_latent_attn_0_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_5 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_0_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_5 (Add)      (None, 64, 32)       0           latent_attn_0_5[0][0]            \n",
            "                                                                 skip_latent_mlp_0_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_0_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_5 (Add)       (None, 64, 32)       0           latent_mlp_0_5[0][0]             \n",
            "                                                                 skip_latent_attn_0_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 64, 32)       19338       skip_latent_mlp_0_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_1_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_2_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_3_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_4_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_5_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_6_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 64, 32)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 64, 32)       2144        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 64, 32)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 64, 32)       4160        skip_cross_mlp_1[0][0]           \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "                                                                 skip_cross_mlp_4[0][0]           \n",
            "                                                                 skip_cross_mlp_5[0][0]           \n",
            "                                                                 skip_cross_mlp_6[0][0]           \n",
            "                                                                 skip_cross_mlp_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 64, 32)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_1_0[0][0]        \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "                                                                 skip_latent_mlp_4_0[0][0]        \n",
            "                                                                 skip_latent_mlp_5_0[0][0]        \n",
            "                                                                 skip_latent_mlp_6_0[0][0]        \n",
            "                                                                 skip_latent_mlp_7_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 64, 32)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_1_1[0][0]        \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "                                                                 skip_latent_mlp_4_1[0][0]        \n",
            "                                                                 skip_latent_mlp_5_1[0][0]        \n",
            "                                                                 skip_latent_mlp_6_1[0][0]        \n",
            "                                                                 skip_latent_mlp_7_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 64, 32)       0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 64, 32)       0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_1_2[0][0]        \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "                                                                 skip_latent_mlp_3_2[0][0]        \n",
            "                                                                 skip_latent_mlp_4_2[0][0]        \n",
            "                                                                 skip_latent_mlp_5_2[0][0]        \n",
            "                                                                 skip_latent_mlp_6_2[0][0]        \n",
            "                                                                 skip_latent_mlp_7_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 64, 32)       0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 64, 32)       0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_4 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_1_3[0][0]        \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "                                                                 skip_latent_mlp_3_3[0][0]        \n",
            "                                                                 skip_latent_mlp_4_3[0][0]        \n",
            "                                                                 skip_latent_mlp_5_3[0][0]        \n",
            "                                                                 skip_latent_mlp_6_3[0][0]        \n",
            "                                                                 skip_latent_mlp_7_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_4 (Add)      (None, 64, 32)       0           latent_attn_1_4[0][0]            \n",
            "                                                                 skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_1_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_4 (Add)       (None, 64, 32)       0           latent_mlp_1_4[0][0]             \n",
            "                                                                 skip_latent_attn_1_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_5 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_1_4[0][0]        \n",
            "                                                                 skip_latent_mlp_2_4[0][0]        \n",
            "                                                                 skip_latent_mlp_3_4[0][0]        \n",
            "                                                                 skip_latent_mlp_4_4[0][0]        \n",
            "                                                                 skip_latent_mlp_5_4[0][0]        \n",
            "                                                                 skip_latent_mlp_6_4[0][0]        \n",
            "                                                                 skip_latent_mlp_7_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_5 (Add)      (None, 64, 32)       0           latent_attn_1_5[0][0]            \n",
            "                                                                 skip_latent_mlp_1_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_1_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_5 (Add)       (None, 64, 32)       0           latent_mlp_1_5[0][0]             \n",
            "                                                                 skip_latent_attn_1_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_2 (Add)         (None, 64, 32)       0           cross_attn_1[1][0]               \n",
            "                                                                 skip_latent_mlp_1_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_2 (MLP)               (None, 64, 32)       2144        skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_2 (Add)          (None, 64, 32)       0           cross_mlp_2[0][0]                \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[1][0]            \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_0 (Add)       (None, 64, 32)       0           latent_mlp_2_0[0][0]             \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[1][0]            \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_1 (Add)       (None, 64, 32)       0           latent_mlp_2_1[0][0]             \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_2 (Add)      (None, 64, 32)       0           latent_attn_1_2[1][0]            \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_2 (Add)       (None, 64, 32)       0           latent_mlp_2_2[0][0]             \n",
            "                                                                 skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_3 (Add)      (None, 64, 32)       0           latent_attn_1_3[1][0]            \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_2_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_3 (Add)       (None, 64, 32)       0           latent_mlp_2_3[0][0]             \n",
            "                                                                 skip_latent_attn_2_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_4 (Add)      (None, 64, 32)       0           latent_attn_1_4[1][0]            \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_2_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_4 (Add)       (None, 64, 32)       0           latent_mlp_2_4[0][0]             \n",
            "                                                                 skip_latent_attn_2_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_5 (Add)      (None, 64, 32)       0           latent_attn_1_5[1][0]            \n",
            "                                                                 skip_latent_mlp_2_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_2_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_5 (Add)       (None, 64, 32)       0           latent_mlp_2_5[0][0]             \n",
            "                                                                 skip_latent_attn_2_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_3 (Add)         (None, 64, 32)       0           cross_attn_1[2][0]               \n",
            "                                                                 skip_latent_mlp_2_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_3 (MLP)               (None, 64, 32)       2144        skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_3 (Add)          (None, 64, 32)       0           cross_mlp_3[0][0]                \n",
            "                                                                 skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[2][0]            \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_0 (Add)       (None, 64, 32)       0           latent_mlp_3_0[0][0]             \n",
            "                                                                 skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[2][0]            \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_1 (Add)       (None, 64, 32)       0           latent_mlp_3_1[0][0]             \n",
            "                                                                 skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_2 (Add)      (None, 64, 32)       0           latent_attn_1_2[2][0]            \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_3_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_2 (Add)       (None, 64, 32)       0           latent_mlp_3_2[0][0]             \n",
            "                                                                 skip_latent_attn_3_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_3 (Add)      (None, 64, 32)       0           latent_attn_1_3[2][0]            \n",
            "                                                                 skip_latent_mlp_3_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_3_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_3 (Add)       (None, 64, 32)       0           latent_mlp_3_3[0][0]             \n",
            "                                                                 skip_latent_attn_3_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_4 (Add)      (None, 64, 32)       0           latent_attn_1_4[2][0]            \n",
            "                                                                 skip_latent_mlp_3_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_3_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_4 (Add)       (None, 64, 32)       0           latent_mlp_3_4[0][0]             \n",
            "                                                                 skip_latent_attn_3_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_5 (Add)      (None, 64, 32)       0           latent_attn_1_5[2][0]            \n",
            "                                                                 skip_latent_mlp_3_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_3_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_5 (Add)       (None, 64, 32)       0           latent_mlp_3_5[0][0]             \n",
            "                                                                 skip_latent_attn_3_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_4 (Add)         (None, 64, 32)       0           cross_attn_1[3][0]               \n",
            "                                                                 skip_latent_mlp_3_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_4 (MLP)               (None, 64, 32)       2144        skip_cross_attn_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_4 (Add)          (None, 64, 32)       0           cross_mlp_4[0][0]                \n",
            "                                                                 skip_cross_attn_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[3][0]            \n",
            "                                                                 skip_cross_mlp_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_4_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_0 (Add)       (None, 64, 32)       0           latent_mlp_4_0[0][0]             \n",
            "                                                                 skip_latent_attn_4_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[3][0]            \n",
            "                                                                 skip_latent_mlp_4_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_4_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_1 (Add)       (None, 64, 32)       0           latent_mlp_4_1[0][0]             \n",
            "                                                                 skip_latent_attn_4_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_2 (Add)      (None, 64, 32)       0           latent_attn_1_2[3][0]            \n",
            "                                                                 skip_latent_mlp_4_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_4_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_2 (Add)       (None, 64, 32)       0           latent_mlp_4_2[0][0]             \n",
            "                                                                 skip_latent_attn_4_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_3 (Add)      (None, 64, 32)       0           latent_attn_1_3[3][0]            \n",
            "                                                                 skip_latent_mlp_4_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_4_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_3 (Add)       (None, 64, 32)       0           latent_mlp_4_3[0][0]             \n",
            "                                                                 skip_latent_attn_4_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_4 (Add)      (None, 64, 32)       0           latent_attn_1_4[3][0]            \n",
            "                                                                 skip_latent_mlp_4_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_4_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_4 (Add)       (None, 64, 32)       0           latent_mlp_4_4[0][0]             \n",
            "                                                                 skip_latent_attn_4_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_5 (Add)      (None, 64, 32)       0           latent_attn_1_5[3][0]            \n",
            "                                                                 skip_latent_mlp_4_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_4_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_5 (Add)       (None, 64, 32)       0           latent_mlp_4_5[0][0]             \n",
            "                                                                 skip_latent_attn_4_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_5 (Add)         (None, 64, 32)       0           cross_attn_1[4][0]               \n",
            "                                                                 skip_latent_mlp_4_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_5 (MLP)               (None, 64, 32)       2144        skip_cross_attn_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_5 (Add)          (None, 64, 32)       0           cross_mlp_5[0][0]                \n",
            "                                                                 skip_cross_attn_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[4][0]            \n",
            "                                                                 skip_cross_mlp_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_5_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_0 (Add)       (None, 64, 32)       0           latent_mlp_5_0[0][0]             \n",
            "                                                                 skip_latent_attn_5_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[4][0]            \n",
            "                                                                 skip_latent_mlp_5_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_5_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_1 (Add)       (None, 64, 32)       0           latent_mlp_5_1[0][0]             \n",
            "                                                                 skip_latent_attn_5_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_2 (Add)      (None, 64, 32)       0           latent_attn_1_2[4][0]            \n",
            "                                                                 skip_latent_mlp_5_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_5_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_2 (Add)       (None, 64, 32)       0           latent_mlp_5_2[0][0]             \n",
            "                                                                 skip_latent_attn_5_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_3 (Add)      (None, 64, 32)       0           latent_attn_1_3[4][0]            \n",
            "                                                                 skip_latent_mlp_5_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_5_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_3 (Add)       (None, 64, 32)       0           latent_mlp_5_3[0][0]             \n",
            "                                                                 skip_latent_attn_5_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_4 (Add)      (None, 64, 32)       0           latent_attn_1_4[4][0]            \n",
            "                                                                 skip_latent_mlp_5_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_5_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_4 (Add)       (None, 64, 32)       0           latent_mlp_5_4[0][0]             \n",
            "                                                                 skip_latent_attn_5_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_5 (Add)      (None, 64, 32)       0           latent_attn_1_5[4][0]            \n",
            "                                                                 skip_latent_mlp_5_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_5_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_5 (Add)       (None, 64, 32)       0           latent_mlp_5_5[0][0]             \n",
            "                                                                 skip_latent_attn_5_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_6 (Add)         (None, 64, 32)       0           cross_attn_1[5][0]               \n",
            "                                                                 skip_latent_mlp_5_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_6 (MLP)               (None, 64, 32)       2144        skip_cross_attn_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_6 (Add)          (None, 64, 32)       0           cross_mlp_6[0][0]                \n",
            "                                                                 skip_cross_attn_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[5][0]            \n",
            "                                                                 skip_cross_mlp_6[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_6_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_0 (Add)       (None, 64, 32)       0           latent_mlp_6_0[0][0]             \n",
            "                                                                 skip_latent_attn_6_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[5][0]            \n",
            "                                                                 skip_latent_mlp_6_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_6_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_1 (Add)       (None, 64, 32)       0           latent_mlp_6_1[0][0]             \n",
            "                                                                 skip_latent_attn_6_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_2 (Add)      (None, 64, 32)       0           latent_attn_1_2[5][0]            \n",
            "                                                                 skip_latent_mlp_6_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_6_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_2 (Add)       (None, 64, 32)       0           latent_mlp_6_2[0][0]             \n",
            "                                                                 skip_latent_attn_6_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_3 (Add)      (None, 64, 32)       0           latent_attn_1_3[5][0]            \n",
            "                                                                 skip_latent_mlp_6_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_6_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_3 (Add)       (None, 64, 32)       0           latent_mlp_6_3[0][0]             \n",
            "                                                                 skip_latent_attn_6_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_4 (Add)      (None, 64, 32)       0           latent_attn_1_4[5][0]            \n",
            "                                                                 skip_latent_mlp_6_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_6_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_4 (Add)       (None, 64, 32)       0           latent_mlp_6_4[0][0]             \n",
            "                                                                 skip_latent_attn_6_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_5 (Add)      (None, 64, 32)       0           latent_attn_1_5[5][0]            \n",
            "                                                                 skip_latent_mlp_6_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_6_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_5 (Add)       (None, 64, 32)       0           latent_mlp_6_5[0][0]             \n",
            "                                                                 skip_latent_attn_6_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_7 (Add)         (None, 64, 32)       0           cross_attn_1[6][0]               \n",
            "                                                                 skip_latent_mlp_6_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_7 (MLP)               (None, 64, 32)       2144        skip_cross_attn_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_7 (Add)          (None, 64, 32)       0           cross_mlp_7[0][0]                \n",
            "                                                                 skip_cross_attn_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[6][0]            \n",
            "                                                                 skip_cross_mlp_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_7_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_0 (Add)       (None, 64, 32)       0           latent_mlp_7_0[0][0]             \n",
            "                                                                 skip_latent_attn_7_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[6][0]            \n",
            "                                                                 skip_latent_mlp_7_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_7_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_1 (Add)       (None, 64, 32)       0           latent_mlp_7_1[0][0]             \n",
            "                                                                 skip_latent_attn_7_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_2 (Add)      (None, 64, 32)       0           latent_attn_1_2[6][0]            \n",
            "                                                                 skip_latent_mlp_7_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_7_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_2 (Add)       (None, 64, 32)       0           latent_mlp_7_2[0][0]             \n",
            "                                                                 skip_latent_attn_7_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_3 (Add)      (None, 64, 32)       0           latent_attn_1_3[6][0]            \n",
            "                                                                 skip_latent_mlp_7_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_7_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_3 (Add)       (None, 64, 32)       0           latent_mlp_7_3[0][0]             \n",
            "                                                                 skip_latent_attn_7_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_4 (Add)      (None, 64, 32)       0           latent_attn_1_4[6][0]            \n",
            "                                                                 skip_latent_mlp_7_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_7_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_4 (Add)       (None, 64, 32)       0           latent_mlp_7_4[0][0]             \n",
            "                                                                 skip_latent_attn_7_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_5 (Add)      (None, 64, 32)       0           latent_attn_1_5[6][0]            \n",
            "                                                                 skip_latent_mlp_7_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_7_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_5 (Add)       (None, 64, 32)       0           latent_mlp_7_5[0][0]             \n",
            "                                                                 skip_latent_attn_7_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 32)        0           skip_latent_mlp_7_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        330         avgpool[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 211,038\n",
            "Trainable params: 211,038\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "196/196 [==============================] - 207s 755ms/step - loss: 11.3680 - sparse_categorical_accuracy: 0.1024 - val_loss: 2.3236 - val_sparse_categorical_accuracy: 0.2116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 2.2684 - sparse_categorical_accuracy: 0.1832 - val_loss: 2.1547 - val_sparse_categorical_accuracy: 0.2378\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/500\n",
            "196/196 [==============================] - 130s 663ms/step - loss: 2.0486 - sparse_categorical_accuracy: 0.2605 - val_loss: 1.9349 - val_sparse_categorical_accuracy: 0.2973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 1.9228 - sparse_categorical_accuracy: 0.3011 - val_loss: 1.8142 - val_sparse_categorical_accuracy: 0.3434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 1.8311 - sparse_categorical_accuracy: 0.3333 - val_loss: 1.8247 - val_sparse_categorical_accuracy: 0.3511\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/500\n",
            "196/196 [==============================] - 130s 663ms/step - loss: 1.7310 - sparse_categorical_accuracy: 0.3732 - val_loss: 1.6788 - val_sparse_categorical_accuracy: 0.4037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/500\n",
            "196/196 [==============================] - 130s 663ms/step - loss: 1.6587 - sparse_categorical_accuracy: 0.4004 - val_loss: 1.6454 - val_sparse_categorical_accuracy: 0.4090\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 1.6040 - sparse_categorical_accuracy: 0.4237 - val_loss: 1.5904 - val_sparse_categorical_accuracy: 0.4392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/500\n",
            "196/196 [==============================] - 130s 663ms/step - loss: 1.5729 - sparse_categorical_accuracy: 0.4341 - val_loss: 1.5563 - val_sparse_categorical_accuracy: 0.4470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 1.5127 - sparse_categorical_accuracy: 0.4580 - val_loss: 1.5505 - val_sparse_categorical_accuracy: 0.4511\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 1.4732 - sparse_categorical_accuracy: 0.4735 - val_loss: 1.4846 - val_sparse_categorical_accuracy: 0.4791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 1.4347 - sparse_categorical_accuracy: 0.4865 - val_loss: 1.4846 - val_sparse_categorical_accuracy: 0.4789\n",
            "Epoch 13/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 1.3941 - sparse_categorical_accuracy: 0.4991 - val_loss: 1.3936 - val_sparse_categorical_accuracy: 0.5092\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/500\n",
            "196/196 [==============================] - 130s 663ms/step - loss: 1.3482 - sparse_categorical_accuracy: 0.5172 - val_loss: 1.4313 - val_sparse_categorical_accuracy: 0.5083\n",
            "Epoch 15/500\n",
            "196/196 [==============================] - 130s 663ms/step - loss: 1.3197 - sparse_categorical_accuracy: 0.5252 - val_loss: 1.3863 - val_sparse_categorical_accuracy: 0.5133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/500\n",
            "196/196 [==============================] - 128s 655ms/step - loss: 1.2832 - sparse_categorical_accuracy: 0.5401 - val_loss: 1.3489 - val_sparse_categorical_accuracy: 0.5227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/500\n",
            "196/196 [==============================] - 128s 652ms/step - loss: 1.2573 - sparse_categorical_accuracy: 0.5502 - val_loss: 1.3412 - val_sparse_categorical_accuracy: 0.5258\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/500\n",
            "196/196 [==============================] - 129s 660ms/step - loss: 1.2417 - sparse_categorical_accuracy: 0.5575 - val_loss: 1.3287 - val_sparse_categorical_accuracy: 0.5312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/500\n",
            "196/196 [==============================] - 130s 664ms/step - loss: 1.2132 - sparse_categorical_accuracy: 0.5643 - val_loss: 1.2911 - val_sparse_categorical_accuracy: 0.5465\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/500\n",
            "196/196 [==============================] - 130s 664ms/step - loss: 1.2007 - sparse_categorical_accuracy: 0.5699 - val_loss: 1.2868 - val_sparse_categorical_accuracy: 0.5449\n",
            "Epoch 21/500\n",
            "196/196 [==============================] - 130s 663ms/step - loss: 1.1825 - sparse_categorical_accuracy: 0.5822 - val_loss: 1.2737 - val_sparse_categorical_accuracy: 0.5556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 1.1552 - sparse_categorical_accuracy: 0.5879 - val_loss: 1.2910 - val_sparse_categorical_accuracy: 0.5500\n",
            "Epoch 23/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 1.1413 - sparse_categorical_accuracy: 0.5928 - val_loss: 1.2281 - val_sparse_categorical_accuracy: 0.5696\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 1.1196 - sparse_categorical_accuracy: 0.6010 - val_loss: 1.2235 - val_sparse_categorical_accuracy: 0.5727\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25/500\n",
            "196/196 [==============================] - 130s 665ms/step - loss: 1.0928 - sparse_categorical_accuracy: 0.6103 - val_loss: 1.2254 - val_sparse_categorical_accuracy: 0.5668\n",
            "Epoch 26/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 1.0944 - sparse_categorical_accuracy: 0.6088 - val_loss: 1.2848 - val_sparse_categorical_accuracy: 0.5576\n",
            "Epoch 27/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 1.0885 - sparse_categorical_accuracy: 0.6116 - val_loss: 1.2275 - val_sparse_categorical_accuracy: 0.5715\n",
            "Epoch 28/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 1.0797 - sparse_categorical_accuracy: 0.6165 - val_loss: 1.2242 - val_sparse_categorical_accuracy: 0.5799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29/500\n",
            "196/196 [==============================] - 130s 663ms/step - loss: 1.0617 - sparse_categorical_accuracy: 0.6230 - val_loss: 1.2538 - val_sparse_categorical_accuracy: 0.5681\n",
            "Epoch 30/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 1.0720 - sparse_categorical_accuracy: 0.6168 - val_loss: 1.2321 - val_sparse_categorical_accuracy: 0.5699\n",
            "Epoch 31/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 1.0757 - sparse_categorical_accuracy: 0.6185 - val_loss: 1.2429 - val_sparse_categorical_accuracy: 0.5717\n",
            "Epoch 32/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 1.0444 - sparse_categorical_accuracy: 0.6279 - val_loss: 1.2277 - val_sparse_categorical_accuracy: 0.5774\n",
            "Epoch 33/500\n",
            "196/196 [==============================] - 129s 659ms/step - loss: 1.0360 - sparse_categorical_accuracy: 0.6285 - val_loss: 1.2795 - val_sparse_categorical_accuracy: 0.5624\n",
            "Epoch 34/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 1.0272 - sparse_categorical_accuracy: 0.6332 - val_loss: 1.3177 - val_sparse_categorical_accuracy: 0.5560\n",
            "Epoch 35/500\n",
            "196/196 [==============================] - 129s 659ms/step - loss: 1.0231 - sparse_categorical_accuracy: 0.6316 - val_loss: 1.2344 - val_sparse_categorical_accuracy: 0.5748\n",
            "Epoch 36/500\n",
            "196/196 [==============================] - 129s 660ms/step - loss: 1.0079 - sparse_categorical_accuracy: 0.6403 - val_loss: 1.2567 - val_sparse_categorical_accuracy: 0.5747\n",
            "Epoch 37/500\n",
            "196/196 [==============================] - 130s 663ms/step - loss: 0.9656 - sparse_categorical_accuracy: 0.6573 - val_loss: 1.2615 - val_sparse_categorical_accuracy: 0.5722\n",
            "Epoch 38/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 0.9812 - sparse_categorical_accuracy: 0.6498 - val_loss: 1.2310 - val_sparse_categorical_accuracy: 0.5786\n",
            "Epoch 39/500\n",
            "196/196 [==============================] - 130s 664ms/step - loss: 0.9593 - sparse_categorical_accuracy: 0.6608 - val_loss: 1.2113 - val_sparse_categorical_accuracy: 0.5871\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40/500\n",
            "196/196 [==============================] - 131s 668ms/step - loss: 0.9349 - sparse_categorical_accuracy: 0.6657 - val_loss: 1.2278 - val_sparse_categorical_accuracy: 0.5831\n",
            "Epoch 41/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 0.9315 - sparse_categorical_accuracy: 0.6653 - val_loss: 1.2443 - val_sparse_categorical_accuracy: 0.5831\n",
            "Epoch 42/500\n",
            "196/196 [==============================] - 130s 664ms/step - loss: 0.9280 - sparse_categorical_accuracy: 0.6671 - val_loss: 1.2561 - val_sparse_categorical_accuracy: 0.5847\n",
            "Epoch 43/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 0.8963 - sparse_categorical_accuracy: 0.6783 - val_loss: 1.2371 - val_sparse_categorical_accuracy: 0.5889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 44/500\n",
            "196/196 [==============================] - 130s 663ms/step - loss: 0.9076 - sparse_categorical_accuracy: 0.6757 - val_loss: 1.2128 - val_sparse_categorical_accuracy: 0.5938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 45/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 0.8956 - sparse_categorical_accuracy: 0.6802 - val_loss: 1.2247 - val_sparse_categorical_accuracy: 0.5878\n",
            "Epoch 46/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 0.8820 - sparse_categorical_accuracy: 0.6829 - val_loss: 1.2364 - val_sparse_categorical_accuracy: 0.5961\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 0.8685 - sparse_categorical_accuracy: 0.6901 - val_loss: 1.3140 - val_sparse_categorical_accuracy: 0.5723\n",
            "Epoch 48/500\n",
            "196/196 [==============================] - 130s 663ms/step - loss: 0.9032 - sparse_categorical_accuracy: 0.6754 - val_loss: 1.2527 - val_sparse_categorical_accuracy: 0.5915\n",
            "Epoch 49/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 0.8719 - sparse_categorical_accuracy: 0.6896 - val_loss: 1.2288 - val_sparse_categorical_accuracy: 0.6020\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_and_return_conditional_losses, ln_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, dense_144_layer_call_and_return_conditional_losses while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50/500\n",
            "196/196 [==============================] - 130s 663ms/step - loss: 0.8370 - sparse_categorical_accuracy: 0.6998 - val_loss: 1.2914 - val_sparse_categorical_accuracy: 0.5900\n",
            "Epoch 51/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 0.8369 - sparse_categorical_accuracy: 0.6986 - val_loss: 1.2738 - val_sparse_categorical_accuracy: 0.5940\n",
            "Epoch 52/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 0.8239 - sparse_categorical_accuracy: 0.7066 - val_loss: 1.3028 - val_sparse_categorical_accuracy: 0.5982\n",
            "Epoch 53/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 0.8164 - sparse_categorical_accuracy: 0.7093 - val_loss: 1.3003 - val_sparse_categorical_accuracy: 0.5911\n",
            "Epoch 54/500\n",
            "196/196 [==============================] - 130s 663ms/step - loss: 0.7973 - sparse_categorical_accuracy: 0.7138 - val_loss: 1.2848 - val_sparse_categorical_accuracy: 0.5949\n",
            "Epoch 55/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 0.7836 - sparse_categorical_accuracy: 0.7220 - val_loss: 1.3312 - val_sparse_categorical_accuracy: 0.5817\n",
            "Epoch 56/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 0.7942 - sparse_categorical_accuracy: 0.7168 - val_loss: 1.3119 - val_sparse_categorical_accuracy: 0.5949\n",
            "Epoch 57/500\n",
            "196/196 [==============================] - 130s 662ms/step - loss: 0.8273 - sparse_categorical_accuracy: 0.7006 - val_loss: 1.3299 - val_sparse_categorical_accuracy: 0.5795\n",
            "Epoch 58/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 0.7815 - sparse_categorical_accuracy: 0.7211 - val_loss: 1.3190 - val_sparse_categorical_accuracy: 0.5881\n",
            "Epoch 59/500\n",
            "196/196 [==============================] - 130s 661ms/step - loss: 0.7964 - sparse_categorical_accuracy: 0.7165 - val_loss: 1.3734 - val_sparse_categorical_accuracy: 0.5819\n",
            "Epoch 60/500\n",
            " 15/196 [=>............................] - ETA: 1:50 - loss: 0.7554 - sparse_categorical_accuracy: 0.7266"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tS88ZvLfbuQV",
        "outputId": "dbcfcc91-67fc-4575-d15f-7e3f7c83ab98"
      },
      "source": [
        "learning_rate = 3e-3\n",
        "epochs = 500\n",
        "batch_size=256\n",
        "for max_freq in [image_height]:\n",
        "  for num_freq_bands in [64]:\n",
        "    for num_latents in [64]:\n",
        "      for latent_dim in [32]:\n",
        "        for latent_head_dim in [32]:\n",
        "          for cross_head_dim in [32]:\n",
        "            for cross_depth in [8]:\n",
        "              for latent_depth in [6]:\n",
        "                # model = Perceiver(num_classes=num_classes, \n",
        "                #                   image_height=image_height, \n",
        "                #                   image_width=image_width,\n",
        "                #                   max_freq=max_freq,\n",
        "                #                   num_freq_bands=num_freq_bands,\n",
        "                #                   freq_base=2.0,\n",
        "                #                   num_latents=num_latents,\n",
        "                #                   latent_dim=latent_dim,\n",
        "                #                   latent_heads=1,\n",
        "                #                   latent_head_dim=latent_head_dim,\n",
        "                #                   cross_heads=1,\n",
        "                #                   cross_head_dim=cross_head_dim,\n",
        "                #                   attn_dropout=0.2,\n",
        "                #                   mlp_dropout=0.2,\n",
        "                #                   cross_depth=cross_depth,\n",
        "                #                   latent_depth=latent_depth,             \n",
        "                #                   weight_tie_layers=True,\n",
        "                #                   preprocess=False).model()\n",
        "                # model.summary()\n",
        "                # train_and_eval(model, \n",
        "                #                SAVE_PATH + f'_PERCEIVER' \\\n",
        "                #                            f'_{max_freq}fq' \\\n",
        "                #                            f'_{num_freq_bands}fqb' \\\n",
        "                #                            f'_{num_latents}l' \\\n",
        "                #                            f'_{latent_dim}ldim' \\\n",
        "                #                            f'_1lh' \\\n",
        "                #                            f'_{latent_head_dim}lhdim' \\\n",
        "                #                            f'_1ch' \\\n",
        "                #                            f'_{cross_head_dim}chdim' \\\n",
        "                #                            f'_{cross_depth}cd' \\\n",
        "                #                            f'_{latent_depth}ld', \n",
        "                #                verbose=1)\n",
        "                model = tf.keras.models.load_model(\n",
        "                               SAVE_PATH + f'_PERCEIVER' \\\n",
        "                                           f'_{max_freq}fq' \\\n",
        "                                           f'_{num_freq_bands}fqb' \\\n",
        "                                           f'_{num_latents}l' \\\n",
        "                                           f'_{latent_dim}ldim' \\\n",
        "                                           f'_1lh' \\\n",
        "                                           f'_{latent_head_dim}lhdim' \\\n",
        "                                           f'_1ch' \\\n",
        "                                           f'_{cross_head_dim}chdim' \\\n",
        "                                           f'_{cross_depth}cd' \\\n",
        "                                           f'_{latent_depth}ld')\n",
        "                model.summary()\n",
        "                train_and_eval(model, \n",
        "                               SAVE_PATH + f'_PERCEIVER' \\\n",
        "                                           f'_{max_freq}fq' \\\n",
        "                                           f'_{num_freq_bands}fqb' \\\n",
        "                                           f'_{num_latents}l' \\\n",
        "                                           f'_{latent_dim}ldim' \\\n",
        "                                           f'_1lh' \\\n",
        "                                           f'_{latent_head_dim}lhdim' \\\n",
        "                                           f'_1ch' \\\n",
        "                                           f'_{cross_head_dim}chdim' \\\n",
        "                                           f'_{cross_depth}cd' \\\n",
        "                                           f'_{latent_depth}ld', \n",
        "                               verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 64, 32)       2048        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 261)    0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 64, 32)       19338       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 64, 32)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 64, 32)       2144        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 64, 32)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 64, 32)       4160        skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 64, 32)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 64, 32)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 64, 32)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 64, 32)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 64, 32)       0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 64, 32)       0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 64, 32)       0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 64, 32)       0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_4 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_4 (Add)      (None, 64, 32)       0           latent_attn_0_4[0][0]            \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_0_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_4 (Add)       (None, 64, 32)       0           latent_mlp_0_4[0][0]             \n",
            "                                                                 skip_latent_attn_0_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_5 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_0_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_5 (Add)      (None, 64, 32)       0           latent_attn_0_5[0][0]            \n",
            "                                                                 skip_latent_mlp_0_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_0_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_5 (Add)       (None, 64, 32)       0           latent_mlp_0_5[0][0]             \n",
            "                                                                 skip_latent_attn_0_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 64, 32)       19338       skip_latent_mlp_0_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_1_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_2_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_3_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_4_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_5_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_6_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 64, 32)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 64, 32)       2144        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 64, 32)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 64, 32)       4160        skip_cross_mlp_1[0][0]           \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "                                                                 skip_cross_mlp_4[0][0]           \n",
            "                                                                 skip_cross_mlp_5[0][0]           \n",
            "                                                                 skip_cross_mlp_6[0][0]           \n",
            "                                                                 skip_cross_mlp_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 64, 32)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_1_0[0][0]        \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "                                                                 skip_latent_mlp_4_0[0][0]        \n",
            "                                                                 skip_latent_mlp_5_0[0][0]        \n",
            "                                                                 skip_latent_mlp_6_0[0][0]        \n",
            "                                                                 skip_latent_mlp_7_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 64, 32)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_1_1[0][0]        \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "                                                                 skip_latent_mlp_4_1[0][0]        \n",
            "                                                                 skip_latent_mlp_5_1[0][0]        \n",
            "                                                                 skip_latent_mlp_6_1[0][0]        \n",
            "                                                                 skip_latent_mlp_7_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 64, 32)       0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 64, 32)       0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_1_2[0][0]        \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "                                                                 skip_latent_mlp_3_2[0][0]        \n",
            "                                                                 skip_latent_mlp_4_2[0][0]        \n",
            "                                                                 skip_latent_mlp_5_2[0][0]        \n",
            "                                                                 skip_latent_mlp_6_2[0][0]        \n",
            "                                                                 skip_latent_mlp_7_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 64, 32)       0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 64, 32)       0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_4 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_1_3[0][0]        \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "                                                                 skip_latent_mlp_3_3[0][0]        \n",
            "                                                                 skip_latent_mlp_4_3[0][0]        \n",
            "                                                                 skip_latent_mlp_5_3[0][0]        \n",
            "                                                                 skip_latent_mlp_6_3[0][0]        \n",
            "                                                                 skip_latent_mlp_7_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_4 (Add)      (None, 64, 32)       0           latent_attn_1_4[0][0]            \n",
            "                                                                 skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_1_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_4 (Add)       (None, 64, 32)       0           latent_mlp_1_4[0][0]             \n",
            "                                                                 skip_latent_attn_1_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_5 (Transformer)   (None, 64, 32)       4160        skip_latent_mlp_1_4[0][0]        \n",
            "                                                                 skip_latent_mlp_2_4[0][0]        \n",
            "                                                                 skip_latent_mlp_3_4[0][0]        \n",
            "                                                                 skip_latent_mlp_4_4[0][0]        \n",
            "                                                                 skip_latent_mlp_5_4[0][0]        \n",
            "                                                                 skip_latent_mlp_6_4[0][0]        \n",
            "                                                                 skip_latent_mlp_7_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_5 (Add)      (None, 64, 32)       0           latent_attn_1_5[0][0]            \n",
            "                                                                 skip_latent_mlp_1_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_1_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_5 (Add)       (None, 64, 32)       0           latent_mlp_1_5[0][0]             \n",
            "                                                                 skip_latent_attn_1_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_2 (Add)         (None, 64, 32)       0           cross_attn_1[1][0]               \n",
            "                                                                 skip_latent_mlp_1_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_2 (MLP)               (None, 64, 32)       2144        skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_2 (Add)          (None, 64, 32)       0           cross_mlp_2[0][0]                \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[1][0]            \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_0 (Add)       (None, 64, 32)       0           latent_mlp_2_0[0][0]             \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[1][0]            \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_1 (Add)       (None, 64, 32)       0           latent_mlp_2_1[0][0]             \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_2 (Add)      (None, 64, 32)       0           latent_attn_1_2[1][0]            \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_2 (Add)       (None, 64, 32)       0           latent_mlp_2_2[0][0]             \n",
            "                                                                 skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_3 (Add)      (None, 64, 32)       0           latent_attn_1_3[1][0]            \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_2_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_3 (Add)       (None, 64, 32)       0           latent_mlp_2_3[0][0]             \n",
            "                                                                 skip_latent_attn_2_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_4 (Add)      (None, 64, 32)       0           latent_attn_1_4[1][0]            \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_2_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_4 (Add)       (None, 64, 32)       0           latent_mlp_2_4[0][0]             \n",
            "                                                                 skip_latent_attn_2_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_5 (Add)      (None, 64, 32)       0           latent_attn_1_5[1][0]            \n",
            "                                                                 skip_latent_mlp_2_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_2_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_5 (Add)       (None, 64, 32)       0           latent_mlp_2_5[0][0]             \n",
            "                                                                 skip_latent_attn_2_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_3 (Add)         (None, 64, 32)       0           cross_attn_1[2][0]               \n",
            "                                                                 skip_latent_mlp_2_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_3 (MLP)               (None, 64, 32)       2144        skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_3 (Add)          (None, 64, 32)       0           cross_mlp_3[0][0]                \n",
            "                                                                 skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[2][0]            \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_0 (Add)       (None, 64, 32)       0           latent_mlp_3_0[0][0]             \n",
            "                                                                 skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[2][0]            \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_1 (Add)       (None, 64, 32)       0           latent_mlp_3_1[0][0]             \n",
            "                                                                 skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_2 (Add)      (None, 64, 32)       0           latent_attn_1_2[2][0]            \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_3_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_2 (Add)       (None, 64, 32)       0           latent_mlp_3_2[0][0]             \n",
            "                                                                 skip_latent_attn_3_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_3 (Add)      (None, 64, 32)       0           latent_attn_1_3[2][0]            \n",
            "                                                                 skip_latent_mlp_3_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_3_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_3 (Add)       (None, 64, 32)       0           latent_mlp_3_3[0][0]             \n",
            "                                                                 skip_latent_attn_3_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_4 (Add)      (None, 64, 32)       0           latent_attn_1_4[2][0]            \n",
            "                                                                 skip_latent_mlp_3_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_3_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_4 (Add)       (None, 64, 32)       0           latent_mlp_3_4[0][0]             \n",
            "                                                                 skip_latent_attn_3_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_5 (Add)      (None, 64, 32)       0           latent_attn_1_5[2][0]            \n",
            "                                                                 skip_latent_mlp_3_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_3_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_5 (Add)       (None, 64, 32)       0           latent_mlp_3_5[0][0]             \n",
            "                                                                 skip_latent_attn_3_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_4 (Add)         (None, 64, 32)       0           cross_attn_1[3][0]               \n",
            "                                                                 skip_latent_mlp_3_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_4 (MLP)               (None, 64, 32)       2144        skip_cross_attn_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_4 (Add)          (None, 64, 32)       0           cross_mlp_4[0][0]                \n",
            "                                                                 skip_cross_attn_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[3][0]            \n",
            "                                                                 skip_cross_mlp_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_4_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_0 (Add)       (None, 64, 32)       0           latent_mlp_4_0[0][0]             \n",
            "                                                                 skip_latent_attn_4_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[3][0]            \n",
            "                                                                 skip_latent_mlp_4_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_4_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_1 (Add)       (None, 64, 32)       0           latent_mlp_4_1[0][0]             \n",
            "                                                                 skip_latent_attn_4_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_2 (Add)      (None, 64, 32)       0           latent_attn_1_2[3][0]            \n",
            "                                                                 skip_latent_mlp_4_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_4_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_2 (Add)       (None, 64, 32)       0           latent_mlp_4_2[0][0]             \n",
            "                                                                 skip_latent_attn_4_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_3 (Add)      (None, 64, 32)       0           latent_attn_1_3[3][0]            \n",
            "                                                                 skip_latent_mlp_4_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_4_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_3 (Add)       (None, 64, 32)       0           latent_mlp_4_3[0][0]             \n",
            "                                                                 skip_latent_attn_4_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_4 (Add)      (None, 64, 32)       0           latent_attn_1_4[3][0]            \n",
            "                                                                 skip_latent_mlp_4_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_4_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_4 (Add)       (None, 64, 32)       0           latent_mlp_4_4[0][0]             \n",
            "                                                                 skip_latent_attn_4_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_5 (Add)      (None, 64, 32)       0           latent_attn_1_5[3][0]            \n",
            "                                                                 skip_latent_mlp_4_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_4_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_5 (Add)       (None, 64, 32)       0           latent_mlp_4_5[0][0]             \n",
            "                                                                 skip_latent_attn_4_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_5 (Add)         (None, 64, 32)       0           cross_attn_1[4][0]               \n",
            "                                                                 skip_latent_mlp_4_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_5 (MLP)               (None, 64, 32)       2144        skip_cross_attn_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_5 (Add)          (None, 64, 32)       0           cross_mlp_5[0][0]                \n",
            "                                                                 skip_cross_attn_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[4][0]            \n",
            "                                                                 skip_cross_mlp_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_5_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_0 (Add)       (None, 64, 32)       0           latent_mlp_5_0[0][0]             \n",
            "                                                                 skip_latent_attn_5_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[4][0]            \n",
            "                                                                 skip_latent_mlp_5_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_5_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_1 (Add)       (None, 64, 32)       0           latent_mlp_5_1[0][0]             \n",
            "                                                                 skip_latent_attn_5_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_2 (Add)      (None, 64, 32)       0           latent_attn_1_2[4][0]            \n",
            "                                                                 skip_latent_mlp_5_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_5_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_2 (Add)       (None, 64, 32)       0           latent_mlp_5_2[0][0]             \n",
            "                                                                 skip_latent_attn_5_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_3 (Add)      (None, 64, 32)       0           latent_attn_1_3[4][0]            \n",
            "                                                                 skip_latent_mlp_5_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_5_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_3 (Add)       (None, 64, 32)       0           latent_mlp_5_3[0][0]             \n",
            "                                                                 skip_latent_attn_5_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_4 (Add)      (None, 64, 32)       0           latent_attn_1_4[4][0]            \n",
            "                                                                 skip_latent_mlp_5_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_5_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_4 (Add)       (None, 64, 32)       0           latent_mlp_5_4[0][0]             \n",
            "                                                                 skip_latent_attn_5_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_5 (Add)      (None, 64, 32)       0           latent_attn_1_5[4][0]            \n",
            "                                                                 skip_latent_mlp_5_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_5_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_5 (Add)       (None, 64, 32)       0           latent_mlp_5_5[0][0]             \n",
            "                                                                 skip_latent_attn_5_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_6 (Add)         (None, 64, 32)       0           cross_attn_1[5][0]               \n",
            "                                                                 skip_latent_mlp_5_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_6 (MLP)               (None, 64, 32)       2144        skip_cross_attn_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_6 (Add)          (None, 64, 32)       0           cross_mlp_6[0][0]                \n",
            "                                                                 skip_cross_attn_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[5][0]            \n",
            "                                                                 skip_cross_mlp_6[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_6_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_0 (Add)       (None, 64, 32)       0           latent_mlp_6_0[0][0]             \n",
            "                                                                 skip_latent_attn_6_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[5][0]            \n",
            "                                                                 skip_latent_mlp_6_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_6_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_1 (Add)       (None, 64, 32)       0           latent_mlp_6_1[0][0]             \n",
            "                                                                 skip_latent_attn_6_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_2 (Add)      (None, 64, 32)       0           latent_attn_1_2[5][0]            \n",
            "                                                                 skip_latent_mlp_6_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_6_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_2 (Add)       (None, 64, 32)       0           latent_mlp_6_2[0][0]             \n",
            "                                                                 skip_latent_attn_6_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_3 (Add)      (None, 64, 32)       0           latent_attn_1_3[5][0]            \n",
            "                                                                 skip_latent_mlp_6_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_6_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_3 (Add)       (None, 64, 32)       0           latent_mlp_6_3[0][0]             \n",
            "                                                                 skip_latent_attn_6_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_4 (Add)      (None, 64, 32)       0           latent_attn_1_4[5][0]            \n",
            "                                                                 skip_latent_mlp_6_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_6_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_4 (Add)       (None, 64, 32)       0           latent_mlp_6_4[0][0]             \n",
            "                                                                 skip_latent_attn_6_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_5 (Add)      (None, 64, 32)       0           latent_attn_1_5[5][0]            \n",
            "                                                                 skip_latent_mlp_6_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_6_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_5 (Add)       (None, 64, 32)       0           latent_mlp_6_5[0][0]             \n",
            "                                                                 skip_latent_attn_6_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_7 (Add)         (None, 64, 32)       0           cross_attn_1[6][0]               \n",
            "                                                                 skip_latent_mlp_6_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_7 (MLP)               (None, 64, 32)       2144        skip_cross_attn_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_7 (Add)          (None, 64, 32)       0           cross_mlp_7[0][0]                \n",
            "                                                                 skip_cross_attn_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[6][0]            \n",
            "                                                                 skip_cross_mlp_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_7_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_0 (Add)       (None, 64, 32)       0           latent_mlp_7_0[0][0]             \n",
            "                                                                 skip_latent_attn_7_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[6][0]            \n",
            "                                                                 skip_latent_mlp_7_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_7_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_1 (Add)       (None, 64, 32)       0           latent_mlp_7_1[0][0]             \n",
            "                                                                 skip_latent_attn_7_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_2 (Add)      (None, 64, 32)       0           latent_attn_1_2[6][0]            \n",
            "                                                                 skip_latent_mlp_7_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_2 (MLP)            (None, 64, 32)       2144        skip_latent_attn_7_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_2 (Add)       (None, 64, 32)       0           latent_mlp_7_2[0][0]             \n",
            "                                                                 skip_latent_attn_7_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_3 (Add)      (None, 64, 32)       0           latent_attn_1_3[6][0]            \n",
            "                                                                 skip_latent_mlp_7_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_3 (MLP)            (None, 64, 32)       2144        skip_latent_attn_7_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_3 (Add)       (None, 64, 32)       0           latent_mlp_7_3[0][0]             \n",
            "                                                                 skip_latent_attn_7_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_4 (Add)      (None, 64, 32)       0           latent_attn_1_4[6][0]            \n",
            "                                                                 skip_latent_mlp_7_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_4 (MLP)            (None, 64, 32)       2144        skip_latent_attn_7_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_4 (Add)       (None, 64, 32)       0           latent_mlp_7_4[0][0]             \n",
            "                                                                 skip_latent_attn_7_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_5 (Add)      (None, 64, 32)       0           latent_attn_1_5[6][0]            \n",
            "                                                                 skip_latent_mlp_7_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_5 (MLP)            (None, 64, 32)       2144        skip_latent_attn_7_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_5 (Add)       (None, 64, 32)       0           latent_mlp_7_5[0][0]             \n",
            "                                                                 skip_latent_attn_7_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 32)        0           skip_latent_mlp_7_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        330         avgpool[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 211,038\n",
            "Trainable params: 211,038\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "196/196 [==============================] - 205s 713ms/step - loss: 0.8815 - sparse_categorical_accuracy: 0.6881 - val_loss: 1.2744 - val_sparse_categorical_accuracy: 0.5929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_144_layer_call_fn while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_144_layer_call_fn while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/500\n",
            "196/196 [==============================] - 128s 652ms/step - loss: 0.8337 - sparse_categorical_accuracy: 0.7022 - val_loss: 1.3152 - val_sparse_categorical_accuracy: 0.5734\n",
            "Epoch 3/500\n",
            "196/196 [==============================] - 128s 654ms/step - loss: 0.8272 - sparse_categorical_accuracy: 0.7013 - val_loss: 1.2530 - val_sparse_categorical_accuracy: 0.5954\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_144_layer_call_fn while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as ln_layer_call_fn, ln_layer_call_and_return_conditional_losses, ln_ctxt_layer_call_fn, ln_ctxt_layer_call_and_return_conditional_losses, dense_144_layer_call_fn while saving (showing 5 of 2040). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/02_Work/05_Experiments/TF-Transformer/Cifar10_PERCEIVER/210327__PERCEIVER_32fq_64fqb_64l_32ldim_1lh_32lhdim_1ch_32chdim_8cd_6ld/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/500\n",
            "196/196 [==============================] - 128s 651ms/step - loss: 0.8142 - sparse_categorical_accuracy: 0.7119 - val_loss: 1.2873 - val_sparse_categorical_accuracy: 0.5902\n",
            "Epoch 5/500\n",
            "196/196 [==============================] - 128s 651ms/step - loss: 0.7985 - sparse_categorical_accuracy: 0.7137 - val_loss: 1.2831 - val_sparse_categorical_accuracy: 0.5937\n",
            "Epoch 6/500\n",
            "196/196 [==============================] - 128s 651ms/step - loss: 0.7864 - sparse_categorical_accuracy: 0.7177 - val_loss: 1.3260 - val_sparse_categorical_accuracy: 0.5797\n",
            "Epoch 7/500\n",
            "196/196 [==============================] - 128s 651ms/step - loss: 0.7960 - sparse_categorical_accuracy: 0.7134 - val_loss: 1.3132 - val_sparse_categorical_accuracy: 0.5913\n",
            "Epoch 8/500\n",
            "196/196 [==============================] - 128s 653ms/step - loss: 0.7734 - sparse_categorical_accuracy: 0.7229 - val_loss: 1.3238 - val_sparse_categorical_accuracy: 0.5862\n",
            "Epoch 9/500\n",
            "196/196 [==============================] - 128s 653ms/step - loss: 0.7667 - sparse_categorical_accuracy: 0.7248 - val_loss: 1.3412 - val_sparse_categorical_accuracy: 0.5847\n",
            "Epoch 10/500\n",
            "196/196 [==============================] - 128s 654ms/step - loss: 0.7521 - sparse_categorical_accuracy: 0.7304 - val_loss: 1.3608 - val_sparse_categorical_accuracy: 0.5865\n",
            "Epoch 11/500\n",
            "196/196 [==============================] - 128s 654ms/step - loss: 0.7363 - sparse_categorical_accuracy: 0.7351 - val_loss: 1.3698 - val_sparse_categorical_accuracy: 0.5935\n",
            "Epoch 12/500\n",
            "196/196 [==============================] - 128s 653ms/step - loss: 0.7415 - sparse_categorical_accuracy: 0.7320 - val_loss: 1.3750 - val_sparse_categorical_accuracy: 0.5838\n",
            "Epoch 13/500\n",
            "196/196 [==============================] - 128s 654ms/step - loss: 0.7126 - sparse_categorical_accuracy: 0.7442 - val_loss: 1.4011 - val_sparse_categorical_accuracy: 0.5864\n",
            "Epoch 14/500\n",
            "196/196 [==============================] - 128s 653ms/step - loss: 0.7050 - sparse_categorical_accuracy: 0.7475 - val_loss: 1.3875 - val_sparse_categorical_accuracy: 0.5869\n",
            "Epoch 15/500\n",
            "196/196 [==============================] - 128s 654ms/step - loss: 0.6974 - sparse_categorical_accuracy: 0.7467 - val_loss: 1.4100 - val_sparse_categorical_accuracy: 0.5935\n",
            "Epoch 16/500\n",
            "163/196 [=======================>......] - ETA: 19s - loss: 0.7066 - sparse_categorical_accuracy: 0.7429"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c8609bf24e89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m                                            \u001b[0;34mf'_{cross_depth}cd'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                            \u001b[0;34mf'_{latent_depth}ld'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                verbose=1)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-6e3649293d1b>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(_model, _checkpoint_filepath, verbose)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   )\n\u001b[1;32m     43\u001b[0m   \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IaUNNStN7A_0",
        "outputId": "9006dac9-1c9a-46ea-86a2-6ec1eabebe9d"
      },
      "source": [
        "epochs = 500\n",
        "for max_freq in [image_height]:\n",
        "  for num_freq_bands in [64]:\n",
        "    for num_latents in [64]:\n",
        "      for latent_dim in [32]:\n",
        "        for latent_head_dim in [32]:\n",
        "          for cross_head_dim in [32]:\n",
        "            for cross_depth in [4]:\n",
        "              for latent_depth in [2]:\n",
        "                model = Perceiver(num_classes=num_classes, \n",
        "                                  image_height=image_height, \n",
        "                                  image_width=image_width,\n",
        "                                  max_freq=max_freq,\n",
        "                                  num_freq_bands=num_freq_bands,\n",
        "                                  freq_base=2.0,\n",
        "                                  num_latents=num_latents,\n",
        "                                  latent_dim=latent_dim,\n",
        "                                  latent_heads=1,\n",
        "                                  latent_head_dim=latent_head_dim,\n",
        "                                  cross_heads=1,\n",
        "                                  cross_head_dim=cross_head_dim,\n",
        "                                  attn_dropout=0.5,\n",
        "                                  mlp_dropout=0.5,\n",
        "                                  cross_depth=cross_depth,\n",
        "                                  latent_depth=latent_depth,             \n",
        "                                  weight_tie_layers=True,\n",
        "                                  preprocess=False).model()\n",
        "                model.summary()\n",
        "                train_and_eval(\n",
        "                  model, \n",
        "                  history_dir=SAVE_PATH + f'_PERCEIVER' \\\n",
        "                                          f'_{max_freq}fq' \\\n",
        "                                          f'_{num_freq_bands}fqb' \\\n",
        "                                          f'_{num_latents}l' \\\n",
        "                                          f'_{latent_dim}ldim' \\\n",
        "                                          f'_1lh' \\\n",
        "                                          f'_{latent_head_dim}lhdim' \\\n",
        "                                          f'_1ch' \\\n",
        "                                          f'_{cross_head_dim}chdim' \\\n",
        "                                          f'_{cross_depth}cd' \\\n",
        "                                          f'_{latent_depth}ld', \n",
        "                  verbose=1\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 64, 32)       2048        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 261)    0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 64, 32)       19370       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 64, 32)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 64, 32)       2144        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 64, 32)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 64, 32)       4192        skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 64, 32)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 64, 32)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 64, 32)       4192        skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 64, 32)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 64, 32)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 64, 32)       19370       skip_latent_mlp_0_1[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 64, 32)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 64, 32)       2144        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 64, 32)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 64, 32)       4192        skip_cross_mlp_1[0][0]           \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 64, 32)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 64, 32)       4192        skip_latent_mlp_1_0[0][0]        \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 64, 32)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_2 (Add)         (None, 64, 32)       0           cross_attn_1[1][0]               \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_2 (MLP)               (None, 64, 32)       2144        skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_2 (Add)          (None, 64, 32)       0           cross_mlp_2[0][0]                \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[1][0]            \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_0 (Add)       (None, 64, 32)       0           latent_mlp_2_0[0][0]             \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[1][0]            \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_1 (Add)       (None, 64, 32)       0           latent_mlp_2_1[0][0]             \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_3 (Add)         (None, 64, 32)       0           cross_attn_1[2][0]               \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_3 (MLP)               (None, 64, 32)       2144        skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_3 (Add)          (None, 64, 32)       0           cross_mlp_3[0][0]                \n",
            "                                                                 skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_0 (Add)      (None, 64, 32)       0           latent_attn_1_0[2][0]            \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_0 (MLP)            (None, 64, 32)       2144        skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_0 (Add)       (None, 64, 32)       0           latent_mlp_3_0[0][0]             \n",
            "                                                                 skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_1 (Add)      (None, 64, 32)       0           latent_attn_1_1[2][0]            \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_1 (MLP)            (None, 64, 32)       2144        skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_1 (Add)       (None, 64, 32)       0           latent_mlp_3_1[0][0]             \n",
            "                                                                 skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 32)        0           skip_latent_mlp_3_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        330         avgpool[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 83,614\n",
            "Trainable params: 83,614\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "196/196 [==============================] - 56s 220ms/step - loss: 2.6562 - sparse_categorical_accuracy: 0.1172 - val_loss: 2.2092 - val_sparse_categorical_accuracy: 0.2251\n",
            "Epoch 2/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 2.0973 - sparse_categorical_accuracy: 0.2096 - val_loss: 2.0662 - val_sparse_categorical_accuracy: 0.3041\n",
            "Epoch 3/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.9462 - sparse_categorical_accuracy: 0.2876 - val_loss: 1.9985 - val_sparse_categorical_accuracy: 0.3300\n",
            "Epoch 4/500\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.8405 - sparse_categorical_accuracy: 0.3323 - val_loss: 1.8964 - val_sparse_categorical_accuracy: 0.3510\n",
            "Epoch 5/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.7677 - sparse_categorical_accuracy: 0.3604 - val_loss: 1.8872 - val_sparse_categorical_accuracy: 0.3662\n",
            "Epoch 6/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.7227 - sparse_categorical_accuracy: 0.3763 - val_loss: 1.8725 - val_sparse_categorical_accuracy: 0.3811\n",
            "Epoch 7/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.6774 - sparse_categorical_accuracy: 0.3960 - val_loss: 1.8829 - val_sparse_categorical_accuracy: 0.3810\n",
            "Epoch 8/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.6577 - sparse_categorical_accuracy: 0.3986 - val_loss: 1.8011 - val_sparse_categorical_accuracy: 0.3981\n",
            "Epoch 9/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.6335 - sparse_categorical_accuracy: 0.4117 - val_loss: 1.7784 - val_sparse_categorical_accuracy: 0.4053\n",
            "Epoch 10/500\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.6116 - sparse_categorical_accuracy: 0.4196 - val_loss: 1.7262 - val_sparse_categorical_accuracy: 0.4125\n",
            "Epoch 11/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.5968 - sparse_categorical_accuracy: 0.4259 - val_loss: 1.7542 - val_sparse_categorical_accuracy: 0.4159\n",
            "Epoch 12/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.5700 - sparse_categorical_accuracy: 0.4362 - val_loss: 1.8379 - val_sparse_categorical_accuracy: 0.3921\n",
            "Epoch 13/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.5648 - sparse_categorical_accuracy: 0.4363 - val_loss: 1.7086 - val_sparse_categorical_accuracy: 0.4340\n",
            "Epoch 14/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.5263 - sparse_categorical_accuracy: 0.4550 - val_loss: 1.7717 - val_sparse_categorical_accuracy: 0.4213\n",
            "Epoch 15/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.5128 - sparse_categorical_accuracy: 0.4553 - val_loss: 1.7666 - val_sparse_categorical_accuracy: 0.4250\n",
            "Epoch 16/500\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.5063 - sparse_categorical_accuracy: 0.4602 - val_loss: 1.7281 - val_sparse_categorical_accuracy: 0.4384\n",
            "Epoch 17/500\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.4747 - sparse_categorical_accuracy: 0.4735 - val_loss: 1.7407 - val_sparse_categorical_accuracy: 0.4317\n",
            "Epoch 18/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.4569 - sparse_categorical_accuracy: 0.4801 - val_loss: 1.7134 - val_sparse_categorical_accuracy: 0.4456\n",
            "Epoch 19/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.4365 - sparse_categorical_accuracy: 0.4880 - val_loss: 1.6523 - val_sparse_categorical_accuracy: 0.4594\n",
            "Epoch 20/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.4161 - sparse_categorical_accuracy: 0.4918 - val_loss: 1.5918 - val_sparse_categorical_accuracy: 0.4818\n",
            "Epoch 21/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.4064 - sparse_categorical_accuracy: 0.4980 - val_loss: 1.6946 - val_sparse_categorical_accuracy: 0.4570\n",
            "Epoch 22/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.3980 - sparse_categorical_accuracy: 0.5002 - val_loss: 1.5396 - val_sparse_categorical_accuracy: 0.4896\n",
            "Epoch 23/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.3805 - sparse_categorical_accuracy: 0.5062 - val_loss: 1.6267 - val_sparse_categorical_accuracy: 0.4771\n",
            "Epoch 24/500\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.3743 - sparse_categorical_accuracy: 0.5098 - val_loss: 1.4958 - val_sparse_categorical_accuracy: 0.5068\n",
            "Epoch 25/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.3599 - sparse_categorical_accuracy: 0.5128 - val_loss: 1.5496 - val_sparse_categorical_accuracy: 0.4941\n",
            "Epoch 26/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.3377 - sparse_categorical_accuracy: 0.5249 - val_loss: 1.5416 - val_sparse_categorical_accuracy: 0.4979\n",
            "Epoch 27/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.3269 - sparse_categorical_accuracy: 0.5300 - val_loss: 1.6222 - val_sparse_categorical_accuracy: 0.4894\n",
            "Epoch 28/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.3278 - sparse_categorical_accuracy: 0.5300 - val_loss: 1.4626 - val_sparse_categorical_accuracy: 0.5174\n",
            "Epoch 29/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.3057 - sparse_categorical_accuracy: 0.5360 - val_loss: 1.5022 - val_sparse_categorical_accuracy: 0.5093\n",
            "Epoch 30/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.2947 - sparse_categorical_accuracy: 0.5392 - val_loss: 1.5430 - val_sparse_categorical_accuracy: 0.5042\n",
            "Epoch 31/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.2930 - sparse_categorical_accuracy: 0.5396 - val_loss: 1.4703 - val_sparse_categorical_accuracy: 0.5085\n",
            "Epoch 32/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.2880 - sparse_categorical_accuracy: 0.5408 - val_loss: 1.5198 - val_sparse_categorical_accuracy: 0.5142\n",
            "Epoch 33/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.2775 - sparse_categorical_accuracy: 0.5461 - val_loss: 1.4966 - val_sparse_categorical_accuracy: 0.5197\n",
            "Epoch 34/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.2619 - sparse_categorical_accuracy: 0.5477 - val_loss: 1.4930 - val_sparse_categorical_accuracy: 0.5179\n",
            "Epoch 35/500\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.2463 - sparse_categorical_accuracy: 0.5572 - val_loss: 1.4939 - val_sparse_categorical_accuracy: 0.5168\n",
            "Epoch 36/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.2547 - sparse_categorical_accuracy: 0.5541 - val_loss: 1.4512 - val_sparse_categorical_accuracy: 0.5238\n",
            "Epoch 37/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.2372 - sparse_categorical_accuracy: 0.5592 - val_loss: 1.5310 - val_sparse_categorical_accuracy: 0.5075\n",
            "Epoch 38/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.2389 - sparse_categorical_accuracy: 0.5597 - val_loss: 1.4221 - val_sparse_categorical_accuracy: 0.5336\n",
            "Epoch 39/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.2381 - sparse_categorical_accuracy: 0.5559 - val_loss: 1.4179 - val_sparse_categorical_accuracy: 0.5404\n",
            "Epoch 40/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.2261 - sparse_categorical_accuracy: 0.5656 - val_loss: 1.4395 - val_sparse_categorical_accuracy: 0.5341\n",
            "Epoch 41/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.2178 - sparse_categorical_accuracy: 0.5678 - val_loss: 1.4306 - val_sparse_categorical_accuracy: 0.5386\n",
            "Epoch 42/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1977 - sparse_categorical_accuracy: 0.5755 - val_loss: 1.4853 - val_sparse_categorical_accuracy: 0.5283\n",
            "Epoch 43/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.2081 - sparse_categorical_accuracy: 0.5707 - val_loss: 1.4552 - val_sparse_categorical_accuracy: 0.5313\n",
            "Epoch 44/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.2009 - sparse_categorical_accuracy: 0.5728 - val_loss: 1.4381 - val_sparse_categorical_accuracy: 0.5342\n",
            "Epoch 45/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1903 - sparse_categorical_accuracy: 0.5756 - val_loss: 1.4243 - val_sparse_categorical_accuracy: 0.5378\n",
            "Epoch 46/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.2020 - sparse_categorical_accuracy: 0.5715 - val_loss: 1.4147 - val_sparse_categorical_accuracy: 0.5429\n",
            "Epoch 47/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1828 - sparse_categorical_accuracy: 0.5809 - val_loss: 1.3999 - val_sparse_categorical_accuracy: 0.5422\n",
            "Epoch 48/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1857 - sparse_categorical_accuracy: 0.5783 - val_loss: 1.4001 - val_sparse_categorical_accuracy: 0.5497\n",
            "Epoch 49/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1758 - sparse_categorical_accuracy: 0.5825 - val_loss: 1.3905 - val_sparse_categorical_accuracy: 0.5544\n",
            "Epoch 50/500\n",
            "196/196 [==============================] - 40s 204ms/step - loss: 1.1705 - sparse_categorical_accuracy: 0.5819 - val_loss: 1.3922 - val_sparse_categorical_accuracy: 0.5518\n",
            "Epoch 51/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1797 - sparse_categorical_accuracy: 0.5781 - val_loss: 1.4144 - val_sparse_categorical_accuracy: 0.5409\n",
            "Epoch 52/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1684 - sparse_categorical_accuracy: 0.5861 - val_loss: 1.4146 - val_sparse_categorical_accuracy: 0.5442\n",
            "Epoch 53/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1636 - sparse_categorical_accuracy: 0.5851 - val_loss: 1.4694 - val_sparse_categorical_accuracy: 0.5321\n",
            "Epoch 54/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1576 - sparse_categorical_accuracy: 0.5862 - val_loss: 1.4269 - val_sparse_categorical_accuracy: 0.5460\n",
            "Epoch 55/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1508 - sparse_categorical_accuracy: 0.5916 - val_loss: 1.3986 - val_sparse_categorical_accuracy: 0.5564\n",
            "Epoch 56/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1580 - sparse_categorical_accuracy: 0.5851 - val_loss: 1.4102 - val_sparse_categorical_accuracy: 0.5564\n",
            "Epoch 57/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1427 - sparse_categorical_accuracy: 0.5961 - val_loss: 1.3998 - val_sparse_categorical_accuracy: 0.5532\n",
            "Epoch 58/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1435 - sparse_categorical_accuracy: 0.5919 - val_loss: 1.4116 - val_sparse_categorical_accuracy: 0.5463\n",
            "Epoch 59/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1394 - sparse_categorical_accuracy: 0.5938 - val_loss: 1.3699 - val_sparse_categorical_accuracy: 0.5588\n",
            "Epoch 60/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1340 - sparse_categorical_accuracy: 0.5949 - val_loss: 1.4184 - val_sparse_categorical_accuracy: 0.5597\n",
            "Epoch 61/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1406 - sparse_categorical_accuracy: 0.5944 - val_loss: 1.4010 - val_sparse_categorical_accuracy: 0.5499\n",
            "Epoch 62/500\n",
            "196/196 [==============================] - 40s 205ms/step - loss: 1.1310 - sparse_categorical_accuracy: 0.6034 - val_loss: 1.5019 - val_sparse_categorical_accuracy: 0.5353\n",
            "Epoch 63/500\n",
            " 58/196 [=======>......................] - ETA: 26s - loss: 1.1296 - sparse_categorical_accuracy: 0.5976"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-2d8bcbfe3686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m                                           \u001b[0;34mf'_{cross_depth}cd'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                           \u001b[0;34mf'_{latent_depth}ld'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 )\n",
            "\u001b[0;32m<ipython-input-16-aa3cbc078dcb>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(_model, checkpoint_dir, history_dir, verbose)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m   )\n\u001b[1;32m     47\u001b[0m   \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9Z6S3-eTZf7x",
        "outputId": "1f9cbc34-25a6-4898-f713-100c99c45b1e"
      },
      "source": [
        "epochs = 500\n",
        "for max_freq in [image_height]:\n",
        "  for num_freq_bands in [64]:\n",
        "    for num_latents in [64]:\n",
        "      for latent_dim in [64]:\n",
        "        for latent_head_dim in [8]:\n",
        "          for cross_head_dim in [64]:\n",
        "            for cross_depth in [8]:\n",
        "              for latent_depth in [6]:\n",
        "                model = Perceiver(num_classes=num_classes, \n",
        "                                  image_height=image_height, \n",
        "                                  image_width=image_width,\n",
        "                                  max_freq=max_freq,\n",
        "                                  num_freq_bands=num_freq_bands,\n",
        "                                  freq_base=2.0,\n",
        "                                  num_latents=num_latents,\n",
        "                                  latent_dim=latent_dim,\n",
        "                                  latent_heads=8,\n",
        "                                  latent_head_dim=latent_head_dim,\n",
        "                                  cross_heads=1,\n",
        "                                  cross_head_dim=cross_head_dim,\n",
        "                                  attn_dropout=0.5,\n",
        "                                  mlp_dropout=0.5,\n",
        "                                  cross_depth=cross_depth,\n",
        "                                  latent_depth=latent_depth,             \n",
        "                                  weight_tie_layers=True,\n",
        "                                  preprocess=False).model()\n",
        "                model.summary()\n",
        "                train_and_eval(\n",
        "                  model, \n",
        "                  history_dir=SAVE_PATH + f'_PERCEIVER' \\\n",
        "                                          f'_{max_freq}fq' \\\n",
        "                                          f'_{num_freq_bands}fqb' \\\n",
        "                                          f'_{num_latents}l' \\\n",
        "                                          f'_{latent_dim}ldim' \\\n",
        "                                          f'_1lh' \\\n",
        "                                          f'_{latent_head_dim}lhdim' \\\n",
        "                                          f'_1ch' \\\n",
        "                                          f'_{cross_head_dim}chdim' \\\n",
        "                                          f'_{cross_depth}cd' \\\n",
        "                                          f'_{latent_depth}ld', \n",
        "                  verbose=1\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 64, 64)       4096        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 261)    0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 64, 64)       42314       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 64, 64)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 64, 64)       8384        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 64, 64)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 64, 64)       16576       skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 64, 64)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 64, 64)       8384        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 64, 64)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 64, 64)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 64, 64)       8384        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 64, 64)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 64, 64)       0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 64, 64)       8384        skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 64, 64)       0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 64, 64)       0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 64, 64)       8384        skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 64, 64)       0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_4 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_4 (Add)      (None, 64, 64)       0           latent_attn_0_4[0][0]            \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_4 (MLP)            (None, 64, 64)       8384        skip_latent_attn_0_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_4 (Add)       (None, 64, 64)       0           latent_mlp_0_4[0][0]             \n",
            "                                                                 skip_latent_attn_0_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_5 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_0_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_5 (Add)      (None, 64, 64)       0           latent_attn_0_5[0][0]            \n",
            "                                                                 skip_latent_mlp_0_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_5 (MLP)            (None, 64, 64)       8384        skip_latent_attn_0_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_5 (Add)       (None, 64, 64)       0           latent_mlp_0_5[0][0]             \n",
            "                                                                 skip_latent_attn_0_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 64, 64)       42314       skip_latent_mlp_0_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_1_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_2_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_3_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_4_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_5_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_6_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 64, 64)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 64, 64)       8384        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 64, 64)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 64, 64)       16576       skip_cross_mlp_1[0][0]           \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "                                                                 skip_cross_mlp_4[0][0]           \n",
            "                                                                 skip_cross_mlp_5[0][0]           \n",
            "                                                                 skip_cross_mlp_6[0][0]           \n",
            "                                                                 skip_cross_mlp_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 64, 64)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 64, 64)       8384        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 64, 64)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_1_0[0][0]        \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "                                                                 skip_latent_mlp_4_0[0][0]        \n",
            "                                                                 skip_latent_mlp_5_0[0][0]        \n",
            "                                                                 skip_latent_mlp_6_0[0][0]        \n",
            "                                                                 skip_latent_mlp_7_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 64, 64)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 64, 64)       8384        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 64, 64)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_1_1[0][0]        \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "                                                                 skip_latent_mlp_4_1[0][0]        \n",
            "                                                                 skip_latent_mlp_5_1[0][0]        \n",
            "                                                                 skip_latent_mlp_6_1[0][0]        \n",
            "                                                                 skip_latent_mlp_7_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 64, 64)       0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 64, 64)       8384        skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 64, 64)       0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_1_2[0][0]        \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "                                                                 skip_latent_mlp_3_2[0][0]        \n",
            "                                                                 skip_latent_mlp_4_2[0][0]        \n",
            "                                                                 skip_latent_mlp_5_2[0][0]        \n",
            "                                                                 skip_latent_mlp_6_2[0][0]        \n",
            "                                                                 skip_latent_mlp_7_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 64, 64)       0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 64, 64)       8384        skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 64, 64)       0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_4 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_1_3[0][0]        \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "                                                                 skip_latent_mlp_3_3[0][0]        \n",
            "                                                                 skip_latent_mlp_4_3[0][0]        \n",
            "                                                                 skip_latent_mlp_5_3[0][0]        \n",
            "                                                                 skip_latent_mlp_6_3[0][0]        \n",
            "                                                                 skip_latent_mlp_7_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_4 (Add)      (None, 64, 64)       0           latent_attn_1_4[0][0]            \n",
            "                                                                 skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_4 (MLP)            (None, 64, 64)       8384        skip_latent_attn_1_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_4 (Add)       (None, 64, 64)       0           latent_mlp_1_4[0][0]             \n",
            "                                                                 skip_latent_attn_1_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_5 (Transformer)   (None, 64, 64)       16576       skip_latent_mlp_1_4[0][0]        \n",
            "                                                                 skip_latent_mlp_2_4[0][0]        \n",
            "                                                                 skip_latent_mlp_3_4[0][0]        \n",
            "                                                                 skip_latent_mlp_4_4[0][0]        \n",
            "                                                                 skip_latent_mlp_5_4[0][0]        \n",
            "                                                                 skip_latent_mlp_6_4[0][0]        \n",
            "                                                                 skip_latent_mlp_7_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_5 (Add)      (None, 64, 64)       0           latent_attn_1_5[0][0]            \n",
            "                                                                 skip_latent_mlp_1_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_5 (MLP)            (None, 64, 64)       8384        skip_latent_attn_1_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_5 (Add)       (None, 64, 64)       0           latent_mlp_1_5[0][0]             \n",
            "                                                                 skip_latent_attn_1_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_2 (Add)         (None, 64, 64)       0           cross_attn_1[1][0]               \n",
            "                                                                 skip_latent_mlp_1_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_2 (MLP)               (None, 64, 64)       8384        skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_2 (Add)          (None, 64, 64)       0           cross_mlp_2[0][0]                \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_0 (Add)      (None, 64, 64)       0           latent_attn_1_0[1][0]            \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_0 (MLP)            (None, 64, 64)       8384        skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_0 (Add)       (None, 64, 64)       0           latent_mlp_2_0[0][0]             \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_1 (Add)      (None, 64, 64)       0           latent_attn_1_1[1][0]            \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_1 (MLP)            (None, 64, 64)       8384        skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_1 (Add)       (None, 64, 64)       0           latent_mlp_2_1[0][0]             \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_2 (Add)      (None, 64, 64)       0           latent_attn_1_2[1][0]            \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_2 (MLP)            (None, 64, 64)       8384        skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_2 (Add)       (None, 64, 64)       0           latent_mlp_2_2[0][0]             \n",
            "                                                                 skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_3 (Add)      (None, 64, 64)       0           latent_attn_1_3[1][0]            \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_3 (MLP)            (None, 64, 64)       8384        skip_latent_attn_2_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_3 (Add)       (None, 64, 64)       0           latent_mlp_2_3[0][0]             \n",
            "                                                                 skip_latent_attn_2_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_4 (Add)      (None, 64, 64)       0           latent_attn_1_4[1][0]            \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_4 (MLP)            (None, 64, 64)       8384        skip_latent_attn_2_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_4 (Add)       (None, 64, 64)       0           latent_mlp_2_4[0][0]             \n",
            "                                                                 skip_latent_attn_2_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_5 (Add)      (None, 64, 64)       0           latent_attn_1_5[1][0]            \n",
            "                                                                 skip_latent_mlp_2_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_5 (MLP)            (None, 64, 64)       8384        skip_latent_attn_2_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_5 (Add)       (None, 64, 64)       0           latent_mlp_2_5[0][0]             \n",
            "                                                                 skip_latent_attn_2_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_3 (Add)         (None, 64, 64)       0           cross_attn_1[2][0]               \n",
            "                                                                 skip_latent_mlp_2_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_3 (MLP)               (None, 64, 64)       8384        skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_3 (Add)          (None, 64, 64)       0           cross_mlp_3[0][0]                \n",
            "                                                                 skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_0 (Add)      (None, 64, 64)       0           latent_attn_1_0[2][0]            \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_0 (MLP)            (None, 64, 64)       8384        skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_0 (Add)       (None, 64, 64)       0           latent_mlp_3_0[0][0]             \n",
            "                                                                 skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_1 (Add)      (None, 64, 64)       0           latent_attn_1_1[2][0]            \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_1 (MLP)            (None, 64, 64)       8384        skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_1 (Add)       (None, 64, 64)       0           latent_mlp_3_1[0][0]             \n",
            "                                                                 skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_2 (Add)      (None, 64, 64)       0           latent_attn_1_2[2][0]            \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_2 (MLP)            (None, 64, 64)       8384        skip_latent_attn_3_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_2 (Add)       (None, 64, 64)       0           latent_mlp_3_2[0][0]             \n",
            "                                                                 skip_latent_attn_3_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_3 (Add)      (None, 64, 64)       0           latent_attn_1_3[2][0]            \n",
            "                                                                 skip_latent_mlp_3_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_3 (MLP)            (None, 64, 64)       8384        skip_latent_attn_3_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_3 (Add)       (None, 64, 64)       0           latent_mlp_3_3[0][0]             \n",
            "                                                                 skip_latent_attn_3_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_4 (Add)      (None, 64, 64)       0           latent_attn_1_4[2][0]            \n",
            "                                                                 skip_latent_mlp_3_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_4 (MLP)            (None, 64, 64)       8384        skip_latent_attn_3_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_4 (Add)       (None, 64, 64)       0           latent_mlp_3_4[0][0]             \n",
            "                                                                 skip_latent_attn_3_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_5 (Add)      (None, 64, 64)       0           latent_attn_1_5[2][0]            \n",
            "                                                                 skip_latent_mlp_3_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_5 (MLP)            (None, 64, 64)       8384        skip_latent_attn_3_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_5 (Add)       (None, 64, 64)       0           latent_mlp_3_5[0][0]             \n",
            "                                                                 skip_latent_attn_3_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_4 (Add)         (None, 64, 64)       0           cross_attn_1[3][0]               \n",
            "                                                                 skip_latent_mlp_3_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_4 (MLP)               (None, 64, 64)       8384        skip_cross_attn_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_4 (Add)          (None, 64, 64)       0           cross_mlp_4[0][0]                \n",
            "                                                                 skip_cross_attn_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_0 (Add)      (None, 64, 64)       0           latent_attn_1_0[3][0]            \n",
            "                                                                 skip_cross_mlp_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_0 (MLP)            (None, 64, 64)       8384        skip_latent_attn_4_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_0 (Add)       (None, 64, 64)       0           latent_mlp_4_0[0][0]             \n",
            "                                                                 skip_latent_attn_4_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_1 (Add)      (None, 64, 64)       0           latent_attn_1_1[3][0]            \n",
            "                                                                 skip_latent_mlp_4_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_1 (MLP)            (None, 64, 64)       8384        skip_latent_attn_4_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_1 (Add)       (None, 64, 64)       0           latent_mlp_4_1[0][0]             \n",
            "                                                                 skip_latent_attn_4_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_2 (Add)      (None, 64, 64)       0           latent_attn_1_2[3][0]            \n",
            "                                                                 skip_latent_mlp_4_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_2 (MLP)            (None, 64, 64)       8384        skip_latent_attn_4_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_2 (Add)       (None, 64, 64)       0           latent_mlp_4_2[0][0]             \n",
            "                                                                 skip_latent_attn_4_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_3 (Add)      (None, 64, 64)       0           latent_attn_1_3[3][0]            \n",
            "                                                                 skip_latent_mlp_4_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_3 (MLP)            (None, 64, 64)       8384        skip_latent_attn_4_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_3 (Add)       (None, 64, 64)       0           latent_mlp_4_3[0][0]             \n",
            "                                                                 skip_latent_attn_4_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_4 (Add)      (None, 64, 64)       0           latent_attn_1_4[3][0]            \n",
            "                                                                 skip_latent_mlp_4_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_4 (MLP)            (None, 64, 64)       8384        skip_latent_attn_4_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_4 (Add)       (None, 64, 64)       0           latent_mlp_4_4[0][0]             \n",
            "                                                                 skip_latent_attn_4_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_5 (Add)      (None, 64, 64)       0           latent_attn_1_5[3][0]            \n",
            "                                                                 skip_latent_mlp_4_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_5 (MLP)            (None, 64, 64)       8384        skip_latent_attn_4_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_5 (Add)       (None, 64, 64)       0           latent_mlp_4_5[0][0]             \n",
            "                                                                 skip_latent_attn_4_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_5 (Add)         (None, 64, 64)       0           cross_attn_1[4][0]               \n",
            "                                                                 skip_latent_mlp_4_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_5 (MLP)               (None, 64, 64)       8384        skip_cross_attn_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_5 (Add)          (None, 64, 64)       0           cross_mlp_5[0][0]                \n",
            "                                                                 skip_cross_attn_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_0 (Add)      (None, 64, 64)       0           latent_attn_1_0[4][0]            \n",
            "                                                                 skip_cross_mlp_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_0 (MLP)            (None, 64, 64)       8384        skip_latent_attn_5_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_0 (Add)       (None, 64, 64)       0           latent_mlp_5_0[0][0]             \n",
            "                                                                 skip_latent_attn_5_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_1 (Add)      (None, 64, 64)       0           latent_attn_1_1[4][0]            \n",
            "                                                                 skip_latent_mlp_5_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_1 (MLP)            (None, 64, 64)       8384        skip_latent_attn_5_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_1 (Add)       (None, 64, 64)       0           latent_mlp_5_1[0][0]             \n",
            "                                                                 skip_latent_attn_5_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_2 (Add)      (None, 64, 64)       0           latent_attn_1_2[4][0]            \n",
            "                                                                 skip_latent_mlp_5_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_2 (MLP)            (None, 64, 64)       8384        skip_latent_attn_5_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_2 (Add)       (None, 64, 64)       0           latent_mlp_5_2[0][0]             \n",
            "                                                                 skip_latent_attn_5_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_3 (Add)      (None, 64, 64)       0           latent_attn_1_3[4][0]            \n",
            "                                                                 skip_latent_mlp_5_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_3 (MLP)            (None, 64, 64)       8384        skip_latent_attn_5_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_3 (Add)       (None, 64, 64)       0           latent_mlp_5_3[0][0]             \n",
            "                                                                 skip_latent_attn_5_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_4 (Add)      (None, 64, 64)       0           latent_attn_1_4[4][0]            \n",
            "                                                                 skip_latent_mlp_5_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_4 (MLP)            (None, 64, 64)       8384        skip_latent_attn_5_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_4 (Add)       (None, 64, 64)       0           latent_mlp_5_4[0][0]             \n",
            "                                                                 skip_latent_attn_5_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_5 (Add)      (None, 64, 64)       0           latent_attn_1_5[4][0]            \n",
            "                                                                 skip_latent_mlp_5_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_5 (MLP)            (None, 64, 64)       8384        skip_latent_attn_5_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_5 (Add)       (None, 64, 64)       0           latent_mlp_5_5[0][0]             \n",
            "                                                                 skip_latent_attn_5_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_6 (Add)         (None, 64, 64)       0           cross_attn_1[5][0]               \n",
            "                                                                 skip_latent_mlp_5_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_6 (MLP)               (None, 64, 64)       8384        skip_cross_attn_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_6 (Add)          (None, 64, 64)       0           cross_mlp_6[0][0]                \n",
            "                                                                 skip_cross_attn_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_0 (Add)      (None, 64, 64)       0           latent_attn_1_0[5][0]            \n",
            "                                                                 skip_cross_mlp_6[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_0 (MLP)            (None, 64, 64)       8384        skip_latent_attn_6_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_0 (Add)       (None, 64, 64)       0           latent_mlp_6_0[0][0]             \n",
            "                                                                 skip_latent_attn_6_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_1 (Add)      (None, 64, 64)       0           latent_attn_1_1[5][0]            \n",
            "                                                                 skip_latent_mlp_6_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_1 (MLP)            (None, 64, 64)       8384        skip_latent_attn_6_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_1 (Add)       (None, 64, 64)       0           latent_mlp_6_1[0][0]             \n",
            "                                                                 skip_latent_attn_6_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_2 (Add)      (None, 64, 64)       0           latent_attn_1_2[5][0]            \n",
            "                                                                 skip_latent_mlp_6_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_2 (MLP)            (None, 64, 64)       8384        skip_latent_attn_6_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_2 (Add)       (None, 64, 64)       0           latent_mlp_6_2[0][0]             \n",
            "                                                                 skip_latent_attn_6_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_3 (Add)      (None, 64, 64)       0           latent_attn_1_3[5][0]            \n",
            "                                                                 skip_latent_mlp_6_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_3 (MLP)            (None, 64, 64)       8384        skip_latent_attn_6_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_3 (Add)       (None, 64, 64)       0           latent_mlp_6_3[0][0]             \n",
            "                                                                 skip_latent_attn_6_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_4 (Add)      (None, 64, 64)       0           latent_attn_1_4[5][0]            \n",
            "                                                                 skip_latent_mlp_6_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_4 (MLP)            (None, 64, 64)       8384        skip_latent_attn_6_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_4 (Add)       (None, 64, 64)       0           latent_mlp_6_4[0][0]             \n",
            "                                                                 skip_latent_attn_6_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_5 (Add)      (None, 64, 64)       0           latent_attn_1_5[5][0]            \n",
            "                                                                 skip_latent_mlp_6_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_5 (MLP)            (None, 64, 64)       8384        skip_latent_attn_6_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_5 (Add)       (None, 64, 64)       0           latent_mlp_6_5[0][0]             \n",
            "                                                                 skip_latent_attn_6_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_7 (Add)         (None, 64, 64)       0           cross_attn_1[6][0]               \n",
            "                                                                 skip_latent_mlp_6_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_7 (MLP)               (None, 64, 64)       8384        skip_cross_attn_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_7 (Add)          (None, 64, 64)       0           cross_mlp_7[0][0]                \n",
            "                                                                 skip_cross_attn_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_0 (Add)      (None, 64, 64)       0           latent_attn_1_0[6][0]            \n",
            "                                                                 skip_cross_mlp_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_0 (MLP)            (None, 64, 64)       8384        skip_latent_attn_7_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_0 (Add)       (None, 64, 64)       0           latent_mlp_7_0[0][0]             \n",
            "                                                                 skip_latent_attn_7_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_1 (Add)      (None, 64, 64)       0           latent_attn_1_1[6][0]            \n",
            "                                                                 skip_latent_mlp_7_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_1 (MLP)            (None, 64, 64)       8384        skip_latent_attn_7_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_1 (Add)       (None, 64, 64)       0           latent_mlp_7_1[0][0]             \n",
            "                                                                 skip_latent_attn_7_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_2 (Add)      (None, 64, 64)       0           latent_attn_1_2[6][0]            \n",
            "                                                                 skip_latent_mlp_7_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_2 (MLP)            (None, 64, 64)       8384        skip_latent_attn_7_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_2 (Add)       (None, 64, 64)       0           latent_mlp_7_2[0][0]             \n",
            "                                                                 skip_latent_attn_7_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_3 (Add)      (None, 64, 64)       0           latent_attn_1_3[6][0]            \n",
            "                                                                 skip_latent_mlp_7_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_3 (MLP)            (None, 64, 64)       8384        skip_latent_attn_7_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_3 (Add)       (None, 64, 64)       0           latent_mlp_7_3[0][0]             \n",
            "                                                                 skip_latent_attn_7_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_4 (Add)      (None, 64, 64)       0           latent_attn_1_4[6][0]            \n",
            "                                                                 skip_latent_mlp_7_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_4 (MLP)            (None, 64, 64)       8384        skip_latent_attn_7_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_4 (Add)       (None, 64, 64)       0           latent_mlp_7_4[0][0]             \n",
            "                                                                 skip_latent_attn_7_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_5 (Add)      (None, 64, 64)       0           latent_attn_1_5[6][0]            \n",
            "                                                                 skip_latent_mlp_7_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_5 (MLP)            (None, 64, 64)       8384        skip_latent_attn_7_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_5 (Add)       (None, 64, 64)       0           latent_mlp_7_5[0][0]             \n",
            "                                                                 skip_latent_attn_7_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 64)        0           skip_latent_mlp_7_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        650         avgpool[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 757,790\n",
            "Trainable params: 757,790\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "196/196 [==============================] - 223s 844ms/step - loss: 9.4700 - sparse_categorical_accuracy: 0.1038 - val_loss: 3.0615 - val_sparse_categorical_accuracy: 0.1191\n",
            "Epoch 2/500\n",
            "196/196 [==============================] - 151s 773ms/step - loss: 2.5757 - sparse_categorical_accuracy: 0.1489 - val_loss: 3.0307 - val_sparse_categorical_accuracy: 0.2105\n",
            "Epoch 3/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 2.1648 - sparse_categorical_accuracy: 0.2283 - val_loss: 3.1688 - val_sparse_categorical_accuracy: 0.1915\n",
            "Epoch 4/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 2.0511 - sparse_categorical_accuracy: 0.2634 - val_loss: 2.7727 - val_sparse_categorical_accuracy: 0.2489\n",
            "Epoch 5/500\n",
            "196/196 [==============================] - 152s 774ms/step - loss: 1.9621 - sparse_categorical_accuracy: 0.2917 - val_loss: 2.3933 - val_sparse_categorical_accuracy: 0.2833\n",
            "Epoch 6/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 1.8885 - sparse_categorical_accuracy: 0.3104 - val_loss: 2.4110 - val_sparse_categorical_accuracy: 0.2949\n",
            "Epoch 7/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 1.8316 - sparse_categorical_accuracy: 0.3368 - val_loss: 2.3804 - val_sparse_categorical_accuracy: 0.3252\n",
            "Epoch 8/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 1.7853 - sparse_categorical_accuracy: 0.3517 - val_loss: 2.2384 - val_sparse_categorical_accuracy: 0.3151\n",
            "Epoch 9/500\n",
            "196/196 [==============================] - 152s 774ms/step - loss: 1.7556 - sparse_categorical_accuracy: 0.3651 - val_loss: 2.0263 - val_sparse_categorical_accuracy: 0.3545\n",
            "Epoch 10/500\n",
            "196/196 [==============================] - 151s 772ms/step - loss: 1.6882 - sparse_categorical_accuracy: 0.3855 - val_loss: 2.0685 - val_sparse_categorical_accuracy: 0.3427\n",
            "Epoch 11/500\n",
            "196/196 [==============================] - 152s 774ms/step - loss: 1.6690 - sparse_categorical_accuracy: 0.3972 - val_loss: 1.9894 - val_sparse_categorical_accuracy: 0.3540\n",
            "Epoch 12/500\n",
            "196/196 [==============================] - 151s 773ms/step - loss: 1.6139 - sparse_categorical_accuracy: 0.4172 - val_loss: 1.9141 - val_sparse_categorical_accuracy: 0.3935\n",
            "Epoch 13/500\n",
            "196/196 [==============================] - 151s 773ms/step - loss: 1.5995 - sparse_categorical_accuracy: 0.4249 - val_loss: 1.8412 - val_sparse_categorical_accuracy: 0.3934\n",
            "Epoch 14/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 1.5624 - sparse_categorical_accuracy: 0.4407 - val_loss: 1.8776 - val_sparse_categorical_accuracy: 0.3952\n",
            "Epoch 15/500\n",
            "196/196 [==============================] - 151s 773ms/step - loss: 1.5317 - sparse_categorical_accuracy: 0.4479 - val_loss: 1.7984 - val_sparse_categorical_accuracy: 0.4091\n",
            "Epoch 16/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 1.4750 - sparse_categorical_accuracy: 0.4704 - val_loss: 1.6473 - val_sparse_categorical_accuracy: 0.4629\n",
            "Epoch 17/500\n",
            "196/196 [==============================] - 151s 772ms/step - loss: 1.4349 - sparse_categorical_accuracy: 0.4841 - val_loss: 1.7218 - val_sparse_categorical_accuracy: 0.4519\n",
            "Epoch 18/500\n",
            "196/196 [==============================] - 151s 772ms/step - loss: 1.4107 - sparse_categorical_accuracy: 0.4929 - val_loss: 1.7851 - val_sparse_categorical_accuracy: 0.4520\n",
            "Epoch 19/500\n",
            "196/196 [==============================] - 151s 773ms/step - loss: 1.3647 - sparse_categorical_accuracy: 0.5154 - val_loss: 1.7536 - val_sparse_categorical_accuracy: 0.4573\n",
            "Epoch 20/500\n",
            "196/196 [==============================] - 151s 773ms/step - loss: 1.3290 - sparse_categorical_accuracy: 0.5229 - val_loss: 1.6144 - val_sparse_categorical_accuracy: 0.4912\n",
            "Epoch 21/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 1.3002 - sparse_categorical_accuracy: 0.5326 - val_loss: 1.6147 - val_sparse_categorical_accuracy: 0.4893\n",
            "Epoch 22/500\n",
            "196/196 [==============================] - 151s 773ms/step - loss: 1.2989 - sparse_categorical_accuracy: 0.5368 - val_loss: 1.5536 - val_sparse_categorical_accuracy: 0.5017\n",
            "Epoch 23/500\n",
            "196/196 [==============================] - 151s 773ms/step - loss: 1.2796 - sparse_categorical_accuracy: 0.5435 - val_loss: 1.5371 - val_sparse_categorical_accuracy: 0.5167\n",
            "Epoch 24/500\n",
            "196/196 [==============================] - 151s 772ms/step - loss: 1.2468 - sparse_categorical_accuracy: 0.5569 - val_loss: 1.6514 - val_sparse_categorical_accuracy: 0.4884\n",
            "Epoch 25/500\n",
            "196/196 [==============================] - 151s 772ms/step - loss: 1.2380 - sparse_categorical_accuracy: 0.5561 - val_loss: 1.6955 - val_sparse_categorical_accuracy: 0.4936\n",
            "Epoch 26/500\n",
            "196/196 [==============================] - 151s 773ms/step - loss: 1.2235 - sparse_categorical_accuracy: 0.5641 - val_loss: 1.6098 - val_sparse_categorical_accuracy: 0.5093\n",
            "Epoch 27/500\n",
            "196/196 [==============================] - 151s 773ms/step - loss: 1.2143 - sparse_categorical_accuracy: 0.5692 - val_loss: 1.6693 - val_sparse_categorical_accuracy: 0.4967\n",
            "Epoch 28/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 1.1849 - sparse_categorical_accuracy: 0.5812 - val_loss: 1.5292 - val_sparse_categorical_accuracy: 0.5256\n",
            "Epoch 29/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 1.1661 - sparse_categorical_accuracy: 0.5843 - val_loss: 1.4808 - val_sparse_categorical_accuracy: 0.5275\n",
            "Epoch 30/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 1.1416 - sparse_categorical_accuracy: 0.5942 - val_loss: 1.4843 - val_sparse_categorical_accuracy: 0.5325\n",
            "Epoch 31/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 1.1438 - sparse_categorical_accuracy: 0.5904 - val_loss: 1.4942 - val_sparse_categorical_accuracy: 0.5368\n",
            "Epoch 32/500\n",
            "196/196 [==============================] - 151s 772ms/step - loss: 1.1247 - sparse_categorical_accuracy: 0.5976 - val_loss: 1.5312 - val_sparse_categorical_accuracy: 0.5318\n",
            "Epoch 33/500\n",
            "196/196 [==============================] - 151s 773ms/step - loss: 1.0948 - sparse_categorical_accuracy: 0.6124 - val_loss: 1.5879 - val_sparse_categorical_accuracy: 0.5280\n",
            "Epoch 34/500\n",
            "196/196 [==============================] - 151s 773ms/step - loss: 1.0751 - sparse_categorical_accuracy: 0.6169 - val_loss: 1.5361 - val_sparse_categorical_accuracy: 0.5283\n",
            "Epoch 35/500\n",
            "196/196 [==============================] - 151s 772ms/step - loss: 1.0646 - sparse_categorical_accuracy: 0.6213 - val_loss: 1.4451 - val_sparse_categorical_accuracy: 0.5469\n",
            "Epoch 36/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 1.0574 - sparse_categorical_accuracy: 0.6234 - val_loss: 1.5247 - val_sparse_categorical_accuracy: 0.5358\n",
            "Epoch 37/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 1.0312 - sparse_categorical_accuracy: 0.6336 - val_loss: 1.7106 - val_sparse_categorical_accuracy: 0.5168\n",
            "Epoch 38/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 1.0292 - sparse_categorical_accuracy: 0.6293 - val_loss: 1.4920 - val_sparse_categorical_accuracy: 0.5525\n",
            "Epoch 39/500\n",
            "196/196 [==============================] - 151s 772ms/step - loss: 1.0095 - sparse_categorical_accuracy: 0.6413 - val_loss: 1.5455 - val_sparse_categorical_accuracy: 0.5497\n",
            "Epoch 40/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 0.9986 - sparse_categorical_accuracy: 0.6427 - val_loss: 1.5964 - val_sparse_categorical_accuracy: 0.5416\n",
            "Epoch 41/500\n",
            "196/196 [==============================] - 151s 772ms/step - loss: 0.9717 - sparse_categorical_accuracy: 0.6538 - val_loss: 1.6394 - val_sparse_categorical_accuracy: 0.5401\n",
            "Epoch 42/500\n",
            "196/196 [==============================] - 151s 772ms/step - loss: 0.9531 - sparse_categorical_accuracy: 0.6595 - val_loss: 1.5710 - val_sparse_categorical_accuracy: 0.5466\n",
            "Epoch 43/500\n",
            "196/196 [==============================] - 151s 772ms/step - loss: 0.9519 - sparse_categorical_accuracy: 0.6581 - val_loss: 1.5410 - val_sparse_categorical_accuracy: 0.5516\n",
            "Epoch 44/500\n",
            "196/196 [==============================] - 151s 772ms/step - loss: 0.9349 - sparse_categorical_accuracy: 0.6661 - val_loss: 1.5861 - val_sparse_categorical_accuracy: 0.5492\n",
            "Epoch 45/500\n",
            "196/196 [==============================] - 151s 772ms/step - loss: 0.9120 - sparse_categorical_accuracy: 0.6752 - val_loss: 1.6525 - val_sparse_categorical_accuracy: 0.5441\n",
            "Epoch 46/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 0.9104 - sparse_categorical_accuracy: 0.6711 - val_loss: 1.6249 - val_sparse_categorical_accuracy: 0.5409\n",
            "Epoch 47/500\n",
            "196/196 [==============================] - 151s 773ms/step - loss: 0.8847 - sparse_categorical_accuracy: 0.6840 - val_loss: 1.6226 - val_sparse_categorical_accuracy: 0.5443\n",
            "Epoch 48/500\n",
            "196/196 [==============================] - 152s 773ms/step - loss: 0.8628 - sparse_categorical_accuracy: 0.6909 - val_loss: 1.6757 - val_sparse_categorical_accuracy: 0.5440\n",
            "Epoch 49/500\n",
            "196/196 [==============================] - 151s 772ms/step - loss: 0.8428 - sparse_categorical_accuracy: 0.7013 - val_loss: 1.6095 - val_sparse_categorical_accuracy: 0.5523\n",
            "Epoch 50/500\n",
            "196/196 [==============================] - 151s 773ms/step - loss: 0.8415 - sparse_categorical_accuracy: 0.6983 - val_loss: 1.6558 - val_sparse_categorical_accuracy: 0.5589\n",
            "Epoch 51/500\n",
            "141/196 [====================>.........] - ETA: 39s - loss: 0.8095 - sparse_categorical_accuracy: 0.7104"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-28d65a404017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m                                           \u001b[0;34mf'_{cross_depth}cd'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                           \u001b[0;34mf'_{latent_depth}ld'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 )\n",
            "\u001b[0;32m<ipython-input-16-aa3cbc078dcb>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(_model, checkpoint_dir, history_dir, verbose)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m   )\n\u001b[1;32m     47\u001b[0m   \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GI95vsX634mE",
        "outputId": "aa87ad1e-8d75-446a-f125-8babc88e2de9"
      },
      "source": [
        "epochs = 500\n",
        "for max_freq in [image_height]:\n",
        "  for num_freq_bands in [16]:\n",
        "    for num_latents in [32]:\n",
        "      for latent_dim in [16]:\n",
        "        for latent_head_dim in [16]:\n",
        "          for cross_head_dim in [16]:\n",
        "            for cross_depth in [8]:\n",
        "              for latent_depth in [6]:\n",
        "                model = Perceiver(num_classes=num_classes, \n",
        "                                  image_height=image_height, \n",
        "                                  image_width=image_width,\n",
        "                                  max_freq=max_freq,\n",
        "                                  num_freq_bands=num_freq_bands,\n",
        "                                  freq_base=2.0,\n",
        "                                  num_latents=num_latents,\n",
        "                                  latent_dim=latent_dim,\n",
        "                                  latent_heads=1,\n",
        "                                  latent_head_dim=latent_head_dim,\n",
        "                                  cross_heads=1,\n",
        "                                  cross_head_dim=cross_head_dim,\n",
        "                                  attn_dropout=0.5,\n",
        "                                  mlp_dropout=0.5,\n",
        "                                  cross_depth=cross_depth,\n",
        "                                  latent_depth=latent_depth,             \n",
        "                                  weight_tie_layers=True,\n",
        "                                  preprocess=False).model()\n",
        "                model.summary()\n",
        "                train_and_eval(\n",
        "                  model, \n",
        "                  history_dir=SAVE_PATH + f'_PERCEIVER' \\\n",
        "                                          f'_{max_freq}fq' \\\n",
        "                                          f'_{num_freq_bands}fqb' \\\n",
        "                                          f'_{num_latents}l' \\\n",
        "                                          f'_{latent_dim}ldim' \\\n",
        "                                          f'_1lh' \\\n",
        "                                          f'_{latent_head_dim}lhdim' \\\n",
        "                                          f'_1ch' \\\n",
        "                                          f'_{cross_head_dim}chdim' \\\n",
        "                                          f'_{cross_depth}cd' \\\n",
        "                                          f'_{latent_depth}ld', \n",
        "                  verbose=1\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 32, 16)       512         resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 69)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 32, 16)       2906        input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 32, 16)       0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 32, 16)       560         skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 32, 16)       0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 32, 16)       1072        skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 32, 16)       0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 32, 16)       560         skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 32, 16)       0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 32, 16)       1072        skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 32, 16)       0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 32, 16)       560         skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 32, 16)       0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_2 (Transformer)   (None, 32, 16)       1072        skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_2 (Add)      (None, 32, 16)       0           latent_attn_0_2[0][0]            \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_2 (MLP)            (None, 32, 16)       560         skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_2 (Add)       (None, 32, 16)       0           latent_mlp_0_2[0][0]             \n",
            "                                                                 skip_latent_attn_0_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_3 (Transformer)   (None, 32, 16)       1072        skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_3 (Add)      (None, 32, 16)       0           latent_attn_0_3[0][0]            \n",
            "                                                                 skip_latent_mlp_0_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_3 (MLP)            (None, 32, 16)       560         skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_3 (Add)       (None, 32, 16)       0           latent_mlp_0_3[0][0]             \n",
            "                                                                 skip_latent_attn_0_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_4 (Transformer)   (None, 32, 16)       1072        skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_4 (Add)      (None, 32, 16)       0           latent_attn_0_4[0][0]            \n",
            "                                                                 skip_latent_mlp_0_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_4 (MLP)            (None, 32, 16)       560         skip_latent_attn_0_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_4 (Add)       (None, 32, 16)       0           latent_mlp_0_4[0][0]             \n",
            "                                                                 skip_latent_attn_0_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_5 (Transformer)   (None, 32, 16)       1072        skip_latent_mlp_0_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_5 (Add)      (None, 32, 16)       0           latent_attn_0_5[0][0]            \n",
            "                                                                 skip_latent_mlp_0_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_5 (MLP)            (None, 32, 16)       560         skip_latent_attn_0_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_5 (Add)       (None, 32, 16)       0           latent_mlp_0_5[0][0]             \n",
            "                                                                 skip_latent_attn_0_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 32, 16)       2906        skip_latent_mlp_0_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_1_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_2_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_3_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_4_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_5_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_6_5[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 32, 16)       0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 32, 16)       560         skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 32, 16)       0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 32, 16)       1072        skip_cross_mlp_1[0][0]           \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "                                                                 skip_cross_mlp_4[0][0]           \n",
            "                                                                 skip_cross_mlp_5[0][0]           \n",
            "                                                                 skip_cross_mlp_6[0][0]           \n",
            "                                                                 skip_cross_mlp_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 32, 16)       0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 32, 16)       560         skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 32, 16)       0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 32, 16)       1072        skip_latent_mlp_1_0[0][0]        \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "                                                                 skip_latent_mlp_4_0[0][0]        \n",
            "                                                                 skip_latent_mlp_5_0[0][0]        \n",
            "                                                                 skip_latent_mlp_6_0[0][0]        \n",
            "                                                                 skip_latent_mlp_7_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 32, 16)       0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 32, 16)       560         skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 32, 16)       0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_2 (Transformer)   (None, 32, 16)       1072        skip_latent_mlp_1_1[0][0]        \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "                                                                 skip_latent_mlp_4_1[0][0]        \n",
            "                                                                 skip_latent_mlp_5_1[0][0]        \n",
            "                                                                 skip_latent_mlp_6_1[0][0]        \n",
            "                                                                 skip_latent_mlp_7_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_2 (Add)      (None, 32, 16)       0           latent_attn_1_2[0][0]            \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_2 (MLP)            (None, 32, 16)       560         skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_2 (Add)       (None, 32, 16)       0           latent_mlp_1_2[0][0]             \n",
            "                                                                 skip_latent_attn_1_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_3 (Transformer)   (None, 32, 16)       1072        skip_latent_mlp_1_2[0][0]        \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "                                                                 skip_latent_mlp_3_2[0][0]        \n",
            "                                                                 skip_latent_mlp_4_2[0][0]        \n",
            "                                                                 skip_latent_mlp_5_2[0][0]        \n",
            "                                                                 skip_latent_mlp_6_2[0][0]        \n",
            "                                                                 skip_latent_mlp_7_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_3 (Add)      (None, 32, 16)       0           latent_attn_1_3[0][0]            \n",
            "                                                                 skip_latent_mlp_1_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_3 (MLP)            (None, 32, 16)       560         skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_3 (Add)       (None, 32, 16)       0           latent_mlp_1_3[0][0]             \n",
            "                                                                 skip_latent_attn_1_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_4 (Transformer)   (None, 32, 16)       1072        skip_latent_mlp_1_3[0][0]        \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "                                                                 skip_latent_mlp_3_3[0][0]        \n",
            "                                                                 skip_latent_mlp_4_3[0][0]        \n",
            "                                                                 skip_latent_mlp_5_3[0][0]        \n",
            "                                                                 skip_latent_mlp_6_3[0][0]        \n",
            "                                                                 skip_latent_mlp_7_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_4 (Add)      (None, 32, 16)       0           latent_attn_1_4[0][0]            \n",
            "                                                                 skip_latent_mlp_1_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_4 (MLP)            (None, 32, 16)       560         skip_latent_attn_1_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_4 (Add)       (None, 32, 16)       0           latent_mlp_1_4[0][0]             \n",
            "                                                                 skip_latent_attn_1_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_5 (Transformer)   (None, 32, 16)       1072        skip_latent_mlp_1_4[0][0]        \n",
            "                                                                 skip_latent_mlp_2_4[0][0]        \n",
            "                                                                 skip_latent_mlp_3_4[0][0]        \n",
            "                                                                 skip_latent_mlp_4_4[0][0]        \n",
            "                                                                 skip_latent_mlp_5_4[0][0]        \n",
            "                                                                 skip_latent_mlp_6_4[0][0]        \n",
            "                                                                 skip_latent_mlp_7_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_5 (Add)      (None, 32, 16)       0           latent_attn_1_5[0][0]            \n",
            "                                                                 skip_latent_mlp_1_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_5 (MLP)            (None, 32, 16)       560         skip_latent_attn_1_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_5 (Add)       (None, 32, 16)       0           latent_mlp_1_5[0][0]             \n",
            "                                                                 skip_latent_attn_1_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_2 (Add)         (None, 32, 16)       0           cross_attn_1[1][0]               \n",
            "                                                                 skip_latent_mlp_1_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_2 (MLP)               (None, 32, 16)       560         skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_2 (Add)          (None, 32, 16)       0           cross_mlp_2[0][0]                \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_0 (Add)      (None, 32, 16)       0           latent_attn_1_0[1][0]            \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_0 (MLP)            (None, 32, 16)       560         skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_0 (Add)       (None, 32, 16)       0           latent_mlp_2_0[0][0]             \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_1 (Add)      (None, 32, 16)       0           latent_attn_1_1[1][0]            \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_1 (MLP)            (None, 32, 16)       560         skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_1 (Add)       (None, 32, 16)       0           latent_mlp_2_1[0][0]             \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_2 (Add)      (None, 32, 16)       0           latent_attn_1_2[1][0]            \n",
            "                                                                 skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_2 (MLP)            (None, 32, 16)       560         skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_2 (Add)       (None, 32, 16)       0           latent_mlp_2_2[0][0]             \n",
            "                                                                 skip_latent_attn_2_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_3 (Add)      (None, 32, 16)       0           latent_attn_1_3[1][0]            \n",
            "                                                                 skip_latent_mlp_2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_3 (MLP)            (None, 32, 16)       560         skip_latent_attn_2_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_3 (Add)       (None, 32, 16)       0           latent_mlp_2_3[0][0]             \n",
            "                                                                 skip_latent_attn_2_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_4 (Add)      (None, 32, 16)       0           latent_attn_1_4[1][0]            \n",
            "                                                                 skip_latent_mlp_2_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_4 (MLP)            (None, 32, 16)       560         skip_latent_attn_2_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_4 (Add)       (None, 32, 16)       0           latent_mlp_2_4[0][0]             \n",
            "                                                                 skip_latent_attn_2_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_5 (Add)      (None, 32, 16)       0           latent_attn_1_5[1][0]            \n",
            "                                                                 skip_latent_mlp_2_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_5 (MLP)            (None, 32, 16)       560         skip_latent_attn_2_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_5 (Add)       (None, 32, 16)       0           latent_mlp_2_5[0][0]             \n",
            "                                                                 skip_latent_attn_2_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_3 (Add)         (None, 32, 16)       0           cross_attn_1[2][0]               \n",
            "                                                                 skip_latent_mlp_2_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_3 (MLP)               (None, 32, 16)       560         skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_3 (Add)          (None, 32, 16)       0           cross_mlp_3[0][0]                \n",
            "                                                                 skip_cross_attn_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_0 (Add)      (None, 32, 16)       0           latent_attn_1_0[2][0]            \n",
            "                                                                 skip_cross_mlp_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_0 (MLP)            (None, 32, 16)       560         skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_0 (Add)       (None, 32, 16)       0           latent_mlp_3_0[0][0]             \n",
            "                                                                 skip_latent_attn_3_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_1 (Add)      (None, 32, 16)       0           latent_attn_1_1[2][0]            \n",
            "                                                                 skip_latent_mlp_3_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_1 (MLP)            (None, 32, 16)       560         skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_1 (Add)       (None, 32, 16)       0           latent_mlp_3_1[0][0]             \n",
            "                                                                 skip_latent_attn_3_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_2 (Add)      (None, 32, 16)       0           latent_attn_1_2[2][0]            \n",
            "                                                                 skip_latent_mlp_3_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_2 (MLP)            (None, 32, 16)       560         skip_latent_attn_3_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_2 (Add)       (None, 32, 16)       0           latent_mlp_3_2[0][0]             \n",
            "                                                                 skip_latent_attn_3_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_3 (Add)      (None, 32, 16)       0           latent_attn_1_3[2][0]            \n",
            "                                                                 skip_latent_mlp_3_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_3 (MLP)            (None, 32, 16)       560         skip_latent_attn_3_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_3 (Add)       (None, 32, 16)       0           latent_mlp_3_3[0][0]             \n",
            "                                                                 skip_latent_attn_3_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_4 (Add)      (None, 32, 16)       0           latent_attn_1_4[2][0]            \n",
            "                                                                 skip_latent_mlp_3_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_4 (MLP)            (None, 32, 16)       560         skip_latent_attn_3_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_4 (Add)       (None, 32, 16)       0           latent_mlp_3_4[0][0]             \n",
            "                                                                 skip_latent_attn_3_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_3_5 (Add)      (None, 32, 16)       0           latent_attn_1_5[2][0]            \n",
            "                                                                 skip_latent_mlp_3_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_3_5 (MLP)            (None, 32, 16)       560         skip_latent_attn_3_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_3_5 (Add)       (None, 32, 16)       0           latent_mlp_3_5[0][0]             \n",
            "                                                                 skip_latent_attn_3_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_4 (Add)         (None, 32, 16)       0           cross_attn_1[3][0]               \n",
            "                                                                 skip_latent_mlp_3_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_4 (MLP)               (None, 32, 16)       560         skip_cross_attn_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_4 (Add)          (None, 32, 16)       0           cross_mlp_4[0][0]                \n",
            "                                                                 skip_cross_attn_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_0 (Add)      (None, 32, 16)       0           latent_attn_1_0[3][0]            \n",
            "                                                                 skip_cross_mlp_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_0 (MLP)            (None, 32, 16)       560         skip_latent_attn_4_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_0 (Add)       (None, 32, 16)       0           latent_mlp_4_0[0][0]             \n",
            "                                                                 skip_latent_attn_4_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_1 (Add)      (None, 32, 16)       0           latent_attn_1_1[3][0]            \n",
            "                                                                 skip_latent_mlp_4_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_1 (MLP)            (None, 32, 16)       560         skip_latent_attn_4_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_1 (Add)       (None, 32, 16)       0           latent_mlp_4_1[0][0]             \n",
            "                                                                 skip_latent_attn_4_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_2 (Add)      (None, 32, 16)       0           latent_attn_1_2[3][0]            \n",
            "                                                                 skip_latent_mlp_4_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_2 (MLP)            (None, 32, 16)       560         skip_latent_attn_4_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_2 (Add)       (None, 32, 16)       0           latent_mlp_4_2[0][0]             \n",
            "                                                                 skip_latent_attn_4_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_3 (Add)      (None, 32, 16)       0           latent_attn_1_3[3][0]            \n",
            "                                                                 skip_latent_mlp_4_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_3 (MLP)            (None, 32, 16)       560         skip_latent_attn_4_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_3 (Add)       (None, 32, 16)       0           latent_mlp_4_3[0][0]             \n",
            "                                                                 skip_latent_attn_4_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_4 (Add)      (None, 32, 16)       0           latent_attn_1_4[3][0]            \n",
            "                                                                 skip_latent_mlp_4_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_4 (MLP)            (None, 32, 16)       560         skip_latent_attn_4_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_4 (Add)       (None, 32, 16)       0           latent_mlp_4_4[0][0]             \n",
            "                                                                 skip_latent_attn_4_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_4_5 (Add)      (None, 32, 16)       0           latent_attn_1_5[3][0]            \n",
            "                                                                 skip_latent_mlp_4_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_4_5 (MLP)            (None, 32, 16)       560         skip_latent_attn_4_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_4_5 (Add)       (None, 32, 16)       0           latent_mlp_4_5[0][0]             \n",
            "                                                                 skip_latent_attn_4_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_5 (Add)         (None, 32, 16)       0           cross_attn_1[4][0]               \n",
            "                                                                 skip_latent_mlp_4_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_5 (MLP)               (None, 32, 16)       560         skip_cross_attn_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_5 (Add)          (None, 32, 16)       0           cross_mlp_5[0][0]                \n",
            "                                                                 skip_cross_attn_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_0 (Add)      (None, 32, 16)       0           latent_attn_1_0[4][0]            \n",
            "                                                                 skip_cross_mlp_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_0 (MLP)            (None, 32, 16)       560         skip_latent_attn_5_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_0 (Add)       (None, 32, 16)       0           latent_mlp_5_0[0][0]             \n",
            "                                                                 skip_latent_attn_5_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_1 (Add)      (None, 32, 16)       0           latent_attn_1_1[4][0]            \n",
            "                                                                 skip_latent_mlp_5_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_1 (MLP)            (None, 32, 16)       560         skip_latent_attn_5_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_1 (Add)       (None, 32, 16)       0           latent_mlp_5_1[0][0]             \n",
            "                                                                 skip_latent_attn_5_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_2 (Add)      (None, 32, 16)       0           latent_attn_1_2[4][0]            \n",
            "                                                                 skip_latent_mlp_5_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_2 (MLP)            (None, 32, 16)       560         skip_latent_attn_5_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_2 (Add)       (None, 32, 16)       0           latent_mlp_5_2[0][0]             \n",
            "                                                                 skip_latent_attn_5_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_3 (Add)      (None, 32, 16)       0           latent_attn_1_3[4][0]            \n",
            "                                                                 skip_latent_mlp_5_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_3 (MLP)            (None, 32, 16)       560         skip_latent_attn_5_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_3 (Add)       (None, 32, 16)       0           latent_mlp_5_3[0][0]             \n",
            "                                                                 skip_latent_attn_5_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_4 (Add)      (None, 32, 16)       0           latent_attn_1_4[4][0]            \n",
            "                                                                 skip_latent_mlp_5_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_4 (MLP)            (None, 32, 16)       560         skip_latent_attn_5_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_4 (Add)       (None, 32, 16)       0           latent_mlp_5_4[0][0]             \n",
            "                                                                 skip_latent_attn_5_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_5_5 (Add)      (None, 32, 16)       0           latent_attn_1_5[4][0]            \n",
            "                                                                 skip_latent_mlp_5_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_5_5 (MLP)            (None, 32, 16)       560         skip_latent_attn_5_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_5_5 (Add)       (None, 32, 16)       0           latent_mlp_5_5[0][0]             \n",
            "                                                                 skip_latent_attn_5_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_6 (Add)         (None, 32, 16)       0           cross_attn_1[5][0]               \n",
            "                                                                 skip_latent_mlp_5_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_6 (MLP)               (None, 32, 16)       560         skip_cross_attn_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_6 (Add)          (None, 32, 16)       0           cross_mlp_6[0][0]                \n",
            "                                                                 skip_cross_attn_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_0 (Add)      (None, 32, 16)       0           latent_attn_1_0[5][0]            \n",
            "                                                                 skip_cross_mlp_6[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_0 (MLP)            (None, 32, 16)       560         skip_latent_attn_6_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_0 (Add)       (None, 32, 16)       0           latent_mlp_6_0[0][0]             \n",
            "                                                                 skip_latent_attn_6_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_1 (Add)      (None, 32, 16)       0           latent_attn_1_1[5][0]            \n",
            "                                                                 skip_latent_mlp_6_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_1 (MLP)            (None, 32, 16)       560         skip_latent_attn_6_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_1 (Add)       (None, 32, 16)       0           latent_mlp_6_1[0][0]             \n",
            "                                                                 skip_latent_attn_6_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_2 (Add)      (None, 32, 16)       0           latent_attn_1_2[5][0]            \n",
            "                                                                 skip_latent_mlp_6_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_2 (MLP)            (None, 32, 16)       560         skip_latent_attn_6_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_2 (Add)       (None, 32, 16)       0           latent_mlp_6_2[0][0]             \n",
            "                                                                 skip_latent_attn_6_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_3 (Add)      (None, 32, 16)       0           latent_attn_1_3[5][0]            \n",
            "                                                                 skip_latent_mlp_6_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_3 (MLP)            (None, 32, 16)       560         skip_latent_attn_6_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_3 (Add)       (None, 32, 16)       0           latent_mlp_6_3[0][0]             \n",
            "                                                                 skip_latent_attn_6_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_4 (Add)      (None, 32, 16)       0           latent_attn_1_4[5][0]            \n",
            "                                                                 skip_latent_mlp_6_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_4 (MLP)            (None, 32, 16)       560         skip_latent_attn_6_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_4 (Add)       (None, 32, 16)       0           latent_mlp_6_4[0][0]             \n",
            "                                                                 skip_latent_attn_6_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_6_5 (Add)      (None, 32, 16)       0           latent_attn_1_5[5][0]            \n",
            "                                                                 skip_latent_mlp_6_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_6_5 (MLP)            (None, 32, 16)       560         skip_latent_attn_6_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_6_5 (Add)       (None, 32, 16)       0           latent_mlp_6_5[0][0]             \n",
            "                                                                 skip_latent_attn_6_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_7 (Add)         (None, 32, 16)       0           cross_attn_1[6][0]               \n",
            "                                                                 skip_latent_mlp_6_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_7 (MLP)               (None, 32, 16)       560         skip_cross_attn_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_7 (Add)          (None, 32, 16)       0           cross_mlp_7[0][0]                \n",
            "                                                                 skip_cross_attn_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_0 (Add)      (None, 32, 16)       0           latent_attn_1_0[6][0]            \n",
            "                                                                 skip_cross_mlp_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_0 (MLP)            (None, 32, 16)       560         skip_latent_attn_7_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_0 (Add)       (None, 32, 16)       0           latent_mlp_7_0[0][0]             \n",
            "                                                                 skip_latent_attn_7_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_1 (Add)      (None, 32, 16)       0           latent_attn_1_1[6][0]            \n",
            "                                                                 skip_latent_mlp_7_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_1 (MLP)            (None, 32, 16)       560         skip_latent_attn_7_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_1 (Add)       (None, 32, 16)       0           latent_mlp_7_1[0][0]             \n",
            "                                                                 skip_latent_attn_7_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_2 (Add)      (None, 32, 16)       0           latent_attn_1_2[6][0]            \n",
            "                                                                 skip_latent_mlp_7_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_2 (MLP)            (None, 32, 16)       560         skip_latent_attn_7_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_2 (Add)       (None, 32, 16)       0           latent_mlp_7_2[0][0]             \n",
            "                                                                 skip_latent_attn_7_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_3 (Add)      (None, 32, 16)       0           latent_attn_1_3[6][0]            \n",
            "                                                                 skip_latent_mlp_7_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_3 (MLP)            (None, 32, 16)       560         skip_latent_attn_7_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_3 (Add)       (None, 32, 16)       0           latent_mlp_7_3[0][0]             \n",
            "                                                                 skip_latent_attn_7_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_4 (Add)      (None, 32, 16)       0           latent_attn_1_4[6][0]            \n",
            "                                                                 skip_latent_mlp_7_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_4 (MLP)            (None, 32, 16)       560         skip_latent_attn_7_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_4 (Add)       (None, 32, 16)       0           latent_mlp_7_4[0][0]             \n",
            "                                                                 skip_latent_attn_7_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_7_5 (Add)      (None, 32, 16)       0           latent_attn_1_5[6][0]            \n",
            "                                                                 skip_latent_mlp_7_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_7_5 (MLP)            (None, 32, 16)       560         skip_latent_attn_7_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_7_5 (Add)       (None, 32, 16)       0           latent_mlp_7_5[0][0]             \n",
            "                                                                 skip_latent_attn_7_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 16)        0           skip_latent_mlp_7_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        170         avgpool[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 50,718\n",
            "Trainable params: 50,718\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "196/196 [==============================] - 122s 315ms/step - loss: 4.2845 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.5232 - val_sparse_categorical_accuracy: 0.1527\n",
            "Epoch 2/500\n",
            "196/196 [==============================] - 49s 251ms/step - loss: 2.3270 - sparse_categorical_accuracy: 0.1469 - val_loss: 2.2919 - val_sparse_categorical_accuracy: 0.1793\n",
            "Epoch 3/500\n",
            "196/196 [==============================] - 49s 251ms/step - loss: 2.1932 - sparse_categorical_accuracy: 0.1733 - val_loss: 2.2035 - val_sparse_categorical_accuracy: 0.1847\n",
            "Epoch 4/500\n",
            "196/196 [==============================] - 49s 250ms/step - loss: 2.1006 - sparse_categorical_accuracy: 0.2124 - val_loss: 2.0697 - val_sparse_categorical_accuracy: 0.2233\n",
            "Epoch 5/500\n",
            "196/196 [==============================] - 49s 252ms/step - loss: 2.0228 - sparse_categorical_accuracy: 0.2518 - val_loss: 2.1116 - val_sparse_categorical_accuracy: 0.2274\n",
            "Epoch 6/500\n",
            "196/196 [==============================] - 49s 250ms/step - loss: 1.9766 - sparse_categorical_accuracy: 0.2764 - val_loss: 2.0740 - val_sparse_categorical_accuracy: 0.2452\n",
            "Epoch 7/500\n",
            "196/196 [==============================] - 49s 251ms/step - loss: 1.9329 - sparse_categorical_accuracy: 0.2970 - val_loss: 1.9672 - val_sparse_categorical_accuracy: 0.2842\n",
            "Epoch 8/500\n",
            "196/196 [==============================] - 49s 251ms/step - loss: 1.9077 - sparse_categorical_accuracy: 0.3080 - val_loss: 1.9308 - val_sparse_categorical_accuracy: 0.2942\n",
            "Epoch 9/500\n",
            "196/196 [==============================] - 49s 251ms/step - loss: 1.8764 - sparse_categorical_accuracy: 0.3173 - val_loss: 1.9800 - val_sparse_categorical_accuracy: 0.2792\n",
            "Epoch 10/500\n",
            "196/196 [==============================] - 49s 250ms/step - loss: 1.8518 - sparse_categorical_accuracy: 0.3299 - val_loss: 1.9062 - val_sparse_categorical_accuracy: 0.3171\n",
            "Epoch 11/500\n",
            "196/196 [==============================] - 50s 253ms/step - loss: 1.8250 - sparse_categorical_accuracy: 0.3384 - val_loss: 1.8109 - val_sparse_categorical_accuracy: 0.3505\n",
            "Epoch 12/500\n",
            "196/196 [==============================] - 49s 251ms/step - loss: 1.7944 - sparse_categorical_accuracy: 0.3516 - val_loss: 1.8748 - val_sparse_categorical_accuracy: 0.3337\n",
            "Epoch 13/500\n",
            "196/196 [==============================] - 49s 251ms/step - loss: 1.7847 - sparse_categorical_accuracy: 0.3558 - val_loss: 1.7894 - val_sparse_categorical_accuracy: 0.3665\n",
            "Epoch 14/500\n",
            "196/196 [==============================] - 49s 250ms/step - loss: 1.7266 - sparse_categorical_accuracy: 0.3773 - val_loss: 1.7837 - val_sparse_categorical_accuracy: 0.3600\n",
            "Epoch 15/500\n",
            "196/196 [==============================] - 49s 250ms/step - loss: 1.6986 - sparse_categorical_accuracy: 0.3912 - val_loss: 1.7968 - val_sparse_categorical_accuracy: 0.3650\n",
            "Epoch 16/500\n",
            "196/196 [==============================] - 49s 250ms/step - loss: 1.6760 - sparse_categorical_accuracy: 0.4023 - val_loss: 1.6887 - val_sparse_categorical_accuracy: 0.3970\n",
            "Epoch 17/500\n",
            "196/196 [==============================] - 49s 250ms/step - loss: 1.6448 - sparse_categorical_accuracy: 0.4119 - val_loss: 1.6793 - val_sparse_categorical_accuracy: 0.4025\n",
            "Epoch 18/500\n",
            "196/196 [==============================] - 50s 254ms/step - loss: 1.6071 - sparse_categorical_accuracy: 0.4271 - val_loss: 1.6808 - val_sparse_categorical_accuracy: 0.4038\n",
            "Epoch 19/500\n",
            "196/196 [==============================] - 49s 251ms/step - loss: 1.5852 - sparse_categorical_accuracy: 0.4331 - val_loss: 1.6258 - val_sparse_categorical_accuracy: 0.4241\n",
            "Epoch 20/500\n",
            "196/196 [==============================] - 49s 251ms/step - loss: 1.5748 - sparse_categorical_accuracy: 0.4346 - val_loss: 1.6766 - val_sparse_categorical_accuracy: 0.4085\n",
            "Epoch 21/500\n",
            "196/196 [==============================] - 49s 250ms/step - loss: 1.5553 - sparse_categorical_accuracy: 0.4459 - val_loss: 1.6327 - val_sparse_categorical_accuracy: 0.4280\n",
            "Epoch 22/500\n",
            "196/196 [==============================] - 49s 251ms/step - loss: 1.5410 - sparse_categorical_accuracy: 0.4512 - val_loss: 1.5691 - val_sparse_categorical_accuracy: 0.4418\n",
            "Epoch 23/500\n",
            " 78/196 [==========>...................] - ETA: 27s - loss: 1.5357 - sparse_categorical_accuracy: 0.4511"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e0fa54541dfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m                                           \u001b[0;34mf'_{cross_depth}cd'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                           \u001b[0;34mf'_{latent_depth}ld'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 )\n",
            "\u001b[0;32m<ipython-input-16-aa3cbc078dcb>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(_model, checkpoint_dir, history_dir, verbose)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m   )\n\u001b[1;32m     47\u001b[0m   \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mlqgnSMT98Iy",
        "outputId": "bde539d2-6317-414a-c54d-963543b60edc"
      },
      "source": [
        "epochs = 500\n",
        "for max_freq in [image_height]:\n",
        "  for num_freq_bands in [16]:\n",
        "    for num_latents in [128]:\n",
        "      for latent_dim in [64]:\n",
        "        for latent_head_dim in [64]:\n",
        "          for cross_head_dim in [64]:\n",
        "            for cross_depth in [3]:\n",
        "              for latent_depth in [2]:\n",
        "                model = Perceiver(num_classes=num_classes, \n",
        "                                  image_height=image_height, \n",
        "                                  image_width=image_width,\n",
        "                                  max_freq=max_freq,\n",
        "                                  num_freq_bands=num_freq_bands,\n",
        "                                  freq_base=2.0,\n",
        "                                  num_latents=num_latents,\n",
        "                                  latent_dim=latent_dim,\n",
        "                                  latent_heads=1,\n",
        "                                  latent_head_dim=latent_head_dim,\n",
        "                                  cross_heads=1,\n",
        "                                  cross_head_dim=cross_head_dim,\n",
        "                                  attn_dropout=0.5,\n",
        "                                  mlp_dropout=0.5,\n",
        "                                  cross_depth=cross_depth,\n",
        "                                  latent_depth=latent_depth,             \n",
        "                                  weight_tie_layers=True,\n",
        "                                  preprocess=False).model()\n",
        "                model.summary()\n",
        "                train_and_eval(\n",
        "                  model, \n",
        "                  history_dir=SAVE_PATH + f'_PERCEIVER' \\\n",
        "                                          f'_{max_freq}fq' \\\n",
        "                                          f'_{num_freq_bands}fqb' \\\n",
        "                                          f'_{num_latents}l' \\\n",
        "                                          f'_{latent_dim}ldim' \\\n",
        "                                          f'_1lh' \\\n",
        "                                          f'_{latent_head_dim}lhdim' \\\n",
        "                                          f'_1ch' \\\n",
        "                                          f'_{cross_head_dim}chdim' \\\n",
        "                                          f'_{cross_depth}cd' \\\n",
        "                                          f'_{latent_depth}ld', \n",
        "                  verbose=1\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 128, 64)      8192        resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 69)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 128, 64)      17354       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 128, 64)      0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 128, 64)      8384        skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 128, 64)      0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 128, 64)      16576       skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 128, 64)      0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 128, 64)      8384        skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 128, 64)      0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 128, 64)      0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 128, 64)      8384        skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 128, 64)      0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 128, 64)      17354       skip_latent_mlp_0_1[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 128, 64)      0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 128, 64)      8384        skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 128, 64)      0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 128, 64)      16576       skip_cross_mlp_1[0][0]           \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 128, 64)      0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 128, 64)      8384        skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 128, 64)      0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 128, 64)      16576       skip_latent_mlp_1_0[0][0]        \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 128, 64)      0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 128, 64)      8384        skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 128, 64)      0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_2 (Add)         (None, 128, 64)      0           cross_attn_1[1][0]               \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_2 (MLP)               (None, 128, 64)      8384        skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_2 (Add)          (None, 128, 64)      0           cross_mlp_2[0][0]                \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_0 (Add)      (None, 128, 64)      0           latent_attn_1_0[1][0]            \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_0 (MLP)            (None, 128, 64)      8384        skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_0 (Add)       (None, 128, 64)      0           latent_mlp_2_0[0][0]             \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_1 (Add)      (None, 128, 64)      0           latent_attn_1_1[1][0]            \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_1 (MLP)            (None, 128, 64)      8384        skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_1 (Add)       (None, 128, 64)      0           latent_mlp_2_1[0][0]             \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 64)        0           skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        650         avgpool[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 185,310\n",
            "Trainable params: 185,310\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "102/196 [==============>...............] - ETA: 3:01 - loss: 2.7242 - sparse_categorical_accuracy: 0.1369"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-236e84a23e15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                   preprocess=False).model()\n\u001b[1;32m     28\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 train_and_eval(\n\u001b[0m\u001b[1;32m     30\u001b[0m                   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                   \u001b[0mhistory_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAVE_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'_PERCEIVER'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-aa3cbc078dcb>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(_model, checkpoint_dir, history_dir, verbose)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   history = _model.fit(\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yDL_frF8ifbD",
        "outputId": "cecc0cdc-d4df-44d1-a0a5-4f6f9462c132"
      },
      "source": [
        "epochs = 500\n",
        "for max_freq in [image_height]:\n",
        "  for num_freq_bands in [16]:\n",
        "    for num_latents in [128]:\n",
        "      for latent_dim in [128]:\n",
        "        for latent_head_dim in [128]:\n",
        "          for cross_head_dim in [128]:\n",
        "            for cross_depth in [3]:\n",
        "              for latent_depth in [2]:\n",
        "                model = Perceiver(num_classes=num_classes, \n",
        "                                  image_height=image_height, \n",
        "                                  image_width=image_width,\n",
        "                                  max_freq=max_freq,\n",
        "                                  num_freq_bands=num_freq_bands,\n",
        "                                  freq_base=2.0,\n",
        "                                  num_latents=num_latents,\n",
        "                                  latent_dim=latent_dim,\n",
        "                                  latent_heads=1,\n",
        "                                  latent_head_dim=latent_head_dim,\n",
        "                                  cross_heads=1,\n",
        "                                  cross_head_dim=cross_head_dim,\n",
        "                                  attn_dropout=0.5,\n",
        "                                  mlp_dropout=0.5,\n",
        "                                  cross_depth=cross_depth,\n",
        "                                  latent_depth=latent_depth,             \n",
        "                                  weight_tie_layers=True,\n",
        "                                  preprocess=False).model()\n",
        "                model.summary()\n",
        "                train_and_eval(\n",
        "                  model, \n",
        "                  history_dir=SAVE_PATH + f'_PERCEIVER' \\\n",
        "                                          f'_{max_freq}fq' \\\n",
        "                                          f'_{num_freq_bands}fqb' \\\n",
        "                                          f'_{num_latents}l' \\\n",
        "                                          f'_{latent_dim}ldim' \\\n",
        "                                          f'_1lh' \\\n",
        "                                          f'_{latent_head_dim}lhdim' \\\n",
        "                                          f'_1ch' \\\n",
        "                                          f'_{cross_head_dim}chdim' \\\n",
        "                                          f'_{cross_depth}cd' \\\n",
        "                                          f'_{latent_depth}ld', \n",
        "                  verbose=1\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescale (Rescaling)             (None, 32, 32, 3)    0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "resize (Resizing)               (None, 32, 32, 3)    0           rescale[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_latent (InputLatentArray) (None, 128, 128)     16384       resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "input_data (InputByteArray)     (None, 1024, 69)     0           resize[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_0 (Transformer)      (None, 128, 128)     50954       input_latent[0][0]               \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_0 (Add)         (None, 128, 128)     0           cross_attn_0[0][0]               \n",
            "                                                                 input_latent[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_0 (MLP)               (None, 128, 128)     33152       skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_0 (Add)          (None, 128, 128)     0           cross_mlp_0[0][0]                \n",
            "                                                                 skip_cross_attn_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_0 (Transformer)   (None, 128, 128)     65920       skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_0 (Add)      (None, 128, 128)     0           latent_attn_0_0[0][0]            \n",
            "                                                                 skip_cross_mlp_0[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_0 (MLP)            (None, 128, 128)     33152       skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_0 (Add)       (None, 128, 128)     0           latent_mlp_0_0[0][0]             \n",
            "                                                                 skip_latent_attn_0_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_0_1 (Transformer)   (None, 128, 128)     65920       skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_0_1 (Add)      (None, 128, 128)     0           latent_attn_0_1[0][0]            \n",
            "                                                                 skip_latent_mlp_0_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_0_1 (MLP)            (None, 128, 128)     33152       skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_0_1 (Add)       (None, 128, 128)     0           latent_mlp_0_1[0][0]             \n",
            "                                                                 skip_latent_attn_0_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "cross_attn_1 (Transformer)      (None, 128, 128)     50954       skip_latent_mlp_0_1[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "                                                                 input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_1 (Add)         (None, 128, 128)     0           cross_attn_1[0][0]               \n",
            "                                                                 skip_latent_mlp_0_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_1 (MLP)               (None, 128, 128)     33152       skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_1 (Add)          (None, 128, 128)     0           cross_mlp_1[0][0]                \n",
            "                                                                 skip_cross_attn_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_0 (Transformer)   (None, 128, 128)     65920       skip_cross_mlp_1[0][0]           \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_0 (Add)      (None, 128, 128)     0           latent_attn_1_0[0][0]            \n",
            "                                                                 skip_cross_mlp_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_0 (MLP)            (None, 128, 128)     33152       skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_0 (Add)       (None, 128, 128)     0           latent_mlp_1_0[0][0]             \n",
            "                                                                 skip_latent_attn_1_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "latent_attn_1_1 (Transformer)   (None, 128, 128)     65920       skip_latent_mlp_1_0[0][0]        \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_1_1 (Add)      (None, 128, 128)     0           latent_attn_1_1[0][0]            \n",
            "                                                                 skip_latent_mlp_1_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_1_1 (MLP)            (None, 128, 128)     33152       skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_1_1 (Add)       (None, 128, 128)     0           latent_mlp_1_1[0][0]             \n",
            "                                                                 skip_latent_attn_1_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_attn_2 (Add)         (None, 128, 128)     0           cross_attn_1[1][0]               \n",
            "                                                                 skip_latent_mlp_1_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cross_mlp_2 (MLP)               (None, 128, 128)     33152       skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_cross_mlp_2 (Add)          (None, 128, 128)     0           cross_mlp_2[0][0]                \n",
            "                                                                 skip_cross_attn_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_0 (Add)      (None, 128, 128)     0           latent_attn_1_0[1][0]            \n",
            "                                                                 skip_cross_mlp_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_0 (MLP)            (None, 128, 128)     33152       skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_0 (Add)       (None, 128, 128)     0           latent_mlp_2_0[0][0]             \n",
            "                                                                 skip_latent_attn_2_0[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_attn_2_1 (Add)      (None, 128, 128)     0           latent_attn_1_1[1][0]            \n",
            "                                                                 skip_latent_mlp_2_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "latent_mlp_2_1 (MLP)            (None, 128, 128)     33152       skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "skip_latent_mlp_2_1 (Add)       (None, 128, 128)     0           latent_mlp_2_1[0][0]             \n",
            "                                                                 skip_latent_attn_2_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (AveragePooling1D)      (None, 1, 128)       0           skip_latent_mlp_2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "logits (Dense)                  (None, 1, 10)        1290        avgpool[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 681,630\n",
            "Trainable params: 681,630\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "196/196 [==============================] - 50s 217ms/step - loss: 2.8333 - sparse_categorical_accuracy: 0.1650 - val_loss: 2.5120 - val_sparse_categorical_accuracy: 0.3128\n",
            "Epoch 2/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.8890 - sparse_categorical_accuracy: 0.3091 - val_loss: 2.2367 - val_sparse_categorical_accuracy: 0.3751\n",
            "Epoch 3/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 1.7202 - sparse_categorical_accuracy: 0.3780 - val_loss: 2.1699 - val_sparse_categorical_accuracy: 0.3917\n",
            "Epoch 4/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 1.6263 - sparse_categorical_accuracy: 0.4141 - val_loss: 1.9136 - val_sparse_categorical_accuracy: 0.4263\n",
            "Epoch 5/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.5524 - sparse_categorical_accuracy: 0.4411 - val_loss: 1.9673 - val_sparse_categorical_accuracy: 0.4317\n",
            "Epoch 6/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.5038 - sparse_categorical_accuracy: 0.4588 - val_loss: 1.9625 - val_sparse_categorical_accuracy: 0.4505\n",
            "Epoch 7/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.4347 - sparse_categorical_accuracy: 0.4860 - val_loss: 1.8736 - val_sparse_categorical_accuracy: 0.4701\n",
            "Epoch 8/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.4011 - sparse_categorical_accuracy: 0.4980 - val_loss: 1.8379 - val_sparse_categorical_accuracy: 0.4744\n",
            "Epoch 9/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.3497 - sparse_categorical_accuracy: 0.5163 - val_loss: 1.7944 - val_sparse_categorical_accuracy: 0.4891\n",
            "Epoch 10/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.3256 - sparse_categorical_accuracy: 0.5204 - val_loss: 1.7292 - val_sparse_categorical_accuracy: 0.4961\n",
            "Epoch 11/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.2854 - sparse_categorical_accuracy: 0.5373 - val_loss: 1.7634 - val_sparse_categorical_accuracy: 0.5082\n",
            "Epoch 12/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.2676 - sparse_categorical_accuracy: 0.5492 - val_loss: 1.6720 - val_sparse_categorical_accuracy: 0.5185\n",
            "Epoch 13/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.2199 - sparse_categorical_accuracy: 0.5653 - val_loss: 1.8842 - val_sparse_categorical_accuracy: 0.5059\n",
            "Epoch 14/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.2002 - sparse_categorical_accuracy: 0.5713 - val_loss: 1.7376 - val_sparse_categorical_accuracy: 0.5153\n",
            "Epoch 15/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.1681 - sparse_categorical_accuracy: 0.5823 - val_loss: 1.5285 - val_sparse_categorical_accuracy: 0.5441\n",
            "Epoch 16/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 1.1566 - sparse_categorical_accuracy: 0.5854 - val_loss: 1.6946 - val_sparse_categorical_accuracy: 0.5249\n",
            "Epoch 17/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 1.1348 - sparse_categorical_accuracy: 0.5928 - val_loss: 1.6570 - val_sparse_categorical_accuracy: 0.5402\n",
            "Epoch 18/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 1.1078 - sparse_categorical_accuracy: 0.6033 - val_loss: 1.6485 - val_sparse_categorical_accuracy: 0.5473\n",
            "Epoch 19/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 1.0953 - sparse_categorical_accuracy: 0.6082 - val_loss: 1.5473 - val_sparse_categorical_accuracy: 0.5521\n",
            "Epoch 20/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 1.0837 - sparse_categorical_accuracy: 0.6124 - val_loss: 1.6072 - val_sparse_categorical_accuracy: 0.5613\n",
            "Epoch 21/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 1.0422 - sparse_categorical_accuracy: 0.6265 - val_loss: 1.6882 - val_sparse_categorical_accuracy: 0.5529\n",
            "Epoch 22/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.0426 - sparse_categorical_accuracy: 0.6264 - val_loss: 1.6231 - val_sparse_categorical_accuracy: 0.5548\n",
            "Epoch 23/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.0170 - sparse_categorical_accuracy: 0.6367 - val_loss: 1.5478 - val_sparse_categorical_accuracy: 0.5688\n",
            "Epoch 24/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.0014 - sparse_categorical_accuracy: 0.6442 - val_loss: 1.6458 - val_sparse_categorical_accuracy: 0.5475\n",
            "Epoch 25/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 1.0027 - sparse_categorical_accuracy: 0.6424 - val_loss: 1.6118 - val_sparse_categorical_accuracy: 0.5725\n",
            "Epoch 26/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.9763 - sparse_categorical_accuracy: 0.6529 - val_loss: 1.6666 - val_sparse_categorical_accuracy: 0.5572\n",
            "Epoch 27/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.9543 - sparse_categorical_accuracy: 0.6591 - val_loss: 1.5970 - val_sparse_categorical_accuracy: 0.5656\n",
            "Epoch 28/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.9476 - sparse_categorical_accuracy: 0.6610 - val_loss: 1.6998 - val_sparse_categorical_accuracy: 0.5699\n",
            "Epoch 29/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.9214 - sparse_categorical_accuracy: 0.6713 - val_loss: 1.6617 - val_sparse_categorical_accuracy: 0.5678\n",
            "Epoch 30/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.9111 - sparse_categorical_accuracy: 0.6769 - val_loss: 1.7073 - val_sparse_categorical_accuracy: 0.5687\n",
            "Epoch 31/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.8925 - sparse_categorical_accuracy: 0.6819 - val_loss: 1.7125 - val_sparse_categorical_accuracy: 0.5638\n",
            "Epoch 32/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.8831 - sparse_categorical_accuracy: 0.6834 - val_loss: 1.6810 - val_sparse_categorical_accuracy: 0.5728\n",
            "Epoch 33/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.8669 - sparse_categorical_accuracy: 0.6917 - val_loss: 1.6807 - val_sparse_categorical_accuracy: 0.5774\n",
            "Epoch 34/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.8434 - sparse_categorical_accuracy: 0.6987 - val_loss: 1.7643 - val_sparse_categorical_accuracy: 0.5748\n",
            "Epoch 35/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.8259 - sparse_categorical_accuracy: 0.7042 - val_loss: 1.6921 - val_sparse_categorical_accuracy: 0.5789\n",
            "Epoch 36/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.8146 - sparse_categorical_accuracy: 0.7107 - val_loss: 1.8525 - val_sparse_categorical_accuracy: 0.5699\n",
            "Epoch 37/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.8126 - sparse_categorical_accuracy: 0.7081 - val_loss: 1.8329 - val_sparse_categorical_accuracy: 0.5717\n",
            "Epoch 38/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.7928 - sparse_categorical_accuracy: 0.7142 - val_loss: 1.7909 - val_sparse_categorical_accuracy: 0.5706\n",
            "Epoch 39/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.7761 - sparse_categorical_accuracy: 0.7226 - val_loss: 1.7639 - val_sparse_categorical_accuracy: 0.5785\n",
            "Epoch 40/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.7538 - sparse_categorical_accuracy: 0.7275 - val_loss: 1.9360 - val_sparse_categorical_accuracy: 0.5725\n",
            "Epoch 41/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.7523 - sparse_categorical_accuracy: 0.7306 - val_loss: 1.8829 - val_sparse_categorical_accuracy: 0.5771\n",
            "Epoch 42/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.7281 - sparse_categorical_accuracy: 0.7365 - val_loss: 1.9272 - val_sparse_categorical_accuracy: 0.5704\n",
            "Epoch 43/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.7234 - sparse_categorical_accuracy: 0.7414 - val_loss: 2.1262 - val_sparse_categorical_accuracy: 0.5538\n",
            "Epoch 44/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.6927 - sparse_categorical_accuracy: 0.7512 - val_loss: 2.0310 - val_sparse_categorical_accuracy: 0.5639\n",
            "Epoch 45/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.6877 - sparse_categorical_accuracy: 0.7511 - val_loss: 2.1367 - val_sparse_categorical_accuracy: 0.5579\n",
            "Epoch 46/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.6769 - sparse_categorical_accuracy: 0.7572 - val_loss: 2.0906 - val_sparse_categorical_accuracy: 0.5728\n",
            "Epoch 47/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.6623 - sparse_categorical_accuracy: 0.7603 - val_loss: 2.2052 - val_sparse_categorical_accuracy: 0.5566\n",
            "Epoch 48/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.6488 - sparse_categorical_accuracy: 0.7677 - val_loss: 2.1866 - val_sparse_categorical_accuracy: 0.5625\n",
            "Epoch 49/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.6209 - sparse_categorical_accuracy: 0.7741 - val_loss: 2.1628 - val_sparse_categorical_accuracy: 0.5570\n",
            "Epoch 50/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.6056 - sparse_categorical_accuracy: 0.7790 - val_loss: 2.3755 - val_sparse_categorical_accuracy: 0.5506\n",
            "Epoch 51/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.5973 - sparse_categorical_accuracy: 0.7834 - val_loss: 2.3281 - val_sparse_categorical_accuracy: 0.5571\n",
            "Epoch 52/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.5834 - sparse_categorical_accuracy: 0.7896 - val_loss: 2.2889 - val_sparse_categorical_accuracy: 0.5561\n",
            "Epoch 53/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.5720 - sparse_categorical_accuracy: 0.7904 - val_loss: 2.4892 - val_sparse_categorical_accuracy: 0.5400\n",
            "Epoch 54/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.5686 - sparse_categorical_accuracy: 0.7921 - val_loss: 2.4136 - val_sparse_categorical_accuracy: 0.5491\n",
            "Epoch 55/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.5383 - sparse_categorical_accuracy: 0.8066 - val_loss: 2.5261 - val_sparse_categorical_accuracy: 0.5481\n",
            "Epoch 56/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.5400 - sparse_categorical_accuracy: 0.8047 - val_loss: 2.4862 - val_sparse_categorical_accuracy: 0.5523\n",
            "Epoch 57/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.5182 - sparse_categorical_accuracy: 0.8113 - val_loss: 2.6930 - val_sparse_categorical_accuracy: 0.5521\n",
            "Epoch 58/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.5122 - sparse_categorical_accuracy: 0.8125 - val_loss: 2.6935 - val_sparse_categorical_accuracy: 0.5489\n",
            "Epoch 59/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.5019 - sparse_categorical_accuracy: 0.8150 - val_loss: 2.7225 - val_sparse_categorical_accuracy: 0.5521\n",
            "Epoch 60/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.4761 - sparse_categorical_accuracy: 0.8282 - val_loss: 2.8518 - val_sparse_categorical_accuracy: 0.5418\n",
            "Epoch 61/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.4868 - sparse_categorical_accuracy: 0.8240 - val_loss: 2.8376 - val_sparse_categorical_accuracy: 0.5437\n",
            "Epoch 62/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.4598 - sparse_categorical_accuracy: 0.8340 - val_loss: 2.8365 - val_sparse_categorical_accuracy: 0.5483\n",
            "Epoch 63/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.4453 - sparse_categorical_accuracy: 0.8374 - val_loss: 2.9583 - val_sparse_categorical_accuracy: 0.5450\n",
            "Epoch 64/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.4531 - sparse_categorical_accuracy: 0.8357 - val_loss: 3.0521 - val_sparse_categorical_accuracy: 0.5375\n",
            "Epoch 65/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.4222 - sparse_categorical_accuracy: 0.8454 - val_loss: 3.0224 - val_sparse_categorical_accuracy: 0.5472\n",
            "Epoch 66/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.4190 - sparse_categorical_accuracy: 0.8466 - val_loss: 3.1058 - val_sparse_categorical_accuracy: 0.5359\n",
            "Epoch 67/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.4118 - sparse_categorical_accuracy: 0.8514 - val_loss: 3.1134 - val_sparse_categorical_accuracy: 0.5491\n",
            "Epoch 68/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.3971 - sparse_categorical_accuracy: 0.8561 - val_loss: 3.2587 - val_sparse_categorical_accuracy: 0.5465\n",
            "Epoch 69/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.4012 - sparse_categorical_accuracy: 0.8501 - val_loss: 3.2184 - val_sparse_categorical_accuracy: 0.5411\n",
            "Epoch 70/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.3745 - sparse_categorical_accuracy: 0.8643 - val_loss: 3.2356 - val_sparse_categorical_accuracy: 0.5446\n",
            "Epoch 71/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.3813 - sparse_categorical_accuracy: 0.8598 - val_loss: 3.4093 - val_sparse_categorical_accuracy: 0.5398\n",
            "Epoch 72/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.3654 - sparse_categorical_accuracy: 0.8659 - val_loss: 3.3446 - val_sparse_categorical_accuracy: 0.5468\n",
            "Epoch 73/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.3581 - sparse_categorical_accuracy: 0.8695 - val_loss: 3.4077 - val_sparse_categorical_accuracy: 0.5470\n",
            "Epoch 74/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.3520 - sparse_categorical_accuracy: 0.8719 - val_loss: 3.5150 - val_sparse_categorical_accuracy: 0.5425\n",
            "Epoch 75/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.3497 - sparse_categorical_accuracy: 0.8725 - val_loss: 3.5990 - val_sparse_categorical_accuracy: 0.5335\n",
            "Epoch 76/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.3369 - sparse_categorical_accuracy: 0.8760 - val_loss: 3.6401 - val_sparse_categorical_accuracy: 0.5449\n",
            "Epoch 77/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.3300 - sparse_categorical_accuracy: 0.8785 - val_loss: 3.7516 - val_sparse_categorical_accuracy: 0.5337\n",
            "Epoch 78/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.3328 - sparse_categorical_accuracy: 0.8769 - val_loss: 3.7977 - val_sparse_categorical_accuracy: 0.5255\n",
            "Epoch 79/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.3151 - sparse_categorical_accuracy: 0.8866 - val_loss: 3.8574 - val_sparse_categorical_accuracy: 0.5265\n",
            "Epoch 80/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.2952 - sparse_categorical_accuracy: 0.8916 - val_loss: 3.9075 - val_sparse_categorical_accuracy: 0.5394\n",
            "Epoch 81/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.3106 - sparse_categorical_accuracy: 0.8858 - val_loss: 3.8654 - val_sparse_categorical_accuracy: 0.5321\n",
            "Epoch 82/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2893 - sparse_categorical_accuracy: 0.8933 - val_loss: 4.0493 - val_sparse_categorical_accuracy: 0.5364\n",
            "Epoch 83/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2936 - sparse_categorical_accuracy: 0.8924 - val_loss: 3.9454 - val_sparse_categorical_accuracy: 0.5339\n",
            "Epoch 84/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2846 - sparse_categorical_accuracy: 0.8950 - val_loss: 4.0478 - val_sparse_categorical_accuracy: 0.5401\n",
            "Epoch 85/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2925 - sparse_categorical_accuracy: 0.8911 - val_loss: 4.1531 - val_sparse_categorical_accuracy: 0.5339\n",
            "Epoch 86/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2856 - sparse_categorical_accuracy: 0.8947 - val_loss: 4.1489 - val_sparse_categorical_accuracy: 0.5297\n",
            "Epoch 87/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2789 - sparse_categorical_accuracy: 0.8990 - val_loss: 4.0465 - val_sparse_categorical_accuracy: 0.5378\n",
            "Epoch 88/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2584 - sparse_categorical_accuracy: 0.9051 - val_loss: 4.2586 - val_sparse_categorical_accuracy: 0.5309\n",
            "Epoch 89/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2571 - sparse_categorical_accuracy: 0.9060 - val_loss: 4.2712 - val_sparse_categorical_accuracy: 0.5345\n",
            "Epoch 90/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.2578 - sparse_categorical_accuracy: 0.9050 - val_loss: 4.3418 - val_sparse_categorical_accuracy: 0.5238\n",
            "Epoch 91/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2556 - sparse_categorical_accuracy: 0.9069 - val_loss: 4.4774 - val_sparse_categorical_accuracy: 0.5247\n",
            "Epoch 92/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2456 - sparse_categorical_accuracy: 0.9097 - val_loss: 4.3382 - val_sparse_categorical_accuracy: 0.5312\n",
            "Epoch 93/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2415 - sparse_categorical_accuracy: 0.9115 - val_loss: 4.4724 - val_sparse_categorical_accuracy: 0.5347\n",
            "Epoch 94/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2387 - sparse_categorical_accuracy: 0.9122 - val_loss: 4.5331 - val_sparse_categorical_accuracy: 0.5282\n",
            "Epoch 95/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2485 - sparse_categorical_accuracy: 0.9079 - val_loss: 4.5216 - val_sparse_categorical_accuracy: 0.5237\n",
            "Epoch 96/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2366 - sparse_categorical_accuracy: 0.9142 - val_loss: 4.4731 - val_sparse_categorical_accuracy: 0.5402\n",
            "Epoch 97/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2320 - sparse_categorical_accuracy: 0.9127 - val_loss: 4.4568 - val_sparse_categorical_accuracy: 0.5341\n",
            "Epoch 98/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2217 - sparse_categorical_accuracy: 0.9174 - val_loss: 4.6322 - val_sparse_categorical_accuracy: 0.5337\n",
            "Epoch 99/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2131 - sparse_categorical_accuracy: 0.9225 - val_loss: 4.8695 - val_sparse_categorical_accuracy: 0.5302\n",
            "Epoch 100/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.2314 - sparse_categorical_accuracy: 0.9156 - val_loss: 4.6712 - val_sparse_categorical_accuracy: 0.5309\n",
            "Epoch 101/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.2112 - sparse_categorical_accuracy: 0.9233 - val_loss: 4.6957 - val_sparse_categorical_accuracy: 0.5329\n",
            "Epoch 102/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.2168 - sparse_categorical_accuracy: 0.9217 - val_loss: 4.7336 - val_sparse_categorical_accuracy: 0.5354\n",
            "Epoch 103/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.2024 - sparse_categorical_accuracy: 0.9257 - val_loss: 4.8011 - val_sparse_categorical_accuracy: 0.5329\n",
            "Epoch 104/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.2198 - sparse_categorical_accuracy: 0.9197 - val_loss: 4.8667 - val_sparse_categorical_accuracy: 0.5294\n",
            "Epoch 105/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.2253 - sparse_categorical_accuracy: 0.9160 - val_loss: 4.9176 - val_sparse_categorical_accuracy: 0.5185\n",
            "Epoch 106/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.2054 - sparse_categorical_accuracy: 0.9258 - val_loss: 4.8952 - val_sparse_categorical_accuracy: 0.5344\n",
            "Epoch 107/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.2118 - sparse_categorical_accuracy: 0.9236 - val_loss: 5.0101 - val_sparse_categorical_accuracy: 0.5267\n",
            "Epoch 108/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.2055 - sparse_categorical_accuracy: 0.9254 - val_loss: 5.0446 - val_sparse_categorical_accuracy: 0.5365\n",
            "Epoch 109/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.1969 - sparse_categorical_accuracy: 0.9293 - val_loss: 5.0319 - val_sparse_categorical_accuracy: 0.5348\n",
            "Epoch 110/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.1952 - sparse_categorical_accuracy: 0.9290 - val_loss: 5.1103 - val_sparse_categorical_accuracy: 0.5340\n",
            "Epoch 111/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1909 - sparse_categorical_accuracy: 0.9292 - val_loss: 5.1072 - val_sparse_categorical_accuracy: 0.5368\n",
            "Epoch 112/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1797 - sparse_categorical_accuracy: 0.9363 - val_loss: 5.3348 - val_sparse_categorical_accuracy: 0.5273\n",
            "Epoch 113/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.1939 - sparse_categorical_accuracy: 0.9300 - val_loss: 5.2342 - val_sparse_categorical_accuracy: 0.5238\n",
            "Epoch 114/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1936 - sparse_categorical_accuracy: 0.9274 - val_loss: 5.1472 - val_sparse_categorical_accuracy: 0.5325\n",
            "Epoch 115/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1816 - sparse_categorical_accuracy: 0.9352 - val_loss: 5.1690 - val_sparse_categorical_accuracy: 0.5246\n",
            "Epoch 116/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1785 - sparse_categorical_accuracy: 0.9350 - val_loss: 5.2858 - val_sparse_categorical_accuracy: 0.5280\n",
            "Epoch 117/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1844 - sparse_categorical_accuracy: 0.9337 - val_loss: 5.1708 - val_sparse_categorical_accuracy: 0.5333\n",
            "Epoch 118/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.1925 - sparse_categorical_accuracy: 0.9304 - val_loss: 5.2771 - val_sparse_categorical_accuracy: 0.5329\n",
            "Epoch 119/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1677 - sparse_categorical_accuracy: 0.9382 - val_loss: 5.3176 - val_sparse_categorical_accuracy: 0.5275\n",
            "Epoch 120/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1869 - sparse_categorical_accuracy: 0.9338 - val_loss: 5.2197 - val_sparse_categorical_accuracy: 0.5361\n",
            "Epoch 121/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1776 - sparse_categorical_accuracy: 0.9344 - val_loss: 5.3949 - val_sparse_categorical_accuracy: 0.5387\n",
            "Epoch 122/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.1641 - sparse_categorical_accuracy: 0.9398 - val_loss: 5.2950 - val_sparse_categorical_accuracy: 0.5267\n",
            "Epoch 123/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.1609 - sparse_categorical_accuracy: 0.9422 - val_loss: 5.4320 - val_sparse_categorical_accuracy: 0.5223\n",
            "Epoch 124/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.1655 - sparse_categorical_accuracy: 0.9393 - val_loss: 5.4309 - val_sparse_categorical_accuracy: 0.5273\n",
            "Epoch 125/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1729 - sparse_categorical_accuracy: 0.9370 - val_loss: 5.3599 - val_sparse_categorical_accuracy: 0.5242\n",
            "Epoch 126/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1722 - sparse_categorical_accuracy: 0.9398 - val_loss: 5.4678 - val_sparse_categorical_accuracy: 0.5271\n",
            "Epoch 127/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1815 - sparse_categorical_accuracy: 0.9360 - val_loss: 5.5012 - val_sparse_categorical_accuracy: 0.5251\n",
            "Epoch 128/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1563 - sparse_categorical_accuracy: 0.9435 - val_loss: 5.4902 - val_sparse_categorical_accuracy: 0.5227\n",
            "Epoch 129/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1596 - sparse_categorical_accuracy: 0.9432 - val_loss: 5.6129 - val_sparse_categorical_accuracy: 0.5219\n",
            "Epoch 130/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.1567 - sparse_categorical_accuracy: 0.9427 - val_loss: 5.5679 - val_sparse_categorical_accuracy: 0.5279\n",
            "Epoch 131/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.1618 - sparse_categorical_accuracy: 0.9418 - val_loss: 5.5205 - val_sparse_categorical_accuracy: 0.5300\n",
            "Epoch 132/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.1608 - sparse_categorical_accuracy: 0.9416 - val_loss: 5.6595 - val_sparse_categorical_accuracy: 0.5226\n",
            "Epoch 133/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1757 - sparse_categorical_accuracy: 0.9359 - val_loss: 5.7206 - val_sparse_categorical_accuracy: 0.5233\n",
            "Epoch 134/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1546 - sparse_categorical_accuracy: 0.9460 - val_loss: 5.5393 - val_sparse_categorical_accuracy: 0.5251\n",
            "Epoch 135/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.1519 - sparse_categorical_accuracy: 0.9449 - val_loss: 5.8235 - val_sparse_categorical_accuracy: 0.5210\n",
            "Epoch 136/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.1577 - sparse_categorical_accuracy: 0.9421 - val_loss: 5.6697 - val_sparse_categorical_accuracy: 0.5337\n",
            "Epoch 137/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1423 - sparse_categorical_accuracy: 0.9497 - val_loss: 5.7538 - val_sparse_categorical_accuracy: 0.5313\n",
            "Epoch 138/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.1380 - sparse_categorical_accuracy: 0.9499 - val_loss: 5.8112 - val_sparse_categorical_accuracy: 0.5273\n",
            "Epoch 139/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1631 - sparse_categorical_accuracy: 0.9421 - val_loss: 5.6438 - val_sparse_categorical_accuracy: 0.5373\n",
            "Epoch 140/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1380 - sparse_categorical_accuracy: 0.9498 - val_loss: 5.7846 - val_sparse_categorical_accuracy: 0.5309\n",
            "Epoch 141/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1396 - sparse_categorical_accuracy: 0.9502 - val_loss: 5.7711 - val_sparse_categorical_accuracy: 0.5349\n",
            "Epoch 142/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1470 - sparse_categorical_accuracy: 0.9458 - val_loss: 5.8009 - val_sparse_categorical_accuracy: 0.5342\n",
            "Epoch 143/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1295 - sparse_categorical_accuracy: 0.9528 - val_loss: 5.6310 - val_sparse_categorical_accuracy: 0.5397\n",
            "Epoch 144/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1421 - sparse_categorical_accuracy: 0.9491 - val_loss: 5.8867 - val_sparse_categorical_accuracy: 0.5238\n",
            "Epoch 145/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1402 - sparse_categorical_accuracy: 0.9501 - val_loss: 5.9104 - val_sparse_categorical_accuracy: 0.5281\n",
            "Epoch 146/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1446 - sparse_categorical_accuracy: 0.9478 - val_loss: 5.8683 - val_sparse_categorical_accuracy: 0.5308\n",
            "Epoch 147/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1594 - sparse_categorical_accuracy: 0.9420 - val_loss: 5.8515 - val_sparse_categorical_accuracy: 0.5253\n",
            "Epoch 148/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1385 - sparse_categorical_accuracy: 0.9508 - val_loss: 5.9235 - val_sparse_categorical_accuracy: 0.5350\n",
            "Epoch 149/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1306 - sparse_categorical_accuracy: 0.9533 - val_loss: 5.9898 - val_sparse_categorical_accuracy: 0.5293\n",
            "Epoch 150/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.1411 - sparse_categorical_accuracy: 0.9498 - val_loss: 5.8143 - val_sparse_categorical_accuracy: 0.5342\n",
            "Epoch 151/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1505 - sparse_categorical_accuracy: 0.9461 - val_loss: 5.9564 - val_sparse_categorical_accuracy: 0.5237\n",
            "Epoch 152/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1337 - sparse_categorical_accuracy: 0.9518 - val_loss: 5.9730 - val_sparse_categorical_accuracy: 0.5291\n",
            "Epoch 153/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1319 - sparse_categorical_accuracy: 0.9527 - val_loss: 5.9393 - val_sparse_categorical_accuracy: 0.5315\n",
            "Epoch 154/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1362 - sparse_categorical_accuracy: 0.9509 - val_loss: 6.1924 - val_sparse_categorical_accuracy: 0.5294\n",
            "Epoch 155/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1418 - sparse_categorical_accuracy: 0.9487 - val_loss: 6.1040 - val_sparse_categorical_accuracy: 0.5223\n",
            "Epoch 156/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1317 - sparse_categorical_accuracy: 0.9521 - val_loss: 5.9462 - val_sparse_categorical_accuracy: 0.5298\n",
            "Epoch 157/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1194 - sparse_categorical_accuracy: 0.9567 - val_loss: 6.1480 - val_sparse_categorical_accuracy: 0.5286\n",
            "Epoch 158/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1382 - sparse_categorical_accuracy: 0.9493 - val_loss: 6.0768 - val_sparse_categorical_accuracy: 0.5274\n",
            "Epoch 159/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.1301 - sparse_categorical_accuracy: 0.9536 - val_loss: 6.0232 - val_sparse_categorical_accuracy: 0.5268\n",
            "Epoch 160/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1366 - sparse_categorical_accuracy: 0.9503 - val_loss: 5.9367 - val_sparse_categorical_accuracy: 0.5308\n",
            "Epoch 161/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1345 - sparse_categorical_accuracy: 0.9511 - val_loss: 5.9531 - val_sparse_categorical_accuracy: 0.5304\n",
            "Epoch 162/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1409 - sparse_categorical_accuracy: 0.9510 - val_loss: 5.9388 - val_sparse_categorical_accuracy: 0.5312\n",
            "Epoch 163/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1326 - sparse_categorical_accuracy: 0.9522 - val_loss: 6.1215 - val_sparse_categorical_accuracy: 0.5291\n",
            "Epoch 164/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1115 - sparse_categorical_accuracy: 0.9605 - val_loss: 6.0682 - val_sparse_categorical_accuracy: 0.5298\n",
            "Epoch 165/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1226 - sparse_categorical_accuracy: 0.9560 - val_loss: 6.0017 - val_sparse_categorical_accuracy: 0.5369\n",
            "Epoch 166/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1345 - sparse_categorical_accuracy: 0.9512 - val_loss: 6.2123 - val_sparse_categorical_accuracy: 0.5276\n",
            "Epoch 167/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1374 - sparse_categorical_accuracy: 0.9507 - val_loss: 5.9973 - val_sparse_categorical_accuracy: 0.5347\n",
            "Epoch 168/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1190 - sparse_categorical_accuracy: 0.9566 - val_loss: 6.1551 - val_sparse_categorical_accuracy: 0.5247\n",
            "Epoch 169/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1321 - sparse_categorical_accuracy: 0.9524 - val_loss: 6.3774 - val_sparse_categorical_accuracy: 0.5208\n",
            "Epoch 170/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1095 - sparse_categorical_accuracy: 0.9616 - val_loss: 6.1892 - val_sparse_categorical_accuracy: 0.5356\n",
            "Epoch 171/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1223 - sparse_categorical_accuracy: 0.9557 - val_loss: 6.3907 - val_sparse_categorical_accuracy: 0.5310\n",
            "Epoch 172/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1178 - sparse_categorical_accuracy: 0.9573 - val_loss: 6.2520 - val_sparse_categorical_accuracy: 0.5258\n",
            "Epoch 173/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1292 - sparse_categorical_accuracy: 0.9531 - val_loss: 6.4187 - val_sparse_categorical_accuracy: 0.5289\n",
            "Epoch 174/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.1250 - sparse_categorical_accuracy: 0.9543 - val_loss: 6.2832 - val_sparse_categorical_accuracy: 0.5282\n",
            "Epoch 175/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.1280 - sparse_categorical_accuracy: 0.9546 - val_loss: 6.3007 - val_sparse_categorical_accuracy: 0.5273\n",
            "Epoch 176/500\n",
            "196/196 [==============================] - 41s 207ms/step - loss: 0.1197 - sparse_categorical_accuracy: 0.9555 - val_loss: 6.2543 - val_sparse_categorical_accuracy: 0.5322\n",
            "Epoch 177/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.1242 - sparse_categorical_accuracy: 0.9537 - val_loss: 6.4208 - val_sparse_categorical_accuracy: 0.5283\n",
            "Epoch 178/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.1060 - sparse_categorical_accuracy: 0.9634 - val_loss: 6.3270 - val_sparse_categorical_accuracy: 0.5295\n",
            "Epoch 179/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.1121 - sparse_categorical_accuracy: 0.9602 - val_loss: 6.2848 - val_sparse_categorical_accuracy: 0.5299\n",
            "Epoch 180/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.1265 - sparse_categorical_accuracy: 0.9560 - val_loss: 6.3268 - val_sparse_categorical_accuracy: 0.5347\n",
            "Epoch 181/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1126 - sparse_categorical_accuracy: 0.9589 - val_loss: 6.2852 - val_sparse_categorical_accuracy: 0.5326\n",
            "Epoch 182/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1147 - sparse_categorical_accuracy: 0.9589 - val_loss: 6.4671 - val_sparse_categorical_accuracy: 0.5251\n",
            "Epoch 183/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1090 - sparse_categorical_accuracy: 0.9605 - val_loss: 6.4952 - val_sparse_categorical_accuracy: 0.5219\n",
            "Epoch 184/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1010 - sparse_categorical_accuracy: 0.9644 - val_loss: 6.5261 - val_sparse_categorical_accuracy: 0.5308\n",
            "Epoch 185/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1221 - sparse_categorical_accuracy: 0.9559 - val_loss: 6.4695 - val_sparse_categorical_accuracy: 0.5277\n",
            "Epoch 186/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1213 - sparse_categorical_accuracy: 0.9556 - val_loss: 6.3602 - val_sparse_categorical_accuracy: 0.5246\n",
            "Epoch 187/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1071 - sparse_categorical_accuracy: 0.9629 - val_loss: 6.7392 - val_sparse_categorical_accuracy: 0.5203\n",
            "Epoch 188/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1052 - sparse_categorical_accuracy: 0.9619 - val_loss: 6.5941 - val_sparse_categorical_accuracy: 0.5282\n",
            "Epoch 189/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1199 - sparse_categorical_accuracy: 0.9572 - val_loss: 6.4323 - val_sparse_categorical_accuracy: 0.5270\n",
            "Epoch 190/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1115 - sparse_categorical_accuracy: 0.9609 - val_loss: 6.7981 - val_sparse_categorical_accuracy: 0.5103\n",
            "Epoch 191/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1099 - sparse_categorical_accuracy: 0.9618 - val_loss: 6.5325 - val_sparse_categorical_accuracy: 0.5232\n",
            "Epoch 192/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1287 - sparse_categorical_accuracy: 0.9545 - val_loss: 6.5932 - val_sparse_categorical_accuracy: 0.5302\n",
            "Epoch 193/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1105 - sparse_categorical_accuracy: 0.9621 - val_loss: 6.4562 - val_sparse_categorical_accuracy: 0.5285\n",
            "Epoch 194/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1023 - sparse_categorical_accuracy: 0.9630 - val_loss: 6.6172 - val_sparse_categorical_accuracy: 0.5220\n",
            "Epoch 195/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1024 - sparse_categorical_accuracy: 0.9630 - val_loss: 6.5407 - val_sparse_categorical_accuracy: 0.5239\n",
            "Epoch 196/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1005 - sparse_categorical_accuracy: 0.9652 - val_loss: 6.7524 - val_sparse_categorical_accuracy: 0.5243\n",
            "Epoch 197/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1045 - sparse_categorical_accuracy: 0.9627 - val_loss: 6.5038 - val_sparse_categorical_accuracy: 0.5231\n",
            "Epoch 198/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0989 - sparse_categorical_accuracy: 0.9633 - val_loss: 6.7488 - val_sparse_categorical_accuracy: 0.5195\n",
            "Epoch 199/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1071 - sparse_categorical_accuracy: 0.9611 - val_loss: 6.7285 - val_sparse_categorical_accuracy: 0.5281\n",
            "Epoch 200/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1083 - sparse_categorical_accuracy: 0.9624 - val_loss: 6.7704 - val_sparse_categorical_accuracy: 0.5312\n",
            "Epoch 201/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1159 - sparse_categorical_accuracy: 0.9607 - val_loss: 6.7045 - val_sparse_categorical_accuracy: 0.5247\n",
            "Epoch 202/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1032 - sparse_categorical_accuracy: 0.9628 - val_loss: 6.5969 - val_sparse_categorical_accuracy: 0.5291\n",
            "Epoch 203/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0924 - sparse_categorical_accuracy: 0.9664 - val_loss: 6.8548 - val_sparse_categorical_accuracy: 0.5238\n",
            "Epoch 204/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1077 - sparse_categorical_accuracy: 0.9615 - val_loss: 6.7474 - val_sparse_categorical_accuracy: 0.5313\n",
            "Epoch 205/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0992 - sparse_categorical_accuracy: 0.9646 - val_loss: 6.7407 - val_sparse_categorical_accuracy: 0.5235\n",
            "Epoch 206/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0909 - sparse_categorical_accuracy: 0.9675 - val_loss: 6.7480 - val_sparse_categorical_accuracy: 0.5270\n",
            "Epoch 207/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0932 - sparse_categorical_accuracy: 0.9675 - val_loss: 6.8722 - val_sparse_categorical_accuracy: 0.5263\n",
            "Epoch 208/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1124 - sparse_categorical_accuracy: 0.9604 - val_loss: 6.7056 - val_sparse_categorical_accuracy: 0.5313\n",
            "Epoch 209/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0974 - sparse_categorical_accuracy: 0.9658 - val_loss: 6.9272 - val_sparse_categorical_accuracy: 0.5203\n",
            "Epoch 210/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1021 - sparse_categorical_accuracy: 0.9637 - val_loss: 6.8751 - val_sparse_categorical_accuracy: 0.5256\n",
            "Epoch 211/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1055 - sparse_categorical_accuracy: 0.9629 - val_loss: 6.8145 - val_sparse_categorical_accuracy: 0.5228\n",
            "Epoch 212/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1026 - sparse_categorical_accuracy: 0.9632 - val_loss: 6.8545 - val_sparse_categorical_accuracy: 0.5258\n",
            "Epoch 213/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1014 - sparse_categorical_accuracy: 0.9645 - val_loss: 6.8088 - val_sparse_categorical_accuracy: 0.5230\n",
            "Epoch 214/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0908 - sparse_categorical_accuracy: 0.9687 - val_loss: 6.8170 - val_sparse_categorical_accuracy: 0.5303\n",
            "Epoch 215/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1028 - sparse_categorical_accuracy: 0.9629 - val_loss: 6.8329 - val_sparse_categorical_accuracy: 0.5269\n",
            "Epoch 216/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0955 - sparse_categorical_accuracy: 0.9666 - val_loss: 7.0045 - val_sparse_categorical_accuracy: 0.5180\n",
            "Epoch 217/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0907 - sparse_categorical_accuracy: 0.9675 - val_loss: 6.8990 - val_sparse_categorical_accuracy: 0.5233\n",
            "Epoch 218/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0911 - sparse_categorical_accuracy: 0.9676 - val_loss: 6.9423 - val_sparse_categorical_accuracy: 0.5264\n",
            "Epoch 219/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0903 - sparse_categorical_accuracy: 0.9672 - val_loss: 6.8804 - val_sparse_categorical_accuracy: 0.5327\n",
            "Epoch 220/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0987 - sparse_categorical_accuracy: 0.9650 - val_loss: 7.0602 - val_sparse_categorical_accuracy: 0.5229\n",
            "Epoch 221/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1029 - sparse_categorical_accuracy: 0.9644 - val_loss: 6.8730 - val_sparse_categorical_accuracy: 0.5268\n",
            "Epoch 222/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.1089 - sparse_categorical_accuracy: 0.9607 - val_loss: 7.0830 - val_sparse_categorical_accuracy: 0.5222\n",
            "Epoch 223/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1005 - sparse_categorical_accuracy: 0.9640 - val_loss: 6.9517 - val_sparse_categorical_accuracy: 0.5246\n",
            "Epoch 224/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0849 - sparse_categorical_accuracy: 0.9693 - val_loss: 7.0955 - val_sparse_categorical_accuracy: 0.5218\n",
            "Epoch 225/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0932 - sparse_categorical_accuracy: 0.9678 - val_loss: 7.0942 - val_sparse_categorical_accuracy: 0.5205\n",
            "Epoch 226/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0970 - sparse_categorical_accuracy: 0.9646 - val_loss: 7.1317 - val_sparse_categorical_accuracy: 0.5191\n",
            "Epoch 227/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0825 - sparse_categorical_accuracy: 0.9706 - val_loss: 7.1108 - val_sparse_categorical_accuracy: 0.5171\n",
            "Epoch 228/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.1022 - sparse_categorical_accuracy: 0.9626 - val_loss: 7.0994 - val_sparse_categorical_accuracy: 0.5271\n",
            "Epoch 229/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0878 - sparse_categorical_accuracy: 0.9683 - val_loss: 6.8571 - val_sparse_categorical_accuracy: 0.5344\n",
            "Epoch 230/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0836 - sparse_categorical_accuracy: 0.9694 - val_loss: 7.0580 - val_sparse_categorical_accuracy: 0.5267\n",
            "Epoch 231/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0957 - sparse_categorical_accuracy: 0.9666 - val_loss: 6.9923 - val_sparse_categorical_accuracy: 0.5244\n",
            "Epoch 232/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0976 - sparse_categorical_accuracy: 0.9658 - val_loss: 7.0990 - val_sparse_categorical_accuracy: 0.5234\n",
            "Epoch 233/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0950 - sparse_categorical_accuracy: 0.9661 - val_loss: 7.1290 - val_sparse_categorical_accuracy: 0.5273\n",
            "Epoch 234/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0913 - sparse_categorical_accuracy: 0.9690 - val_loss: 7.0247 - val_sparse_categorical_accuracy: 0.5300\n",
            "Epoch 235/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0868 - sparse_categorical_accuracy: 0.9692 - val_loss: 7.1894 - val_sparse_categorical_accuracy: 0.5192\n",
            "Epoch 236/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0897 - sparse_categorical_accuracy: 0.9684 - val_loss: 7.3302 - val_sparse_categorical_accuracy: 0.5125\n",
            "Epoch 237/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0941 - sparse_categorical_accuracy: 0.9666 - val_loss: 7.2315 - val_sparse_categorical_accuracy: 0.5211\n",
            "Epoch 238/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.1111 - sparse_categorical_accuracy: 0.9610 - val_loss: 6.9260 - val_sparse_categorical_accuracy: 0.5364\n",
            "Epoch 239/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0860 - sparse_categorical_accuracy: 0.9685 - val_loss: 7.1975 - val_sparse_categorical_accuracy: 0.5224\n",
            "Epoch 240/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0959 - sparse_categorical_accuracy: 0.9654 - val_loss: 7.1398 - val_sparse_categorical_accuracy: 0.5282\n",
            "Epoch 241/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0829 - sparse_categorical_accuracy: 0.9708 - val_loss: 7.1058 - val_sparse_categorical_accuracy: 0.5276\n",
            "Epoch 242/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0934 - sparse_categorical_accuracy: 0.9687 - val_loss: 7.2137 - val_sparse_categorical_accuracy: 0.5256\n",
            "Epoch 243/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0856 - sparse_categorical_accuracy: 0.9707 - val_loss: 7.0801 - val_sparse_categorical_accuracy: 0.5320\n",
            "Epoch 244/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0903 - sparse_categorical_accuracy: 0.9686 - val_loss: 7.3121 - val_sparse_categorical_accuracy: 0.5191\n",
            "Epoch 245/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0937 - sparse_categorical_accuracy: 0.9680 - val_loss: 7.2291 - val_sparse_categorical_accuracy: 0.5217\n",
            "Epoch 246/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0925 - sparse_categorical_accuracy: 0.9674 - val_loss: 7.1096 - val_sparse_categorical_accuracy: 0.5292\n",
            "Epoch 247/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0761 - sparse_categorical_accuracy: 0.9727 - val_loss: 7.3664 - val_sparse_categorical_accuracy: 0.5267\n",
            "Epoch 248/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0794 - sparse_categorical_accuracy: 0.9728 - val_loss: 7.3586 - val_sparse_categorical_accuracy: 0.5210\n",
            "Epoch 249/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0932 - sparse_categorical_accuracy: 0.9674 - val_loss: 7.3866 - val_sparse_categorical_accuracy: 0.5231\n",
            "Epoch 250/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0906 - sparse_categorical_accuracy: 0.9671 - val_loss: 7.3267 - val_sparse_categorical_accuracy: 0.5259\n",
            "Epoch 251/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0954 - sparse_categorical_accuracy: 0.9667 - val_loss: 7.1690 - val_sparse_categorical_accuracy: 0.5302\n",
            "Epoch 252/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0809 - sparse_categorical_accuracy: 0.9715 - val_loss: 7.3780 - val_sparse_categorical_accuracy: 0.5210\n",
            "Epoch 253/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0771 - sparse_categorical_accuracy: 0.9718 - val_loss: 7.2246 - val_sparse_categorical_accuracy: 0.5309\n",
            "Epoch 254/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0807 - sparse_categorical_accuracy: 0.9713 - val_loss: 7.4168 - val_sparse_categorical_accuracy: 0.5224\n",
            "Epoch 255/500\n",
            "196/196 [==============================] - 41s 207ms/step - loss: 0.0778 - sparse_categorical_accuracy: 0.9717 - val_loss: 7.3717 - val_sparse_categorical_accuracy: 0.5243\n",
            "Epoch 256/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0796 - sparse_categorical_accuracy: 0.9718 - val_loss: 7.4067 - val_sparse_categorical_accuracy: 0.5220\n",
            "Epoch 257/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0993 - sparse_categorical_accuracy: 0.9651 - val_loss: 7.1283 - val_sparse_categorical_accuracy: 0.5281\n",
            "Epoch 258/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0798 - sparse_categorical_accuracy: 0.9718 - val_loss: 7.1524 - val_sparse_categorical_accuracy: 0.5264\n",
            "Epoch 259/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0942 - sparse_categorical_accuracy: 0.9668 - val_loss: 7.2309 - val_sparse_categorical_accuracy: 0.5217\n",
            "Epoch 260/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0733 - sparse_categorical_accuracy: 0.9744 - val_loss: 7.4515 - val_sparse_categorical_accuracy: 0.5260\n",
            "Epoch 261/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0785 - sparse_categorical_accuracy: 0.9721 - val_loss: 7.5365 - val_sparse_categorical_accuracy: 0.5256\n",
            "Epoch 262/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0917 - sparse_categorical_accuracy: 0.9665 - val_loss: 7.4281 - val_sparse_categorical_accuracy: 0.5275\n",
            "Epoch 263/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.1058 - sparse_categorical_accuracy: 0.9637 - val_loss: 7.3189 - val_sparse_categorical_accuracy: 0.5298\n",
            "Epoch 264/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0715 - sparse_categorical_accuracy: 0.9753 - val_loss: 7.4537 - val_sparse_categorical_accuracy: 0.5214\n",
            "Epoch 265/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0774 - sparse_categorical_accuracy: 0.9728 - val_loss: 7.4264 - val_sparse_categorical_accuracy: 0.5277\n",
            "Epoch 266/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0845 - sparse_categorical_accuracy: 0.9705 - val_loss: 7.3634 - val_sparse_categorical_accuracy: 0.5264\n",
            "Epoch 267/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0901 - sparse_categorical_accuracy: 0.9679 - val_loss: 7.3924 - val_sparse_categorical_accuracy: 0.5272\n",
            "Epoch 268/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0822 - sparse_categorical_accuracy: 0.9708 - val_loss: 7.3201 - val_sparse_categorical_accuracy: 0.5323\n",
            "Epoch 269/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0813 - sparse_categorical_accuracy: 0.9719 - val_loss: 7.2825 - val_sparse_categorical_accuracy: 0.5237\n",
            "Epoch 270/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0763 - sparse_categorical_accuracy: 0.9727 - val_loss: 7.3086 - val_sparse_categorical_accuracy: 0.5251\n",
            "Epoch 271/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0723 - sparse_categorical_accuracy: 0.9745 - val_loss: 7.4894 - val_sparse_categorical_accuracy: 0.5286\n",
            "Epoch 272/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0775 - sparse_categorical_accuracy: 0.9731 - val_loss: 7.5679 - val_sparse_categorical_accuracy: 0.5255\n",
            "Epoch 273/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0939 - sparse_categorical_accuracy: 0.9674 - val_loss: 7.2292 - val_sparse_categorical_accuracy: 0.5281\n",
            "Epoch 274/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0860 - sparse_categorical_accuracy: 0.9692 - val_loss: 7.3012 - val_sparse_categorical_accuracy: 0.5313\n",
            "Epoch 275/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0736 - sparse_categorical_accuracy: 0.9750 - val_loss: 7.5647 - val_sparse_categorical_accuracy: 0.5226\n",
            "Epoch 276/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0721 - sparse_categorical_accuracy: 0.9744 - val_loss: 7.5114 - val_sparse_categorical_accuracy: 0.5287\n",
            "Epoch 277/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0759 - sparse_categorical_accuracy: 0.9738 - val_loss: 7.4484 - val_sparse_categorical_accuracy: 0.5284\n",
            "Epoch 278/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0861 - sparse_categorical_accuracy: 0.9705 - val_loss: 7.4531 - val_sparse_categorical_accuracy: 0.5316\n",
            "Epoch 279/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0797 - sparse_categorical_accuracy: 0.9722 - val_loss: 7.4848 - val_sparse_categorical_accuracy: 0.5302\n",
            "Epoch 280/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0724 - sparse_categorical_accuracy: 0.9745 - val_loss: 7.4820 - val_sparse_categorical_accuracy: 0.5200\n",
            "Epoch 281/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0803 - sparse_categorical_accuracy: 0.9723 - val_loss: 7.4092 - val_sparse_categorical_accuracy: 0.5272\n",
            "Epoch 282/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0806 - sparse_categorical_accuracy: 0.9716 - val_loss: 7.6282 - val_sparse_categorical_accuracy: 0.5231\n",
            "Epoch 283/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0803 - sparse_categorical_accuracy: 0.9717 - val_loss: 7.6816 - val_sparse_categorical_accuracy: 0.5198\n",
            "Epoch 284/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0787 - sparse_categorical_accuracy: 0.9728 - val_loss: 7.4399 - val_sparse_categorical_accuracy: 0.5348\n",
            "Epoch 285/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0689 - sparse_categorical_accuracy: 0.9751 - val_loss: 7.4249 - val_sparse_categorical_accuracy: 0.5300\n",
            "Epoch 286/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0852 - sparse_categorical_accuracy: 0.9711 - val_loss: 7.5862 - val_sparse_categorical_accuracy: 0.5214\n",
            "Epoch 287/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0767 - sparse_categorical_accuracy: 0.9726 - val_loss: 7.4296 - val_sparse_categorical_accuracy: 0.5243\n",
            "Epoch 288/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0931 - sparse_categorical_accuracy: 0.9682 - val_loss: 7.5532 - val_sparse_categorical_accuracy: 0.5192\n",
            "Epoch 289/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0815 - sparse_categorical_accuracy: 0.9719 - val_loss: 7.3958 - val_sparse_categorical_accuracy: 0.5303\n",
            "Epoch 290/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0628 - sparse_categorical_accuracy: 0.9775 - val_loss: 7.3639 - val_sparse_categorical_accuracy: 0.5291\n",
            "Epoch 291/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0836 - sparse_categorical_accuracy: 0.9698 - val_loss: 7.4729 - val_sparse_categorical_accuracy: 0.5252\n",
            "Epoch 292/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0672 - sparse_categorical_accuracy: 0.9756 - val_loss: 7.6719 - val_sparse_categorical_accuracy: 0.5232\n",
            "Epoch 293/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0840 - sparse_categorical_accuracy: 0.9707 - val_loss: 7.4704 - val_sparse_categorical_accuracy: 0.5292\n",
            "Epoch 294/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0798 - sparse_categorical_accuracy: 0.9734 - val_loss: 7.3879 - val_sparse_categorical_accuracy: 0.5327\n",
            "Epoch 295/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0847 - sparse_categorical_accuracy: 0.9706 - val_loss: 7.6488 - val_sparse_categorical_accuracy: 0.5255\n",
            "Epoch 296/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0772 - sparse_categorical_accuracy: 0.9724 - val_loss: 7.6754 - val_sparse_categorical_accuracy: 0.5216\n",
            "Epoch 297/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0778 - sparse_categorical_accuracy: 0.9729 - val_loss: 7.7417 - val_sparse_categorical_accuracy: 0.5221\n",
            "Epoch 298/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0793 - sparse_categorical_accuracy: 0.9733 - val_loss: 7.5746 - val_sparse_categorical_accuracy: 0.5304\n",
            "Epoch 299/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0684 - sparse_categorical_accuracy: 0.9767 - val_loss: 7.6657 - val_sparse_categorical_accuracy: 0.5258\n",
            "Epoch 300/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0747 - sparse_categorical_accuracy: 0.9730 - val_loss: 7.5032 - val_sparse_categorical_accuracy: 0.5241\n",
            "Epoch 301/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0806 - sparse_categorical_accuracy: 0.9709 - val_loss: 7.6816 - val_sparse_categorical_accuracy: 0.5222\n",
            "Epoch 302/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0889 - sparse_categorical_accuracy: 0.9683 - val_loss: 7.6454 - val_sparse_categorical_accuracy: 0.5259\n",
            "Epoch 303/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0765 - sparse_categorical_accuracy: 0.9738 - val_loss: 7.7083 - val_sparse_categorical_accuracy: 0.5234\n",
            "Epoch 304/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0698 - sparse_categorical_accuracy: 0.9761 - val_loss: 7.6479 - val_sparse_categorical_accuracy: 0.5273\n",
            "Epoch 305/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0847 - sparse_categorical_accuracy: 0.9708 - val_loss: 7.4821 - val_sparse_categorical_accuracy: 0.5280\n",
            "Epoch 306/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0806 - sparse_categorical_accuracy: 0.9713 - val_loss: 7.9143 - val_sparse_categorical_accuracy: 0.5217\n",
            "Epoch 307/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0651 - sparse_categorical_accuracy: 0.9776 - val_loss: 7.6203 - val_sparse_categorical_accuracy: 0.5317\n",
            "Epoch 308/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0693 - sparse_categorical_accuracy: 0.9756 - val_loss: 7.8493 - val_sparse_categorical_accuracy: 0.5240\n",
            "Epoch 309/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0700 - sparse_categorical_accuracy: 0.9755 - val_loss: 7.6618 - val_sparse_categorical_accuracy: 0.5326\n",
            "Epoch 310/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0715 - sparse_categorical_accuracy: 0.9744 - val_loss: 7.8354 - val_sparse_categorical_accuracy: 0.5255\n",
            "Epoch 311/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0711 - sparse_categorical_accuracy: 0.9756 - val_loss: 7.8693 - val_sparse_categorical_accuracy: 0.5249\n",
            "Epoch 312/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0770 - sparse_categorical_accuracy: 0.9727 - val_loss: 7.6642 - val_sparse_categorical_accuracy: 0.5296\n",
            "Epoch 313/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0875 - sparse_categorical_accuracy: 0.9699 - val_loss: 7.7036 - val_sparse_categorical_accuracy: 0.5216\n",
            "Epoch 314/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0775 - sparse_categorical_accuracy: 0.9727 - val_loss: 7.6006 - val_sparse_categorical_accuracy: 0.5267\n",
            "Epoch 315/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0789 - sparse_categorical_accuracy: 0.9717 - val_loss: 7.8968 - val_sparse_categorical_accuracy: 0.5205\n",
            "Epoch 316/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0690 - sparse_categorical_accuracy: 0.9760 - val_loss: 7.8131 - val_sparse_categorical_accuracy: 0.5269\n",
            "Epoch 317/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0835 - sparse_categorical_accuracy: 0.9701 - val_loss: 7.4710 - val_sparse_categorical_accuracy: 0.5299\n",
            "Epoch 318/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0602 - sparse_categorical_accuracy: 0.9798 - val_loss: 7.9125 - val_sparse_categorical_accuracy: 0.5286\n",
            "Epoch 319/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0631 - sparse_categorical_accuracy: 0.9780 - val_loss: 7.9057 - val_sparse_categorical_accuracy: 0.5232\n",
            "Epoch 320/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0618 - sparse_categorical_accuracy: 0.9787 - val_loss: 7.7270 - val_sparse_categorical_accuracy: 0.5261\n",
            "Epoch 321/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0657 - sparse_categorical_accuracy: 0.9770 - val_loss: 7.8970 - val_sparse_categorical_accuracy: 0.5293\n",
            "Epoch 322/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0699 - sparse_categorical_accuracy: 0.9749 - val_loss: 7.8176 - val_sparse_categorical_accuracy: 0.5270\n",
            "Epoch 323/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0816 - sparse_categorical_accuracy: 0.9723 - val_loss: 7.8971 - val_sparse_categorical_accuracy: 0.5274\n",
            "Epoch 324/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0680 - sparse_categorical_accuracy: 0.9753 - val_loss: 7.9901 - val_sparse_categorical_accuracy: 0.5194\n",
            "Epoch 325/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0835 - sparse_categorical_accuracy: 0.9704 - val_loss: 7.9875 - val_sparse_categorical_accuracy: 0.5203\n",
            "Epoch 326/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0682 - sparse_categorical_accuracy: 0.9757 - val_loss: 8.0329 - val_sparse_categorical_accuracy: 0.5257\n",
            "Epoch 327/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0694 - sparse_categorical_accuracy: 0.9748 - val_loss: 7.7240 - val_sparse_categorical_accuracy: 0.5338\n",
            "Epoch 328/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0752 - sparse_categorical_accuracy: 0.9734 - val_loss: 7.8650 - val_sparse_categorical_accuracy: 0.5216\n",
            "Epoch 329/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0619 - sparse_categorical_accuracy: 0.9788 - val_loss: 7.7021 - val_sparse_categorical_accuracy: 0.5349\n",
            "Epoch 330/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0655 - sparse_categorical_accuracy: 0.9774 - val_loss: 7.8971 - val_sparse_categorical_accuracy: 0.5212\n",
            "Epoch 331/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0710 - sparse_categorical_accuracy: 0.9749 - val_loss: 7.6592 - val_sparse_categorical_accuracy: 0.5295\n",
            "Epoch 332/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0715 - sparse_categorical_accuracy: 0.9752 - val_loss: 7.6517 - val_sparse_categorical_accuracy: 0.5273\n",
            "Epoch 333/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0718 - sparse_categorical_accuracy: 0.9751 - val_loss: 7.8290 - val_sparse_categorical_accuracy: 0.5209\n",
            "Epoch 334/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0771 - sparse_categorical_accuracy: 0.9741 - val_loss: 7.7694 - val_sparse_categorical_accuracy: 0.5268\n",
            "Epoch 335/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0709 - sparse_categorical_accuracy: 0.9762 - val_loss: 7.7069 - val_sparse_categorical_accuracy: 0.5280\n",
            "Epoch 336/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0655 - sparse_categorical_accuracy: 0.9774 - val_loss: 7.7004 - val_sparse_categorical_accuracy: 0.5290\n",
            "Epoch 337/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0601 - sparse_categorical_accuracy: 0.9788 - val_loss: 7.7102 - val_sparse_categorical_accuracy: 0.5287\n",
            "Epoch 338/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0702 - sparse_categorical_accuracy: 0.9753 - val_loss: 7.9434 - val_sparse_categorical_accuracy: 0.5289\n",
            "Epoch 339/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0721 - sparse_categorical_accuracy: 0.9750 - val_loss: 7.6454 - val_sparse_categorical_accuracy: 0.5360\n",
            "Epoch 340/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0645 - sparse_categorical_accuracy: 0.9775 - val_loss: 7.5868 - val_sparse_categorical_accuracy: 0.5283\n",
            "Epoch 341/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0633 - sparse_categorical_accuracy: 0.9780 - val_loss: 7.8380 - val_sparse_categorical_accuracy: 0.5204\n",
            "Epoch 342/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0601 - sparse_categorical_accuracy: 0.9795 - val_loss: 7.9689 - val_sparse_categorical_accuracy: 0.5259\n",
            "Epoch 343/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0642 - sparse_categorical_accuracy: 0.9772 - val_loss: 7.8913 - val_sparse_categorical_accuracy: 0.5267\n",
            "Epoch 344/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0704 - sparse_categorical_accuracy: 0.9750 - val_loss: 7.9899 - val_sparse_categorical_accuracy: 0.5190\n",
            "Epoch 345/500\n",
            "196/196 [==============================] - 41s 207ms/step - loss: 0.0620 - sparse_categorical_accuracy: 0.9777 - val_loss: 7.8914 - val_sparse_categorical_accuracy: 0.5218\n",
            "Epoch 346/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0701 - sparse_categorical_accuracy: 0.9751 - val_loss: 7.8516 - val_sparse_categorical_accuracy: 0.5246\n",
            "Epoch 347/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0682 - sparse_categorical_accuracy: 0.9764 - val_loss: 8.1218 - val_sparse_categorical_accuracy: 0.5187\n",
            "Epoch 348/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0645 - sparse_categorical_accuracy: 0.9772 - val_loss: 8.0517 - val_sparse_categorical_accuracy: 0.5207\n",
            "Epoch 349/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0674 - sparse_categorical_accuracy: 0.9756 - val_loss: 7.9498 - val_sparse_categorical_accuracy: 0.5211\n",
            "Epoch 350/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0672 - sparse_categorical_accuracy: 0.9767 - val_loss: 7.9994 - val_sparse_categorical_accuracy: 0.5137\n",
            "Epoch 351/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0742 - sparse_categorical_accuracy: 0.9744 - val_loss: 7.7402 - val_sparse_categorical_accuracy: 0.5209\n",
            "Epoch 352/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0596 - sparse_categorical_accuracy: 0.9792 - val_loss: 7.9975 - val_sparse_categorical_accuracy: 0.5176\n",
            "Epoch 353/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0715 - sparse_categorical_accuracy: 0.9757 - val_loss: 7.8911 - val_sparse_categorical_accuracy: 0.5182\n",
            "Epoch 354/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0597 - sparse_categorical_accuracy: 0.9788 - val_loss: 8.0714 - val_sparse_categorical_accuracy: 0.5241\n",
            "Epoch 355/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0682 - sparse_categorical_accuracy: 0.9753 - val_loss: 8.1001 - val_sparse_categorical_accuracy: 0.5171\n",
            "Epoch 356/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0710 - sparse_categorical_accuracy: 0.9743 - val_loss: 8.0330 - val_sparse_categorical_accuracy: 0.5263\n",
            "Epoch 357/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0613 - sparse_categorical_accuracy: 0.9785 - val_loss: 8.1576 - val_sparse_categorical_accuracy: 0.5185\n",
            "Epoch 358/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0724 - sparse_categorical_accuracy: 0.9757 - val_loss: 8.0747 - val_sparse_categorical_accuracy: 0.5274\n",
            "Epoch 359/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0630 - sparse_categorical_accuracy: 0.9784 - val_loss: 8.1553 - val_sparse_categorical_accuracy: 0.5185\n",
            "Epoch 360/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0601 - sparse_categorical_accuracy: 0.9789 - val_loss: 8.0582 - val_sparse_categorical_accuracy: 0.5293\n",
            "Epoch 361/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0669 - sparse_categorical_accuracy: 0.9776 - val_loss: 8.2231 - val_sparse_categorical_accuracy: 0.5156\n",
            "Epoch 362/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0678 - sparse_categorical_accuracy: 0.9762 - val_loss: 8.2486 - val_sparse_categorical_accuracy: 0.5192\n",
            "Epoch 363/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0717 - sparse_categorical_accuracy: 0.9763 - val_loss: 8.2376 - val_sparse_categorical_accuracy: 0.5218\n",
            "Epoch 364/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0578 - sparse_categorical_accuracy: 0.9797 - val_loss: 8.1044 - val_sparse_categorical_accuracy: 0.5251\n",
            "Epoch 365/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0630 - sparse_categorical_accuracy: 0.9771 - val_loss: 8.0411 - val_sparse_categorical_accuracy: 0.5222\n",
            "Epoch 366/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0648 - sparse_categorical_accuracy: 0.9784 - val_loss: 7.9682 - val_sparse_categorical_accuracy: 0.5216\n",
            "Epoch 367/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0621 - sparse_categorical_accuracy: 0.9774 - val_loss: 8.3898 - val_sparse_categorical_accuracy: 0.5122\n",
            "Epoch 368/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0738 - sparse_categorical_accuracy: 0.9745 - val_loss: 8.1248 - val_sparse_categorical_accuracy: 0.5208\n",
            "Epoch 369/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0709 - sparse_categorical_accuracy: 0.9746 - val_loss: 8.0912 - val_sparse_categorical_accuracy: 0.5216\n",
            "Epoch 370/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0618 - sparse_categorical_accuracy: 0.9786 - val_loss: 8.1134 - val_sparse_categorical_accuracy: 0.5323\n",
            "Epoch 371/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0657 - sparse_categorical_accuracy: 0.9773 - val_loss: 8.0215 - val_sparse_categorical_accuracy: 0.5247\n",
            "Epoch 372/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0686 - sparse_categorical_accuracy: 0.9751 - val_loss: 7.9556 - val_sparse_categorical_accuracy: 0.5287\n",
            "Epoch 373/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0614 - sparse_categorical_accuracy: 0.9776 - val_loss: 8.2556 - val_sparse_categorical_accuracy: 0.5248\n",
            "Epoch 374/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0613 - sparse_categorical_accuracy: 0.9781 - val_loss: 7.9570 - val_sparse_categorical_accuracy: 0.5295\n",
            "Epoch 375/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0673 - sparse_categorical_accuracy: 0.9763 - val_loss: 8.0552 - val_sparse_categorical_accuracy: 0.5276\n",
            "Epoch 376/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0664 - sparse_categorical_accuracy: 0.9760 - val_loss: 8.2508 - val_sparse_categorical_accuracy: 0.5224\n",
            "Epoch 377/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0694 - sparse_categorical_accuracy: 0.9751 - val_loss: 7.9765 - val_sparse_categorical_accuracy: 0.5263\n",
            "Epoch 378/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0541 - sparse_categorical_accuracy: 0.9806 - val_loss: 8.0884 - val_sparse_categorical_accuracy: 0.5278\n",
            "Epoch 379/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0569 - sparse_categorical_accuracy: 0.9790 - val_loss: 7.9936 - val_sparse_categorical_accuracy: 0.5292\n",
            "Epoch 380/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0686 - sparse_categorical_accuracy: 0.9765 - val_loss: 8.2116 - val_sparse_categorical_accuracy: 0.5209\n",
            "Epoch 381/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0680 - sparse_categorical_accuracy: 0.9761 - val_loss: 8.0978 - val_sparse_categorical_accuracy: 0.5277\n",
            "Epoch 382/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0668 - sparse_categorical_accuracy: 0.9759 - val_loss: 8.2132 - val_sparse_categorical_accuracy: 0.5185\n",
            "Epoch 383/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0529 - sparse_categorical_accuracy: 0.9813 - val_loss: 8.1917 - val_sparse_categorical_accuracy: 0.5184\n",
            "Epoch 384/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0584 - sparse_categorical_accuracy: 0.9797 - val_loss: 8.2633 - val_sparse_categorical_accuracy: 0.5134\n",
            "Epoch 385/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0646 - sparse_categorical_accuracy: 0.9772 - val_loss: 8.2179 - val_sparse_categorical_accuracy: 0.5188\n",
            "Epoch 386/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0619 - sparse_categorical_accuracy: 0.9788 - val_loss: 8.2319 - val_sparse_categorical_accuracy: 0.5205\n",
            "Epoch 387/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0642 - sparse_categorical_accuracy: 0.9780 - val_loss: 8.0562 - val_sparse_categorical_accuracy: 0.5258\n",
            "Epoch 388/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0572 - sparse_categorical_accuracy: 0.9802 - val_loss: 8.1187 - val_sparse_categorical_accuracy: 0.5260\n",
            "Epoch 389/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0610 - sparse_categorical_accuracy: 0.9789 - val_loss: 8.5476 - val_sparse_categorical_accuracy: 0.5119\n",
            "Epoch 390/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0626 - sparse_categorical_accuracy: 0.9785 - val_loss: 8.4868 - val_sparse_categorical_accuracy: 0.5186\n",
            "Epoch 391/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0704 - sparse_categorical_accuracy: 0.9755 - val_loss: 8.0713 - val_sparse_categorical_accuracy: 0.5271\n",
            "Epoch 392/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0590 - sparse_categorical_accuracy: 0.9786 - val_loss: 8.2922 - val_sparse_categorical_accuracy: 0.5219\n",
            "Epoch 393/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0664 - sparse_categorical_accuracy: 0.9757 - val_loss: 8.2408 - val_sparse_categorical_accuracy: 0.5229\n",
            "Epoch 394/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0589 - sparse_categorical_accuracy: 0.9791 - val_loss: 8.5340 - val_sparse_categorical_accuracy: 0.5177\n",
            "Epoch 395/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0621 - sparse_categorical_accuracy: 0.9790 - val_loss: 8.2464 - val_sparse_categorical_accuracy: 0.5161\n",
            "Epoch 396/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0605 - sparse_categorical_accuracy: 0.9799 - val_loss: 8.2937 - val_sparse_categorical_accuracy: 0.5226\n",
            "Epoch 397/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0597 - sparse_categorical_accuracy: 0.9785 - val_loss: 8.1921 - val_sparse_categorical_accuracy: 0.5230\n",
            "Epoch 398/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0563 - sparse_categorical_accuracy: 0.9797 - val_loss: 8.4364 - val_sparse_categorical_accuracy: 0.5183\n",
            "Epoch 399/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0564 - sparse_categorical_accuracy: 0.9801 - val_loss: 8.3767 - val_sparse_categorical_accuracy: 0.5220\n",
            "Epoch 400/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0498 - sparse_categorical_accuracy: 0.9814 - val_loss: 8.3524 - val_sparse_categorical_accuracy: 0.5251\n",
            "Epoch 401/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0557 - sparse_categorical_accuracy: 0.9801 - val_loss: 8.5051 - val_sparse_categorical_accuracy: 0.5097\n",
            "Epoch 402/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0633 - sparse_categorical_accuracy: 0.9783 - val_loss: 8.4436 - val_sparse_categorical_accuracy: 0.5243\n",
            "Epoch 403/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0646 - sparse_categorical_accuracy: 0.9772 - val_loss: 8.5167 - val_sparse_categorical_accuracy: 0.5236\n",
            "Epoch 404/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0647 - sparse_categorical_accuracy: 0.9782 - val_loss: 8.3406 - val_sparse_categorical_accuracy: 0.5191\n",
            "Epoch 405/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0614 - sparse_categorical_accuracy: 0.9783 - val_loss: 8.1732 - val_sparse_categorical_accuracy: 0.5259\n",
            "Epoch 406/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0529 - sparse_categorical_accuracy: 0.9814 - val_loss: 8.2096 - val_sparse_categorical_accuracy: 0.5288\n",
            "Epoch 407/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0561 - sparse_categorical_accuracy: 0.9803 - val_loss: 8.2324 - val_sparse_categorical_accuracy: 0.5272\n",
            "Epoch 408/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0580 - sparse_categorical_accuracy: 0.9794 - val_loss: 8.5481 - val_sparse_categorical_accuracy: 0.5168\n",
            "Epoch 409/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0652 - sparse_categorical_accuracy: 0.9780 - val_loss: 8.2863 - val_sparse_categorical_accuracy: 0.5261\n",
            "Epoch 410/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0580 - sparse_categorical_accuracy: 0.9792 - val_loss: 8.3775 - val_sparse_categorical_accuracy: 0.5163\n",
            "Epoch 411/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0627 - sparse_categorical_accuracy: 0.9786 - val_loss: 8.3973 - val_sparse_categorical_accuracy: 0.5192\n",
            "Epoch 412/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0616 - sparse_categorical_accuracy: 0.9781 - val_loss: 8.4257 - val_sparse_categorical_accuracy: 0.5233\n",
            "Epoch 413/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0563 - sparse_categorical_accuracy: 0.9811 - val_loss: 8.5159 - val_sparse_categorical_accuracy: 0.5092\n",
            "Epoch 414/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0626 - sparse_categorical_accuracy: 0.9784 - val_loss: 8.4550 - val_sparse_categorical_accuracy: 0.5189\n",
            "Epoch 415/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0599 - sparse_categorical_accuracy: 0.9790 - val_loss: 8.1646 - val_sparse_categorical_accuracy: 0.5327\n",
            "Epoch 416/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0502 - sparse_categorical_accuracy: 0.9823 - val_loss: 8.3185 - val_sparse_categorical_accuracy: 0.5273\n",
            "Epoch 417/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0666 - sparse_categorical_accuracy: 0.9766 - val_loss: 8.3330 - val_sparse_categorical_accuracy: 0.5278\n",
            "Epoch 418/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0517 - sparse_categorical_accuracy: 0.9816 - val_loss: 8.5119 - val_sparse_categorical_accuracy: 0.5206\n",
            "Epoch 419/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0577 - sparse_categorical_accuracy: 0.9798 - val_loss: 8.6723 - val_sparse_categorical_accuracy: 0.5148\n",
            "Epoch 420/500\n",
            "196/196 [==============================] - 41s 209ms/step - loss: 0.0545 - sparse_categorical_accuracy: 0.9811 - val_loss: 8.2830 - val_sparse_categorical_accuracy: 0.5175\n",
            "Epoch 421/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0589 - sparse_categorical_accuracy: 0.9780 - val_loss: 8.4138 - val_sparse_categorical_accuracy: 0.5224\n",
            "Epoch 422/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0633 - sparse_categorical_accuracy: 0.9787 - val_loss: 8.2121 - val_sparse_categorical_accuracy: 0.5258\n",
            "Epoch 423/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0524 - sparse_categorical_accuracy: 0.9812 - val_loss: 8.5374 - val_sparse_categorical_accuracy: 0.5233\n",
            "Epoch 424/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0553 - sparse_categorical_accuracy: 0.9811 - val_loss: 8.3239 - val_sparse_categorical_accuracy: 0.5243\n",
            "Epoch 425/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0533 - sparse_categorical_accuracy: 0.9812 - val_loss: 8.4762 - val_sparse_categorical_accuracy: 0.5194\n",
            "Epoch 426/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0595 - sparse_categorical_accuracy: 0.9801 - val_loss: 8.6934 - val_sparse_categorical_accuracy: 0.5115\n",
            "Epoch 427/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0591 - sparse_categorical_accuracy: 0.9798 - val_loss: 8.3884 - val_sparse_categorical_accuracy: 0.5290\n",
            "Epoch 428/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0515 - sparse_categorical_accuracy: 0.9813 - val_loss: 8.3905 - val_sparse_categorical_accuracy: 0.5269\n",
            "Epoch 429/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0612 - sparse_categorical_accuracy: 0.9782 - val_loss: 8.3614 - val_sparse_categorical_accuracy: 0.5184\n",
            "Epoch 430/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0632 - sparse_categorical_accuracy: 0.9778 - val_loss: 8.3951 - val_sparse_categorical_accuracy: 0.5225\n",
            "Epoch 431/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0577 - sparse_categorical_accuracy: 0.9797 - val_loss: 8.4439 - val_sparse_categorical_accuracy: 0.5248\n",
            "Epoch 432/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0586 - sparse_categorical_accuracy: 0.9805 - val_loss: 8.4052 - val_sparse_categorical_accuracy: 0.5294\n",
            "Epoch 433/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0529 - sparse_categorical_accuracy: 0.9813 - val_loss: 8.3608 - val_sparse_categorical_accuracy: 0.5249\n",
            "Epoch 434/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0637 - sparse_categorical_accuracy: 0.9776 - val_loss: 8.5281 - val_sparse_categorical_accuracy: 0.5144\n",
            "Epoch 435/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0649 - sparse_categorical_accuracy: 0.9775 - val_loss: 8.3222 - val_sparse_categorical_accuracy: 0.5279\n",
            "Epoch 436/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0551 - sparse_categorical_accuracy: 0.9809 - val_loss: 8.4117 - val_sparse_categorical_accuracy: 0.5243\n",
            "Epoch 437/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0513 - sparse_categorical_accuracy: 0.9820 - val_loss: 8.5043 - val_sparse_categorical_accuracy: 0.5273\n",
            "Epoch 438/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0622 - sparse_categorical_accuracy: 0.9772 - val_loss: 8.3710 - val_sparse_categorical_accuracy: 0.5279\n",
            "Epoch 439/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0579 - sparse_categorical_accuracy: 0.9799 - val_loss: 8.5096 - val_sparse_categorical_accuracy: 0.5176\n",
            "Epoch 440/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0635 - sparse_categorical_accuracy: 0.9787 - val_loss: 8.1786 - val_sparse_categorical_accuracy: 0.5331\n",
            "Epoch 441/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0496 - sparse_categorical_accuracy: 0.9826 - val_loss: 8.4656 - val_sparse_categorical_accuracy: 0.5252\n",
            "Epoch 442/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0557 - sparse_categorical_accuracy: 0.9804 - val_loss: 8.4941 - val_sparse_categorical_accuracy: 0.5224\n",
            "Epoch 443/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0576 - sparse_categorical_accuracy: 0.9790 - val_loss: 8.3900 - val_sparse_categorical_accuracy: 0.5248\n",
            "Epoch 444/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0587 - sparse_categorical_accuracy: 0.9806 - val_loss: 8.2948 - val_sparse_categorical_accuracy: 0.5217\n",
            "Epoch 445/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0631 - sparse_categorical_accuracy: 0.9775 - val_loss: 8.0968 - val_sparse_categorical_accuracy: 0.5331\n",
            "Epoch 446/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0491 - sparse_categorical_accuracy: 0.9834 - val_loss: 8.3523 - val_sparse_categorical_accuracy: 0.5313\n",
            "Epoch 447/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0518 - sparse_categorical_accuracy: 0.9809 - val_loss: 8.3080 - val_sparse_categorical_accuracy: 0.5178\n",
            "Epoch 448/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0500 - sparse_categorical_accuracy: 0.9820 - val_loss: 8.6648 - val_sparse_categorical_accuracy: 0.5255\n",
            "Epoch 449/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0472 - sparse_categorical_accuracy: 0.9837 - val_loss: 8.4877 - val_sparse_categorical_accuracy: 0.5265\n",
            "Epoch 450/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0621 - sparse_categorical_accuracy: 0.9784 - val_loss: 8.5485 - val_sparse_categorical_accuracy: 0.5201\n",
            "Epoch 451/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0613 - sparse_categorical_accuracy: 0.9781 - val_loss: 8.4344 - val_sparse_categorical_accuracy: 0.5227\n",
            "Epoch 452/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0552 - sparse_categorical_accuracy: 0.9811 - val_loss: 8.4139 - val_sparse_categorical_accuracy: 0.5200\n",
            "Epoch 453/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0573 - sparse_categorical_accuracy: 0.9795 - val_loss: 8.5314 - val_sparse_categorical_accuracy: 0.5162\n",
            "Epoch 454/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0590 - sparse_categorical_accuracy: 0.9800 - val_loss: 8.5028 - val_sparse_categorical_accuracy: 0.5228\n",
            "Epoch 455/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0587 - sparse_categorical_accuracy: 0.9807 - val_loss: 8.5089 - val_sparse_categorical_accuracy: 0.5255\n",
            "Epoch 456/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0562 - sparse_categorical_accuracy: 0.9801 - val_loss: 8.6187 - val_sparse_categorical_accuracy: 0.5229\n",
            "Epoch 457/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0536 - sparse_categorical_accuracy: 0.9814 - val_loss: 8.3278 - val_sparse_categorical_accuracy: 0.5262\n",
            "Epoch 458/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0455 - sparse_categorical_accuracy: 0.9833 - val_loss: 8.6384 - val_sparse_categorical_accuracy: 0.5240\n",
            "Epoch 459/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0538 - sparse_categorical_accuracy: 0.9811 - val_loss: 8.5666 - val_sparse_categorical_accuracy: 0.5217\n",
            "Epoch 460/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0550 - sparse_categorical_accuracy: 0.9802 - val_loss: 8.5317 - val_sparse_categorical_accuracy: 0.5157\n",
            "Epoch 461/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0535 - sparse_categorical_accuracy: 0.9808 - val_loss: 8.5831 - val_sparse_categorical_accuracy: 0.5208\n",
            "Epoch 462/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0504 - sparse_categorical_accuracy: 0.9822 - val_loss: 8.6315 - val_sparse_categorical_accuracy: 0.5231\n",
            "Epoch 463/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0512 - sparse_categorical_accuracy: 0.9823 - val_loss: 8.5601 - val_sparse_categorical_accuracy: 0.5275\n",
            "Epoch 464/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0571 - sparse_categorical_accuracy: 0.9806 - val_loss: 8.6350 - val_sparse_categorical_accuracy: 0.5269\n",
            "Epoch 465/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0541 - sparse_categorical_accuracy: 0.9817 - val_loss: 8.8044 - val_sparse_categorical_accuracy: 0.5170\n",
            "Epoch 466/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0583 - sparse_categorical_accuracy: 0.9794 - val_loss: 8.6869 - val_sparse_categorical_accuracy: 0.5141\n",
            "Epoch 467/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0720 - sparse_categorical_accuracy: 0.9754 - val_loss: 8.5899 - val_sparse_categorical_accuracy: 0.5065\n",
            "Epoch 468/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0529 - sparse_categorical_accuracy: 0.9818 - val_loss: 8.6004 - val_sparse_categorical_accuracy: 0.5192\n",
            "Epoch 469/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0486 - sparse_categorical_accuracy: 0.9838 - val_loss: 8.5804 - val_sparse_categorical_accuracy: 0.5183\n",
            "Epoch 470/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0445 - sparse_categorical_accuracy: 0.9839 - val_loss: 8.5361 - val_sparse_categorical_accuracy: 0.5257\n",
            "Epoch 471/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0488 - sparse_categorical_accuracy: 0.9826 - val_loss: 8.7706 - val_sparse_categorical_accuracy: 0.5158\n",
            "Epoch 472/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0554 - sparse_categorical_accuracy: 0.9802 - val_loss: 8.7818 - val_sparse_categorical_accuracy: 0.5193\n",
            "Epoch 473/500\n",
            "196/196 [==============================] - 41s 208ms/step - loss: 0.0620 - sparse_categorical_accuracy: 0.9795 - val_loss: 8.6474 - val_sparse_categorical_accuracy: 0.5225\n",
            "Epoch 474/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0536 - sparse_categorical_accuracy: 0.9815 - val_loss: 8.8433 - val_sparse_categorical_accuracy: 0.5180\n",
            "Epoch 475/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0514 - sparse_categorical_accuracy: 0.9822 - val_loss: 8.9564 - val_sparse_categorical_accuracy: 0.5170\n",
            "Epoch 476/500\n",
            "196/196 [==============================] - 41s 207ms/step - loss: 0.0536 - sparse_categorical_accuracy: 0.9824 - val_loss: 8.5619 - val_sparse_categorical_accuracy: 0.5223\n",
            "Epoch 477/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0516 - sparse_categorical_accuracy: 0.9819 - val_loss: 8.6648 - val_sparse_categorical_accuracy: 0.5171\n",
            "Epoch 478/500\n",
            "196/196 [==============================] - 41s 211ms/step - loss: 0.0626 - sparse_categorical_accuracy: 0.9792 - val_loss: 8.7551 - val_sparse_categorical_accuracy: 0.5191\n",
            "Epoch 479/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0599 - sparse_categorical_accuracy: 0.9797 - val_loss: 8.8484 - val_sparse_categorical_accuracy: 0.5184\n",
            "Epoch 480/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0580 - sparse_categorical_accuracy: 0.9798 - val_loss: 8.6786 - val_sparse_categorical_accuracy: 0.5228\n",
            "Epoch 481/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0565 - sparse_categorical_accuracy: 0.9806 - val_loss: 8.6777 - val_sparse_categorical_accuracy: 0.5171\n",
            "Epoch 482/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0445 - sparse_categorical_accuracy: 0.9842 - val_loss: 8.6095 - val_sparse_categorical_accuracy: 0.5238\n",
            "Epoch 483/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0474 - sparse_categorical_accuracy: 0.9838 - val_loss: 8.6776 - val_sparse_categorical_accuracy: 0.5289\n",
            "Epoch 484/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0567 - sparse_categorical_accuracy: 0.9800 - val_loss: 8.5852 - val_sparse_categorical_accuracy: 0.5250\n",
            "Epoch 485/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0565 - sparse_categorical_accuracy: 0.9807 - val_loss: 8.8088 - val_sparse_categorical_accuracy: 0.5188\n",
            "Epoch 486/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0535 - sparse_categorical_accuracy: 0.9827 - val_loss: 8.7843 - val_sparse_categorical_accuracy: 0.5293\n",
            "Epoch 487/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0577 - sparse_categorical_accuracy: 0.9803 - val_loss: 8.5157 - val_sparse_categorical_accuracy: 0.5265\n",
            "Epoch 488/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0530 - sparse_categorical_accuracy: 0.9819 - val_loss: 8.6253 - val_sparse_categorical_accuracy: 0.5155\n",
            "Epoch 489/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0488 - sparse_categorical_accuracy: 0.9826 - val_loss: 8.7325 - val_sparse_categorical_accuracy: 0.5245\n",
            "Epoch 490/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0545 - sparse_categorical_accuracy: 0.9805 - val_loss: 8.7628 - val_sparse_categorical_accuracy: 0.5133\n",
            "Epoch 491/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0431 - sparse_categorical_accuracy: 0.9848 - val_loss: 8.7949 - val_sparse_categorical_accuracy: 0.5178\n",
            "Epoch 492/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0547 - sparse_categorical_accuracy: 0.9805 - val_loss: 8.8592 - val_sparse_categorical_accuracy: 0.5209\n",
            "Epoch 493/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0601 - sparse_categorical_accuracy: 0.9796 - val_loss: 8.9371 - val_sparse_categorical_accuracy: 0.5154\n",
            "Epoch 494/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0564 - sparse_categorical_accuracy: 0.9798 - val_loss: 8.6904 - val_sparse_categorical_accuracy: 0.5179\n",
            "Epoch 495/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0511 - sparse_categorical_accuracy: 0.9819 - val_loss: 8.6625 - val_sparse_categorical_accuracy: 0.5242\n",
            "Epoch 496/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0495 - sparse_categorical_accuracy: 0.9829 - val_loss: 8.7900 - val_sparse_categorical_accuracy: 0.5194\n",
            "Epoch 497/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0493 - sparse_categorical_accuracy: 0.9826 - val_loss: 8.7232 - val_sparse_categorical_accuracy: 0.5227\n",
            "Epoch 498/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0545 - sparse_categorical_accuracy: 0.9809 - val_loss: 8.5117 - val_sparse_categorical_accuracy: 0.5272\n",
            "Epoch 499/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0528 - sparse_categorical_accuracy: 0.9818 - val_loss: 8.7946 - val_sparse_categorical_accuracy: 0.5219\n",
            "Epoch 500/500\n",
            "196/196 [==============================] - 41s 210ms/step - loss: 0.0482 - sparse_categorical_accuracy: 0.9829 - val_loss: 8.7903 - val_sparse_categorical_accuracy: 0.5225\n",
            "Total training time 20614.528964996338 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-dfe9a9bf6a4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m                                           \u001b[0;34mf'_{cross_depth}cd'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                           \u001b[0;34mf'_{latent_depth}ld'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 )\n",
            "\u001b[0;32m<ipython-input-13-aa3cbc078dcb>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(_model, checkpoint_dir, history_dir, verbose)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhistory_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Save history output, should be the same as the tensorboard logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'history.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mfile_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'work/results/perceiver/210402__PERCEIVER_32fq_16fqb_128l_128ldim_1lh_128lhdim_1ch_128chdim_3cd_2ld/history.npy'"
          ]
        }
      ]
    }
  ]
}